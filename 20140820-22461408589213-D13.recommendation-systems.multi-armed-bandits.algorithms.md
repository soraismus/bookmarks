[![RichRelevance](http://engineering.richrelevance.com/wp-content/themes/rr/img/logo_new.png)](http://www.richrelevance.com/)

-   [Solutions](http://www.richrelevance.com/solutions/technology/)
    -   [Technology](http://www.richrelevance.com/solutions/technology/)
    -   [Personalization](http://www.richrelevance.com/solutions/personalization/)
        -   [Overview](http://www.richrelevance.com/solutions/personalization/)
        -   [Recommendations](http://www.richrelevance.com/solutions/personalization/recommendations/)
        -   [Promotions](http://www.richrelevance.com/solutions/personalization/promotions/)
        -   [Email](http://www.richrelevance.com/solutions/personalization/email/)
        -   [Mobile](http://www.richrelevance.com/solutions/personalization/mobile/)
        -   [Content](http://www.richrelevance.com/solutions/personalization/content/)
        -   [Site
            Monetization](http://www.richrelevance.com/solutions/personalization/monetization/)

    -   [Cloud
        Platform](http://www.richrelevance.com/solutions/cloud-platform/)
        -   [Overview](http://www.richrelevance.com/solutions/cloud-platform/)
        -   [Capabilities](http://www.richrelevance.com/solutions/cloud-platform/capabilities/)
        -   [Learn](http://www.richrelevance.com/solutions/cloud-platform/learn/)

    -   [Advertising](http://www.richrelevance.com/solutions/advertising/)
        -   [Overview](http://www.richrelevance.com/solutions/advertising/)
        -   [Targeting](http://www.richrelevance.com/solutions/advertising/targeting/)
        -   [Ad
            Units](http://www.richrelevance.com/solutions/advertising/ad-units/)
        -   [Reporting](http://www.richrelevance.com/solutions/advertising/reporting/)
        -   [For
            Retailers](http://www.richrelevance.com/solutions/personalization/monetization/)

    -   [Verticals](http://www.richrelevance.com/solutions/verticals/apparel/)
        -   [Apparel](http://www.richrelevance.com/solutions/verticals/apparel/)
        -   [Electronics](http://www.richrelevance.com/solutions/verticals/electronics/)
        -   [Office
            Supplies](http://www.richrelevance.com/solutions/verticals/office-supplies/)
        -   [Big
            Box](http://www.richrelevance.com/solutions/verticals/big-box/)

-   [Services](http://www.richrelevance.com/services/)
-   [Insights](http://www.richrelevance.com/insights/)
-   [Company](http://www.richrelevance.com/company/)
    -   [About Us](http://www.richrelevance.com/company/)
    -   [Leadership](http://www.richrelevance.com/company/leadership/)
    -   [Newsroom](http://www.richrelevance.com/company/newsroom/)
    -   [Partners](http://www.richrelevance.com/company/partners/)
    -   [Events](http://www.richrelevance.com/company/events/)
    -   [Careers](http://www.richrelevance.com/company/careers/)
    -   [Contact Us](http://www.richrelevance.com/company/contact-us/)

-   [Blog](http://www.richrelevance.com/blog/)
    -   [Corporate](http://www.richrelevance.com/blog/)
    -   [Engineering](http://engineering.richrelevance.com/)

[](http://www.richrelevance.com/)

-   [USA](# "USA")
    -   [USA](http://www.richrelevance.com/ "USA")
    -   [UK](http://www.richrelevance.com/uk/ "UK")

[Contact
Us](http://engineering.richrelevance.com/bandits-recommendation-systems/ "Contact Us")

Search

-   [Solutions](http://www.richrelevance.com/solutions/technology/)
    -   [Technology](http://www.richrelevance.com/solutions/technology/)
    -   [Personalization](http://www.richrelevance.com/solutions/personalization/)
        -   [Overview](http://www.richrelevance.com/solutions/personalization/)
        -   [Recommendations](http://www.richrelevance.com/solutions/personalization/recommendations/)
        -   [Promotions](http://www.richrelevance.com/solutions/personalization/promotions/)
        -   [Email](http://www.richrelevance.com/solutions/personalization/email/)
        -   [Mobile](http://www.richrelevance.com/solutions/personalization/mobile/)
        -   [Content](http://www.richrelevance.com/solutions/personalization/content/)
        -   [Site
            Monetization](http://www.richrelevance.com/solutions/personalization/monetization/)

    -   [Cloud
        Platform](http://www.richrelevance.com/solutions/cloud-platform/)
        -   [Overview](http://www.richrelevance.com/solutions/cloud-platform/)
        -   [Capabilities](http://www.richrelevance.com/solutions/cloud-platform/capabilities/)
        -   [Learn](http://www.richrelevance.com/solutions/cloud-platform/learn/)

    -   [Advertising](http://www.richrelevance.com/solutions/advertising/)
        -   [Overview](http://www.richrelevance.com/solutions/advertising/)
        -   [Targeting](http://www.richrelevance.com/solutions/advertising/targeting/)
        -   [Ad
            Units](http://www.richrelevance.com/solutions/advertising/ad-units/)
        -   [Reporting](http://www.richrelevance.com/solutions/advertising/reporting/)
        -   [For
            Retailers](http://www.richrelevance.com/solutions/personalization/monetization/)

    -   [Verticals](http://www.richrelevance.com/solutions/verticals/apparel/)
        -   [Apparel](http://www.richrelevance.com/solutions/verticals/apparel/)
        -   [Electronics](http://www.richrelevance.com/solutions/verticals/electronics/)
        -   [Office
            Supplies](http://www.richrelevance.com/solutions/verticals/office-supplies/)
        -   [Big
            Box](http://www.richrelevance.com/solutions/verticals/big-box/)

-   [Services](http://www.richrelevance.com/services/)
-   [Insights](http://www.richrelevance.com/insights/)
-   [Company](http://www.richrelevance.com/company/)
    -   [About Us](http://www.richrelevance.com/company/)
    -   [Leadership](http://www.richrelevance.com/company/leadership/)
    -   [Newsroom](http://www.richrelevance.com/company/newsroom/)
    -   [Partners](http://www.richrelevance.com/company/partners/)
    -   [Events](http://www.richrelevance.com/company/events/)
    -   [Careers](http://www.richrelevance.com/company/careers/)
    -   [Contact Us](http://www.richrelevance.com/company/contact-us/)

-   [Blog](http://www.richrelevance.com/blog/)
    -   [Corporate](http://www.richrelevance.com/blog/)
    -   [Engineering](http://engineering.richrelevance.com/)

Engineering Blog
================

Bandits for Recommendation Systems
----------------------------------

06/02/2014 â€¢ Topics:
[Bayesian](http://engineering.richrelevance.com/category/bayesian/ "View all posts in Bayesian"),
[Big
data](http://engineering.richrelevance.com/category/big-data/ "View all posts in Big data"),
[Data
Science](http://engineering.richrelevance.com/category/data-science/ "View all posts in Data Science")

by [Sergey
Feldman](http://engineering.richrelevance.com/author/sergey-feldman/ "Posts by Sergey Feldman")

![Sergey
Feldman](http://engineering.richrelevance.com/wp-content/uploads/sites/2/2014/06/Sergey-Feldman-blog-avatar-150x150.png)

*This is the first in a series of three blog posts on bandits for
recommendation systems.*

In this blog post, we will discuss the bandit problem and how it relates
to online recommender systems. Then, we'll cover some classic algorithms
and see how well they do in simulation.

A common problem for internet-based companies is: *which piece of
content should we display?* Google has this problem (which ad to show),
Facebook has this problem (which friend's post to show), and
RichRelevance has this problem (which product recommendation to show).
Many of the promising solutions come from the study of the *multi-armed
bandit* problem. A one-armed "bandit" is another way to say slot machine
(probably because both will leave you with empty pockets). Here is a
description that I hijacked from Wikipedia:

"The **multi-armed bandit problem**is the problem a gambler faces at a
row of slot machines when deciding which machines to play, how many
times to play each machine and in which order to play them.^^When
played, each machine provides a random reward from a distribution
specific to that machine. The objective of the gambler is to maximize
the sum of rewards earned through a sequence of lever pulls."

Let's rewrite this in retail language. Each time a shopper looks at a
webpage, we have to show them one of
![K](http://engineering.richrelevance.com/wp-content/plugins/latex/cache/tex_a5f3c6a11b03839d46af9fb43c97c188.gif)

product recommendations. They either click on it or do not, and we log
this (binary) reward. Next, we proceed to either the next shopper or the
next page view of this shopper and have to choose one of
![K](http://engineering.richrelevance.com/wp-content/plugins/latex/cache/tex_a5f3c6a11b03839d46af9fb43c97c188.gif)

product recommendations again. (Actually, we have to choose multiple
recommendations per page, and our 'reward' could instead be sales
revenue, but let's ignore these aspects for now.)

Multi-armed bandits come in two flavors: stochastic and adversarial. The
stochastic case is where each bandit doesn't change in response to your
actions, while in the adversarial case the bandits learn from your
actions and adjust their behavior to minimize your rewards. We care
about the stochastic case, and our goal is to find the arm which has the
largest *expected reward*. I will index the arms by
![a](http://engineering.richrelevance.com/wp-content/plugins/latex/cache/tex_0cc175b9c0f1b6a831c399e269772661.gif)

, and the probability distribution over possible rewards
![r](http://engineering.richrelevance.com/wp-content/plugins/latex/cache/tex_4b43b0aee35624cd95b910189b3dc231.gif)

for each arm
![a](http://engineering.richrelevance.com/wp-content/plugins/latex/cache/tex_0cc175b9c0f1b6a831c399e269772661.gif)

can be written as
![p\_a(r)](http://engineering.richrelevance.com/wp-content/plugins/latex/cache/tex_6716a09359fd3fd67b3d08220af0ebd0.gif)

. We have to find the arm with the largest mean reward

![\\mu\_a =
E\_a[r]](http://engineering.richrelevance.com/wp-content/plugins/latex/cache/tex_0a2a28fd74db2774957f497adfe923a1.gif)

as quickly as possible while accumulating the most rewards along the
way. One important point is that in practice
![p\_a(r)](http://engineering.richrelevance.com/wp-content/plugins/latex/cache/tex_6716a09359fd3fd67b3d08220af0ebd0.gif)

are *non-stationary*, **that is, rewards change over time, and we have
to take that into account when we design our algorithms.

Approach \#1: A Naive Algorithm
-------------------------------

We need to figure out the mean reward (expected value) of each arm. So,
let's just try each arm 100 times, take the sample mean of the rewards
we get back, and then pull the arm with the best sample mean forever
more. Problem solved?

Not exactly. This approach will get you in trouble in a few key ways:

1.  If
    ![K](http://engineering.richrelevance.com/wp-content/plugins/latex/cache/tex_a5f3c6a11b03839d46af9fb43c97c188.gif)
    is of even moderate size (10-100), you'll spend a long time
    gathering data before you can actually benefit from feedback.
2.  Is 100 samples for each arm enough? How many should we use? This is
    an arbitrary parameter that will require experimentation to
    determine.
3.  If after 100 samples (or however many), the arm you settle on is not
    actually the optimal one, you can never recover.
4.  In practice, the reward distribution is likely to change over time,
    and we should use an algorithm that can take that into account.

OK, so maybe the naive approach won't work. Let's move on to a few that
are actually used in practice.

Approach \#2: The
![\\epsilon-](http://engineering.richrelevance.com/wp-content/plugins/latex/cache/tex_4c11e771dab4f7aba15f0edbc2b4f9d6.gif)

Greedy Algorithm

What we'd like to do is start using what we think is the best arm as
soon as possible, and adjust course when information to the contrary
becomes available. And we don't want to get stuck in a sub-optimal state
forever. The "
![\\epsilon-](http://engineering.richrelevance.com/wp-content/plugins/latex/cache/tex_4c11e771dab4f7aba15f0edbc2b4f9d6.gif)

greedy" algorithm addresses both of these concerns. Here is how it
works: with probability
![1-\\epsilon](http://engineering.richrelevance.com/wp-content/plugins/latex/cache/tex_39eb128cbb659e434ad0f0284ea30ec0.gif)

pull the arm with the best current sample mean reward, and otherwise
pull a random other arm (uniformly). The advantages over the naive are:

1.  Guaranteed to not get stuck in a suboptimal state forever.
2.  Will use the current best performing arm a large proportion of the
    time.

But setting
![\\epsilon](http://engineering.richrelevance.com/wp-content/plugins/latex/cache/tex_92e4da341fe8f4cd46192f21b6ff3aa7.gif)

is *hard*. If itâ€™s too small, learning is slow at the start, and you
will be slow to react to changes. If we happen to sample, say, the
second-best arm the first few times, it may take a long time to discover
that another arm is actually better. If
![\\epsilon](http://engineering.richrelevance.com/wp-content/plugins/latex/cache/tex_92e4da341fe8f4cd46192f21b6ff3aa7.gif)

is too big, youâ€™ll waste many trials pulling random arms without gaining
much. After a while, we'll have enough samples to be pretty sure which
is best, but we will still be wasting an
![\\epsilon](http://engineering.richrelevance.com/wp-content/plugins/latex/cache/tex_92e4da341fe8f4cd46192f21b6ff3aa7.gif)

of our traffic exploring other options. In short,
![\\epsilon](http://engineering.richrelevance.com/wp-content/plugins/latex/cache/tex_92e4da341fe8f4cd46192f21b6ff3aa7.gif)

is a parameter that gives poor performance at the extremes, and we have
little guidance as to how to set it.

Approach \#3: Upper Confidence Bound Algorithms
-----------------------------------------------

In the world of statistics, whenever you estimate some unknown parameter
(such as the mean of a distribution) using random samples, there is a
way to quantify the uncertainty inherent in your estimate. For example,
the true mean of a fair six-sided die is 3.5. But if you only roll it
once and get a 2, your best estimate of the mean is just 2. Obviously
that estimate is not very good, and we can quantify just how variable it
is. The**re **are *confidence bounds* which can be written, for example,
as: "The mean of this die is 2, with a 95-th percentile lower bound of
1.4 and a 95-th percentile upper bound of 5.2."

The upper confidence bound (UCB) family of algorithms, as its name
suggests, simply selects the arm with the largest upper confidence bound
at each round. The intuition is this: the more times you roll the die,
the tighter the confidence bounds. If your roll the die an infinite
number of times then the width of the confidence bound is zero, and
before you ever roll it the width of the confidence bound is the largest
it will ever be. So, as the number of rolls increases, the uncertainty
decreases, and so does the width of the confidence bound.

In the bandit case, imagine that you have to introduce a brand new
choice to the set of
![K](http://engineering.richrelevance.com/wp-content/plugins/latex/cache/tex_a5f3c6a11b03839d46af9fb43c97c188.gif)

choices a week into your experiment. The
![\\epsilon-](http://engineering.richrelevance.com/wp-content/plugins/latex/cache/tex_4c11e771dab4f7aba15f0edbc2b4f9d6.gif)

greedy algorithm would keep chugging along, showing this new choice
rarely (if the initial mean is defined to be 0). But the upper
confidence bound of this new choice will be very large because of the
uncertainty that results from us never having pulled it. So UCB will
choose this new arm until its upper bound is below the upper bound of
the more established choices.

So, the advantages are:

1.  Take uncertainty of sample mean estimate into account in a smart
    way.
2.  No parameters to validate.

And the major disadvantage is that the confidence bounds designed in the
machine learning literature require heuristic adjustments. One way to
get around having to wade through heuristics is to recall the central
limit theorem. I'll skip the math but it
[says](http://www.stat.yale.edu/Courses/1997-98/101/sampmn.htm) that the
distribution of the sample mean computed from samples from *any*
distribution converges to a Normal (Gaussian) as the number of samples
increases (and fairly quickly). Why does that matter here? Because we
are estimating the true expected reward for each arm with a sample mean.
Ideally we want a posterior of where the true mean is, but that's hard
in non-Bernoulli and non-Gaussian cases. So we will instead content
ourselves with an approximation and use a Gaussian distribution centered
at the sample mean instead. We can thus always use a, say, 95% upper
confidence bound, and be secure in the knowledge that it will become
more and more accurate the more samples we get. I will discuss this in
more detail in the next blog post.**

Simulation Comparison
---------------------

So how do these 3 algorithms perform? To find out, I ran a simple
simulation 100 times with
![K=5](http://engineering.richrelevance.com/wp-content/plugins/latex/cache/tex_cce2b8b99164ef2d0c3074e12a6c4df1.gif)

and binary rewards (aka a Bernoulli bandit). Here are the 5 algorithms
compared:

1.  Random - just pick a random arm each time without learning anything.
2.  Naive - with 100 samples of each arm before committing to the best
    one
3.  ![\\epsilon-](http://engineering.richrelevance.com/wp-content/plugins/latex/cache/tex_4c11e771dab4f7aba15f0edbc2b4f9d6.gif)
    Greedy - with
    ![\\epsilon=0.01](http://engineering.richrelevance.com/wp-content/plugins/latex/cache/tex_1e3f1ef3eaa535484de89938e3ee8d86.gif)
4.  UCB - with (1 - 1/t) bounds (heuristic modification of
    [UCB1](http://lane.compbio.cmu.edu/courses/slides_ucb.pdf))
5.  UCB - with 95% bounds

The metric used to compare these algorithms is average (over all the
trials) *expected regret* (lower is better), which quantifies how much
reward we missed out on by pulling the suboptimal arm at each time step.
The Python code is
[here](https://gist.github.com/sergeyf/63a37a5db18f8cebf962) and the
results are in the plot below.

[![Simulation
Results](http://engineering.richrelevance.com/wp-content/uploads/sites/2/2014/05/part_1_simulations-700x507.png)](http://engineering.richrelevance.com/wp-content/uploads/sites/2/2014/05/part_1_simulations.png)

What can we conclude from this plot?

1.  Naive is as bad as random for the first
    ![100K](http://engineering.richrelevance.com/wp-content/plugins/latex/cache/tex_70c7acef529834326f5a880ae840b93b.gif)
    rounds, but then has effectively flat performance. In the real
    world, the arms have shifting rewards, so this algorithm is
    impractical because it over-commits
2.  ![\\epsilon-](http://engineering.richrelevance.com/wp-content/plugins/latex/cache/tex_4c11e771dab4f7aba15f0edbc2b4f9d6.gif)
    greedy is OK but without a decaying
    ![\\epsilon](http://engineering.richrelevance.com/wp-content/plugins/latex/cache/tex_92e4da341fe8f4cd46192f21b6ff3aa7.gif)
    we're still wasting 1% of traffic on exploration when it may no
    longer be necessary.
3.  The UCB algorithms are great. It's not clear which one is the winner
    in this limited horizon, but both handily beat all of the other
    algorithms.

Now you know all about bandits, and have a good idea of how they might
be relevant to online recommender systems. But there's more to do before
we have a system that is really up to the job.

Coming up next: [let's get Bayesian with Thompson
Sampling](http://engineering.richrelevance.com/recommendations-thompson-sampling/)!

About [Sergey
Feldman](http://engineering.richrelevance.com/author/sergey-feldman/ "Posts by Sergey Feldman"):
![image](http://engineering.richrelevance.com/wp-content/uploads/sites/2/avatars/48/5eb2c9c5e67de506179de078dd69927b-bpthumb.jpg)

Sergey Feldman is a data scientist & machine learning cowboy with the
RichRelevance Analytics team. He was born in Ukraine, moved with his
family to Skokie, Illinois at age 10, and now lives in Seattle. In 2012
he obtained his machine learning PhD from the University of Washington.
Sergey loves random forests and thinks the Fourier transform is pure
magic.

20 Comments
-----------

![image](http://1.gravatar.com/avatar/5bc3aeec09464c41078ccafacfa7832b?s=96&d=http%3A%2F%2F1.gravatar.com%2Favatar%2Fad516503a11cd5ca435acc9bb6523536%3Fs%3D96&r=G)
Zach says:

[June 2, 2014 at 5:56
pm](http://engineering.richrelevance.com/bandits-recommendation-systems/#comment-10269)

Great article. I really like the upper confidence band approach. I have
a question: what would you do in a situation where you have delayed
feedback on successes/failures? E.g. lets say you have to pick 1,000,000
actions, observe their outcomes, and then set your next 1,000,000
actions?

Initially, an UCB algorithm would probably work well, but if you ever
introduced a new lever you would spend a way too many pulls discovering
it was sub-optimal.

[Reply](/bandits-recommendation-systems/?replytocom=10269#respond)

-   ![image](http://engineering.richrelevance.com/wp-content/uploads/sites/2/avatars/48/5eb2c9c5e67de506179de078dd69927b-bpthumb.jpg)
    Sergey Feldman says:

    [June 2, 2014 at 6:09
    pm](http://engineering.richrelevance.com/bandits-recommendation-systems/#comment-10270)

    Hi Zach,

    Thanks for the comment. In the next two blog posts of this series I
    will discuss the delayed feedback case. The short answer is: don't
    use UCB. Use Thompson Sampling instead. It works just fine in
    between updates.

    [Reply](/bandits-recommendation-systems/?replytocom=10270#respond)

![image](http://0.gravatar.com/avatar/e2dec1c8266d15082181260c2900f4a3?s=96&d=http%3A%2F%2F0.gravatar.com%2Favatar%2Fad516503a11cd5ca435acc9bb6523536%3Fs%3D96&r=G)
[Thijs](http://Www.sitmo.com) says:

[June 2, 2014 at 6:55
pm](http://engineering.richrelevance.com/bandits-recommendation-systems/#comment-10271)

Thanks for the great article, very clean an easy. This was time well
spent!

[Reply](/bandits-recommendation-systems/?replytocom=10271#respond)

![image](http://1.gravatar.com/avatar/572f0d27f183a820fdd2089564329717?s=96&d=http%3A%2F%2F1.gravatar.com%2Favatar%2Fad516503a11cd5ca435acc9bb6523536%3Fs%3D96&r=G)
Thomas Levi says:

[June 2, 2014 at 10:45
pm](http://engineering.richrelevance.com/bandits-recommendation-systems/#comment-10274)

Nice, concise post. Any chance you'll be discussing Bayesian bandits in
a follow up?

[Reply](/bandits-recommendation-systems/?replytocom=10274#respond)

-   ![image](http://engineering.richrelevance.com/wp-content/uploads/sites/2/avatars/48/5eb2c9c5e67de506179de078dd69927b-bpthumb.jpg)
    Sergey Feldman says:

    [June 2, 2014 at 10:47
    pm](http://engineering.richrelevance.com/bandits-recommendation-systems/#comment-10275)

    Thomas: yup! The second blog post will be about Thompson Sampling,
    which is effectively a Bayesian Bandit.

    [Reply](/bandits-recommendation-systems/?replytocom=10275#respond)

    -   ![image](http://1.gravatar.com/avatar/572f0d27f183a820fdd2089564329717?s=96&d=http%3A%2F%2F1.gravatar.com%2Favatar%2Fad516503a11cd5ca435acc9bb6523536%3Fs%3D96&r=G)
        Thomas Levi says:

        [June 2, 2014 at 11:05
        pm](http://engineering.richrelevance.com/bandits-recommendation-systems/#comment-10276)

        Great! I've been playing around with them a bit, but it's always
        good to hear another's take on it.

        [Reply](/bandits-recommendation-systems/?replytocom=10276#respond)

![image](http://0.gravatar.com/avatar/8526d17aea65f67ae3c05c755f2ebba8?s=96&d=http%3A%2F%2F0.gravatar.com%2Favatar%2Fad516503a11cd5ca435acc9bb6523536%3Fs%3D96&r=G)
Krishnan says:

[June 2, 2014 at 11:47
pm](http://engineering.richrelevance.com/bandits-recommendation-systems/#comment-10277)

Thank you so much for the python code! numpy dependence notwithstanding,
your variable names & clear explanation helped me in translating this to
Scala. Thanks again Sir!

[Reply](/bandits-recommendation-systems/?replytocom=10277#respond)

![image](http://0.gravatar.com/avatar/a76725c28539953a5d1933f965d98deb?s=96&d=http%3A%2F%2F0.gravatar.com%2Favatar%2Fad516503a11cd5ca435acc9bb6523536%3Fs%3D96&r=G)
Daniel Conti says:

[June 2, 2014 at 11:47
pm](http://engineering.richrelevance.com/bandits-recommendation-systems/#comment-10278)

Hey,

My main curiosity is a difference in assumptions btwn MAB and
recommending products, which is you can pull a slot machine any number
of times, while you can only show an individual product, every K times.
Basically product x can only be shown once per user session.

In regards to this i'm curious about how you measured regret in your
python code. For the K products the algorithms choose (Native, UCB1
etc), when you calculate the total regret do you take the above into
account? IE, the K product choices shouldn't be measured against the
maximum product (estimated) expected value, but the K max maximum
product (estimated) expected values. Do you think this would have any
effect on your measure of cumulative expected regret?

Also would the UCB account for this fact? That products can not be shown
repeatedly to a single user.

I hope that I havnt been too all over the place.

[Reply](/bandits-recommendation-systems/?replytocom=10278#respond)

-   ![image](http://engineering.richrelevance.com/wp-content/uploads/sites/2/avatars/48/5eb2c9c5e67de506179de078dd69927b-bpthumb.jpg)
    Sergey Feldman says:

    [June 3, 2014 at 2:58
    am](http://engineering.richrelevance.com/bandits-recommendation-systems/#comment-10279)

    Hi Daniel,

    First, let me say that there are certainly bandit models in the
    literature that take into account "resource constraints".

    In the code & experiments I posted, there is no notion of a session
    at all. Each view counts as independent and separate, and a product
    can be shown to the same person as often as the algorithm chooses.
    These assumptions do not reflect reality, and would result in the
    same products being shown over and over again. In third blog post, I
    will describe what exactly we use bandits for and the point you
    bring up will be less pressing (preview: we don't choose between
    products, but between *strategies* which choose sets of products to
    show).

    I personally think that taking session constraints into account is a
    good idea, but it is hard to do right. For example, in the middle of
    a session we have no idea how much longer the session will last
    (apart from middling guesses). What if we show all the reasonable
    products in the first 10 views, but there are another 50 views
    coming up? In that case, the restriction that we show each product
    only once will mean we start showing irrelevant products, and risk
    annoying the shopper. I'm sure there is room to develop models that
    act on a session level, but these blog posts are more basic than a
    true in-production model would require.

    One thing that does seem to help is to increase the amount of
    exploration that you do. Then the algorithm is more likely to show a
    diverse array of products, and this can help for longer sessions.

    Hope that helps!

    [Reply](/bandits-recommendation-systems/?replytocom=10279#respond)

![image](http://www.gravatar.com/avatar/8fd6160ec2cca78ffa6734bb707a67df?d=http://engineering.richrelevance.com/wp-content/plugins/avatar/mystery-man.jpg&s=96)
[Zac Aghion](https://www.facebook.com/zacaghion) says:

[June 3, 2014 at 3:19
am](http://engineering.richrelevance.com/bandits-recommendation-systems/#comment-10280)

Nice post, Sergey. We've been using Thompson Sampling to deal with
delayed feedback, as we're dealing with data coming from mobile
applications which are not always connected. The results have been
pretty good, here's a breakdown of how it works if you're interested:
[https://splitforce.com/resources/auto-optimization/](https://splitforce.com/resources/auto-optimization/)

Have you thought about how to deal with changes in environmental factors
over relatively longer periods of time? For example, seasonality or
changes in popular taste.

[Reply](/bandits-recommendation-systems/?replytocom=10280#respond)

![image](http://1.gravatar.com/avatar/bdb09d7d6dce8f49ad7509ccf5785157?s=96&d=http%3A%2F%2F1.gravatar.com%2Favatar%2Fad516503a11cd5ca435acc9bb6523536%3Fs%3D96&r=G)
Timo Leiter says:

[June 3, 2014 at 6:07
am](http://engineering.richrelevance.com/bandits-recommendation-systems/#comment-10282)

What a great article. Rarely I find such enjoyable reads These Days.
Will definitely check in here more frequently.

[Reply](/bandits-recommendation-systems/?replytocom=10282#respond)

![image](http://1.gravatar.com/avatar/17fb42436a67be669538c0e75f3b4b65?s=96&d=http%3A%2F%2F1.gravatar.com%2Favatar%2Fad516503a11cd5ca435acc9bb6523536%3Fs%3D96&r=G)
[Alexandru Cobuz](http://skyul.com) says:

[June 3, 2014 at 3:20
pm](http://engineering.richrelevance.com/bandits-recommendation-systems/#comment-10289)

Didn't expect to see this performance from UCB, it's above the rest
which is interesting.

[Reply](/bandits-recommendation-systems/?replytocom=10289#respond)

![image](http://0.gravatar.com/avatar/a76725c28539953a5d1933f965d98deb?s=96&d=http%3A%2F%2F0.gravatar.com%2Favatar%2Fad516503a11cd5ca435acc9bb6523536%3Fs%3D96&r=G)
Daniel Conti says:

[June 3, 2014 at 4:42
pm](http://engineering.richrelevance.com/bandits-recommendation-systems/#comment-10292)

\>First, let me say that there are certainly bandit models in the
literature that take into account "resource constraints".

Do you know of any off the top of your head?

[Reply](/bandits-recommendation-systems/?replytocom=10292#respond)

-   ![image](http://engineering.richrelevance.com/wp-content/uploads/sites/2/avatars/48/5eb2c9c5e67de506179de078dd69927b-bpthumb.jpg)
    Sergey Feldman says:

    [June 3, 2014 at 9:29
    pm](http://engineering.richrelevance.com/bandits-recommendation-systems/#comment-10294)

    I trust these authors, but haven't read the paper:
    [http://arxiv.org/pdf/1402.6779.pdf](http://arxiv.org/pdf/1402.6779.pdf)

    [Reply](/bandits-recommendation-systems/?replytocom=10294#respond)

![image](http://1.gravatar.com/avatar/f0dd11c4e03f9280e13eb4ce6e3d1f03?s=96&d=http%3A%2F%2F1.gravatar.com%2Favatar%2Fad516503a11cd5ca435acc9bb6523536%3Fs%3D96&r=G)
Justin says:

[June 10, 2014 at 4:34
pm](http://engineering.richrelevance.com/bandits-recommendation-systems/#comment-10418)

I just wanted to say that this is a very well-written blog article on
this topic. Most articles I read on statistics, probability, and machine
learning present things in a very opaque manner. Thanks for the clear
blog post.

[Reply](/bandits-recommendation-systems/?replytocom=10418#respond)

![image](http://0.gravatar.com/avatar/0b5390fa4ebb2519d6165318c8d6fa7f?s=96&d=http%3A%2F%2F0.gravatar.com%2Favatar%2Fad516503a11cd5ca435acc9bb6523536%3Fs%3D96&r=G)
Ira says:

[July 30, 2014 at 1:02
pm](http://engineering.richrelevance.com/bandits-recommendation-systems/#comment-10835)

A updated link to the new article\

[http://engineering.richrelevance.com/recommendations-thompson-sampling/](http://engineering.richrelevance.com/recommendations-thompson-sampling/)\
 would be good.

It's not any more "coming up next":) It's there!

[Reply](/bandits-recommendation-systems/?replytocom=10835#respond)

### Leave a Comment [Cancel reply](/bandits-recommendation-systems/#respond)

Your email address will not be published. Required fields are marked \*

Name\*

Email\*

Website

Comment

You may use these HTML tags and attributes:
`<a href="" title=""> <abbr title=""> <acronym title=""> <b> <blockquote cite=""> <cite> <code> <del datetime=""> <em> <i> <q cite=""> <strike> <strong> `

#### Categories

Select Category Analytics Bayesian Big data Content Personalization Data
Mining & Algorithms Data Science Engineering Culture Hack Week Hadoop
Java Join Ou{rr} Family Kafka Programming R Recommendations Scala

#### Latest Tweet

-   .[@L2\_ThinkTank](http://twitter.com/L2_ThinkTank) &
    [@RichRelevance](http://twitter.com/RichRelevance) report shows how
    retailers drive customers from clicks to bricks and back again:
    [http://t.co/DfGPtZ3xVu](http://t.co/DfGPtZ3xVu), Jul 30

[Follow @richrelevance](https://twitter.com/richrelevance)

#### Archives

Select Month June 2014 May 2014 January 2014 December 2013 October 2013
August 2013 July 2013 May 2013

-   [Facebook](http://www.facebook.com/richrelevance "Facebook")
-   [Google+](http://plus.google.com/100318823509149944458/posts "Google+")
-   [Twitter](http://www.twitter.com/richrelevance "Twitter")
-   [Linkedin](http://www.linkedin.com/company/richrelevance "Linkedin")
-   [Youtube](http://www.youtube.com/user/richrelevance "Youtube")

-   [Solutions](#)
    -   [Technology](http://www.richrelevance.com/solutions/technology/)
    -   [Personalization](http://www.richrelevance.com/solutions/personalization/)
    -   [Cloud
        Platform](http://www.richrelevance.com/solutions/cloud-platform/)
    -   [Advertising](http://www.richrelevance.com/solutions/advertising/)
    -   [Verticals](http://www.richrelevance.com/solutions/verticals/)

-   [Services](http://www.richrelevance.com/services/)
-   [Insights](http://www.richrelevance.com/insights/)
-   [Company](http://www.richrelevance.com/company/)
    -   [About Us](http://www.richrelevance.com/company/)
    -   [Leadership](http://www.richrelevance.com/company/leadership/)
    -   [Newsroom](http://www.richrelevance.com/company/newsroom/)
    -   [Partners](http://www.richrelevance.com/company/partners/)
    -   [Events](http://www.richrelevance.com/company/events/)
    -   [Careers](http://www.richrelevance.com/company/careers/)
    -   [Contact Us](http://www.richrelevance.com/company/contact-us/)

-   [Blog](http://www.richrelevance.com/blog/)
    -   [Corporate](http://www.richrelevance.com/blog/)
    -   [Engineering Blog](http://engineering.richrelevance.com/)

Search

[Contact
Us](http://engineering.richrelevance.com/bandits-recommendation-systems/ "Contact Us")

-   [USA](# "USA")
    -   [USA](# "USA")
    -   [UK](# "UK")
    -   [France](# "France")
    -   [Deutschland](# "Deutschland")

[Privacy (Updated July 24,
2013)](http://engineering.richrelevance.com/privacy/ "Privacy")

[Retargeting
Opt-out](http://engineering.richrelevance.com/privacy/opt-out/ "Opt-out")

Copyright Â© 2007-2014 RichRelevance, Inc. All Rights Reserved.

![image](//googleads.g.doubleclick.net/pagead/viewthroughconversion/989365112/?value=0&label=xbnRCMC7lwUQ-Ibi1wM&guid=ON&script=0)

[]()

This markdown document has been converted from the html document located at:
http://engineering.richrelevance.com/bandits-recommendation-systems/
