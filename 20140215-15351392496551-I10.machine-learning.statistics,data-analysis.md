Kernel embedding of distributions
=================================

From Wikipedia, the free encyclopedia

Jump to: [navigation](#mw-navigation), [search](#p-search)

In [machine learning](/wiki/Machine_learning "Machine learning"), the
**kernel embedding of distributions** (also called the **kernel mean**
or **mean map**) comprises a class of
[nonparametric](/wiki/Nonparametric "Nonparametric") methods in which a
[probability
distribution](/wiki/Probability_distribution "Probability distribution")
is represented as an element of a [reproducing kernel Hilbert
space](/wiki/Reproducing_kernel_Hilbert_space "Reproducing kernel Hilbert space")
(RKHS).^[[1]](#cite_note-Smola2007-1)^ A generalization of the
individual data-point feature mapping done in classical [kernel
methods](/wiki/Kernel_methods "Kernel methods"), the embedding of
distributions into infinite-dimensional feature spaces can preserve all
of the statistical features of arbitrary distributions, while allowing
one to compare and manipulate distributions using Hilbert space
operations such as [inner
products](/wiki/Inner_product "Inner product"), distances,
[projections](/wiki/Projection_(linear_algebra) "Projection (linear algebra)"),
[linear
transformations](/wiki/Linear_transformation "Linear transformation"),
and [spectral
analysis](/wiki/Spectral_theory "Spectral theory").^[[2]](#cite_note-Song2013-2)^
This [learning](/wiki/Machine_learning "Machine learning") framework is
very general and can be applied to distributions over any space
![\\Omega
](//upload.wikimedia.org/math/b/9/d/b9d99db8626de63193c7fe96273a6cae.png)
on which a sensible [kernel
function](/wiki/Kernel_function "Kernel function") (measuring similarity
between elements of ![\\Omega
](//upload.wikimedia.org/math/b/9/d/b9d99db8626de63193c7fe96273a6cae.png))
may be defined. For example, various kernels have been proposed for
learning from data which are:
[vectors](/wiki/Vector_(mathematics_and_physics) "Vector (mathematics and physics)")
in ![{\\mathbb
{R}}\^{d}](//upload.wikimedia.org/math/5/e/8/5e8bb29789ff92c1fa703ebf600d64b9.png),
discrete classes/categories,
[strings](/wiki/String_(computer_science) "String (computer science)"),
[graphs](/wiki/Graph_(mathematics) "Graph (mathematics)")/[networks](/wiki/Network_theory "Network theory"),
images, [time series](/wiki/Time_series "Time series"),
[manifolds](/wiki/Manifold "Manifold"), [dynamical
systems](/wiki/Dynamical_systems "Dynamical systems"), and other
structured objects.^[[3]](#cite_note-3)^^[[4]](#cite_note-4)^ The theory
behind kernel embeddings of distributions has been primarily developed
by [Alex Smola](http://alex.smola.org/), [Le
Song](http://www.cc.gatech.edu/~lsong/) , [Arthur
Gretton](http://www.gatsby.ucl.ac.uk/~gretton/), and [Bernhard
Schölkopf](/wiki/Bernhard_Sch%C3%B6lkopf "Bernhard Schölkopf").

The analysis of distributions is fundamental in [machine
learning](/wiki/Machine_learning "Machine learning") and
[statistics](/wiki/Statistics "Statistics"), and many algorithms in
these fields rely on information theoretic approaches such as
[entropy](/wiki/Entropy "Entropy"), [mutual
information](/wiki/Mutual_information "Mutual information"), or
[Kullback–Leibler
divergence](/wiki/Kullback%E2%80%93Leibler_divergence "Kullback–Leibler divergence").
However, to estimate these quantities, one must first either perform
density estimation, or employ sophisticated
space-partitioning/bias-correction strategies which are typically
infeasible for high-dimensional data.^[[5]](#cite_note-SongThesis-5)^
Commonly, methods for modeling complex distributions rely on parametric
assumptions that may be unfounded or computationally challenging (e.g.
[Gaussian mixture
models](/wiki/Mixture_model#Gaussian_mixture_model "Mixture model")),
while nonparametric methods like [kernel density
estimation](/wiki/Kernel_density_estimation "Kernel density estimation")
(Note: the smoothing kernels in this context have a different
interpretation than the kernels discussed here) or [characteristic
function](/wiki/Characteristic_function_(probability_theory) "Characteristic function (probability theory)")
representation (via the [Fourier
transform](/wiki/Fourier_transform "Fourier transform") of the
distribution) break down in high-dimensional
settings.^[[2]](#cite_note-Song2013-2)^

Methods based on the kernel embedding of distributions sidestep these
problems and also possess the following
advantages:^[[5]](#cite_note-SongThesis-5)^

1.  Data may be modeled without restrictive assumptions about the form
    of the distributions and relationships between variables
2.  Intermediate density estimation is not needed
3.  Practitioners may specify the properties of a distribution most
    relevant for their problem (incorporating prior knowledge via choice
    of the kernel)
4.  If a *characteristic* kernel is used, then the embedding can
    uniquely preserve all information about a distribution, while thanks
    to the [kernel trick](/wiki/Kernel_trick "Kernel trick"),
    computations on the potentially infinite-dimensional RKHS can be
    implemented in practice as simple
    [Gram](/wiki/Gramian_matrix "Gramian matrix") matrix operations
5.  Dimensionality-independent rates of convergence for the empirical
    kernel mean (estimated using samples from the distribution) to the
    kernel embedding of the true underlying distribution can be proven.
6.  Learning algorithms based on this framework exhibit good
    generalization ability and finite sample convergence, while often
    being simpler and more effective than information theoretic methods

Thus, learning via the kernel embedding of distributions offers a
principled drop-in replacement for information theoretic approaches and
is a framework which not only subsumes many popular methods in machine
learning and statistics as special cases, but also can lead to entirely
new learning algorithms.

Contents
--------

-   [1 Definitions](#Definitions)
    -   [1.1 Kernel embedding](#Kernel_embedding)
    -   [1.2 Empirical kernel embedding](#Empirical_kernel_embedding)
    -   [1.3 Joint distribution
        embedding](#Joint_distribution_embedding)
    -   [1.4 Conditional distribution
        embedding](#Conditional_distribution_embedding)

-   [2 Properties](#Properties)
    -   [2.1 Convergence of empirical kernel mean to the true
        distribution
        embedding](#Convergence_of_empirical_kernel_mean_to_the_true_distribution_embedding)
    -   [2.2 Universal kernels](#Universal_kernels)
    -   [2.3 Parameter selection for conditional distribution kernel
        embeddings](#Parameter_selection_for_conditional_distribution_kernel_embeddings)

-   [3 Rules of probability as operations in the
    RKHS](#Rules_of_probability_as_operations_in_the_RKHS)
    -   [3.1 Kernel sum rule](#Kernel_sum_rule)
    -   [3.2 Kernel chain rule](#Kernel_chain_rule)
    -   [3.3 Kernel Bayes' rule](#Kernel_Bayes.27_rule)

-   [4 Applications](#Applications)
    -   [4.1 Measuring distance between
        distributions](#Measuring_distance_between_distributions)
    -   [4.2 Kernel two sample test](#Kernel_two_sample_test)
    -   [4.3 Density estimation via kernel
        embeddings](#Density_estimation_via_kernel_embeddings)
    -   [4.4 Measuring dependence of random
        variables](#Measuring_dependence_of_random_variables)
    -   [4.5 Kernel belief propagation](#Kernel_belief_propagation)
    -   [4.6 Nonparametric filtering in hidden Markov
        models](#Nonparametric_filtering_in_hidden_Markov_models)
    -   [4.7 Support measure machines](#Support_measure_machines)
    -   [4.8 Domain adaptation under covariate, target, and conditional
        shift](#Domain_adaptation_under_covariate.2C_target.2C_and_conditional_shift)
    -   [4.9 Domain generalization via invariant feature
        representation](#Domain_generalization_via_invariant_feature_representation)

-   [5 Example](#Example)
-   [6 References](#References)

Definitions[[edit](/w/index.php?title=Kernel_embedding_of_distributions&action=edit&section=1 "Edit section: Definitions")]
---------------------------------------------------------------------------------------------------------------------------

Let
![X](//upload.wikimedia.org/math/0/2/1/02129bb861061d1a052c592e2dc6b383.png)
denote a random variable with domain ![\\Omega
](//upload.wikimedia.org/math/b/9/d/b9d99db8626de63193c7fe96273a6cae.png)
and distribution
![P(X)](//upload.wikimedia.org/math/0/c/3/0c3d72395d7576ab13b9e9389f865960.png).
Given a kernel
![k](//upload.wikimedia.org/math/8/c/e/8ce4b16b22b58894aa86c421e8759df3.png)
on ![\\Omega \\times \\Omega
](//upload.wikimedia.org/math/b/c/0/bc0b7238f76bf535ae3278bad81d1d08.png),
the [Moore-Aronszajn
Theorem](/wiki/Reproducing_kernel_Hilbert_space#Moore-Aronszajn_Theorem "Reproducing kernel Hilbert space")
asserts the existence of a RKHS ![{\\mathcal
{H}}](//upload.wikimedia.org/math/c/6/d/c6d79f9e5dd728bae3a00468c627d341.png)
(a [Hilbert space](/wiki/Hilbert_space "Hilbert space") of functions
![f:\\Omega \\mapsto {\\mathbb
{R}}](//upload.wikimedia.org/math/2/3/0/2305077034d611aa7a7aa122f61bfb9b.png)
equipped with inner products ![\\langle \\cdot ,\\cdot \\rangle
\_{{\\mathcal
{H}}}](//upload.wikimedia.org/math/9/b/4/9b4f66ec3b7466a3ab84d885ac9d27ca.png)
and norms ![||\\cdot ||\_{{\\mathcal
{H}}}](//upload.wikimedia.org/math/5/6/3/563827ce82242b0f8a217928e92b3d35.png))
in which the element ![\\ k(x,\\cdot
)](//upload.wikimedia.org/math/b/a/9/ba993169a2ab5df7b6ad0673e3440c91.png)
satisfies the reproducing property ![\\langle f,k(x,\\cdot )\\rangle
\_{{\\mathcal {H}}}=f(x)\\ \\forall f\\in {\\mathcal {H}},\\forall x\\in
\\Omega
](//upload.wikimedia.org/math/e/8/c/e8c35d77c42d2f7963445e55b305ebc0.png).
One may alternatively consider ![\\ k(x,\\cdot
)](//upload.wikimedia.org/math/b/a/9/ba993169a2ab5df7b6ad0673e3440c91.png)
an implicit feature mapping ![\\phi
(x)](//upload.wikimedia.org/math/0/f/c/0fc322eb39e7f8cfe71f7b568b4fba61.png)
from ![\\Omega
](//upload.wikimedia.org/math/b/9/d/b9d99db8626de63193c7fe96273a6cae.png)
to ![{\\mathcal
{H}}](//upload.wikimedia.org/math/c/6/d/c6d79f9e5dd728bae3a00468c627d341.png)
(which is therefore also called the feature space), so that ![\\
k(x,x')=\\langle \\phi (x),\\phi (x')\\rangle \_{{\\mathcal
{H}}}](//upload.wikimedia.org/math/a/6/4/a64897e7d3983fe9e7762d1bc534ba40.png)
can be viewed as a measure of similarity between points ![x,x'\\in
\\Omega
](//upload.wikimedia.org/math/c/5/a/c5af2b1d72cebcc147667c5406139b68.png).
While the similarity measure is linear in the feature space, it may be
highly nonlinear in the original space depending on the choice of
kernel.

### Kernel embedding[[edit](/w/index.php?title=Kernel_embedding_of_distributions&action=edit&section=2 "Edit section: Kernel embedding")]

The kernel embedding of the distribution
![P(X)](//upload.wikimedia.org/math/0/c/3/0c3d72395d7576ab13b9e9389f865960.png)
in ![{\\mathcal
{H}}](//upload.wikimedia.org/math/c/6/d/c6d79f9e5dd728bae3a00468c627d341.png)
(also called the **kernel mean** or **mean map**) is given
by:^[[1]](#cite_note-Smola2007-1)^

![\\mu \_{X}:={\\mathbb {E}}\_{X}[k(X,\\cdot )]={\\mathbb
{E}}\_{X}[\\phi (X)]=\\int \_{\\Omega }\\phi (x)\\ {\\mathrm
{d}}P(x)](//upload.wikimedia.org/math/d/c/d/dcd234e29a5e99730633dfe930ffe63f.png)

A kernel is *characteristic* if the embedding ![\\mu :\\Omega \\mapsto
{\\mathcal
{H}}](//upload.wikimedia.org/math/b/b/5/bb5a733dae50aa41d5b05349354031a0.png)
is one-to-one. Each distribution can thus be uniquely represented in the
RKHS and all statistical features of distributions are preserved by the
kernel embedding if a characteristic kernel is used.

### Empirical kernel embedding[[edit](/w/index.php?title=Kernel_embedding_of_distributions&action=edit&section=3 "Edit section: Empirical kernel embedding")]

Given
![n](//upload.wikimedia.org/math/7/b/8/7b8b965ad4bca0e41ab51de7b31363a1.png)
training examples ![\\{x\_{1},\\dots
,x\_{n}\\}](//upload.wikimedia.org/math/d/f/f/dff62703edcd0f89897fb48d31fc8555.png)
drawn [independently and identically
distributed](/wiki/Independent_and_identically_distributed_random_variables "Independent and identically distributed random variables")
(i.i.d.) from
![P](//upload.wikimedia.org/math/4/4/c/44c29edb103a2872f519ad0c9a0fdaaa.png),
the kernel embedding of
![P](//upload.wikimedia.org/math/4/4/c/44c29edb103a2872f519ad0c9a0fdaaa.png)
can be empirically estimated as

![\\widehat {\\mu }\_{X}={\\frac {1}{n}}\\sum \_{{i=1}}\^{n}\\phi
(x\_{i})](//upload.wikimedia.org/math/4/c/f/4cf03393de8da3849a460a81bda9f895.png)

### Joint distribution embedding[[edit](/w/index.php?title=Kernel_embedding_of_distributions&action=edit&section=4 "Edit section: Joint distribution embedding")]

If
![Y](//upload.wikimedia.org/math/5/7/c/57cec4137b614c87cb4e24a3d003a3e0.png)
denotes another random variable (for simplicity, assume the domain of
![Y](//upload.wikimedia.org/math/5/7/c/57cec4137b614c87cb4e24a3d003a3e0.png)
is also ![\\Omega
](//upload.wikimedia.org/math/b/9/d/b9d99db8626de63193c7fe96273a6cae.png)
with the same kernel
![k](//upload.wikimedia.org/math/8/c/e/8ce4b16b22b58894aa86c421e8759df3.png)
which satisfies ![\\langle \\phi (x)\\otimes \\phi (y),\\phi
(x')\\otimes \\phi (y')\\rangle =k(x,x')\\otimes
k(y,y')](//upload.wikimedia.org/math/d/5/b/d5bb9d9902c584db8ed0134ec424a671.png)),
then the [joint
distribution](/wiki/Joint_probability_distribution "Joint probability distribution")
![P(X,Y)](//upload.wikimedia.org/math/0/8/7/0876eb74d7563c0a832dff7c7998bd50.png)
can be mapped into a [tensor
product](/wiki/Tensor_product "Tensor product") feature space
![{\\mathcal {H}}\\otimes {\\mathcal
{H}}](//upload.wikimedia.org/math/c/c/5/cc5fbaa81dd44d8acc37dd7ef6c0c221.png)
via ^[[2]](#cite_note-Song2013-2)^

![{\\mathcal {C}}\_{{XY}}={\\mathbb {E}}\_{{XY}}[\\phi (X)\\otimes \\phi
(Y)]=\\int \_{{\\Omega \\times \\Omega }}\\phi (x)\\otimes \\phi (y)\\
{\\mathrm
{d}}P(x,y)](//upload.wikimedia.org/math/d/3/2/d32cba36ed4f3320a04bb579e532f555.png)

By the equivalence between a [tensor](/wiki/Tensor "Tensor") and a
[linear map](/wiki/Linear_map "Linear map"), this joint embedding may be
interpreted as an uncentered
[cross-covariance](/wiki/Cross-covariance "Cross-covariance") operator
![{\\mathcal {C}}\_{{XY}}:{\\mathcal {H}}\\mapsto {\\mathcal
{H}}](//upload.wikimedia.org/math/3/9/1/39129147d17577b213ef55a98ea7a2b5.png)
from which the cross-covariance of mean-zero functions ![f,g\\in
{\\mathcal
{H}}](//upload.wikimedia.org/math/8/0/6/8065b849be25a5eaaa87de5528913b5e.png)
can be computed as ^[[6]](#cite_note-SongCDE-6)^

![{\\text{Cov}}\_{{XY}}(h(X),g(Y)):={\\mathbb
{E}}\_{{XY}}[f(X)g(Y)]=\\langle f,{\\mathcal {C}}\_{{XY}}g\\rangle
\_{{{\\mathcal {H}}}}=\\langle f\\otimes g,{\\mathcal
{C}}\_{{XY}}\\rangle \_{{{\\mathcal {H}}\\otimes {\\mathcal
{H}}}}](//upload.wikimedia.org/math/9/e/0/9e027227594870dacc7b7e89c9688649.png)

Given
![n](//upload.wikimedia.org/math/7/b/8/7b8b965ad4bca0e41ab51de7b31363a1.png)
pairs of training examples ![\\{(x\_{1},y\_{1}),\\dots
,(x\_{n},y\_{n})\\}](//upload.wikimedia.org/math/2/6/5/26543b2c0626e475e1ac2948760dc83d.png)
drawn i.i.d. from
![P](//upload.wikimedia.org/math/4/4/c/44c29edb103a2872f519ad0c9a0fdaaa.png),
we can also empirically estimate the joint distribution kernel embedding
via

![\\widehat {{\\mathcal {C}}}\_{{XY}}={\\frac {1}{n}}\\sum
\_{{i=1}}\^{n}\\phi (x\_{i})\\otimes \\phi
(y\_{i})](//upload.wikimedia.org/math/7/1/0/710a37ab3ac013cbc8eb26fcbac1455d.png)

### Conditional distribution embedding[[edit](/w/index.php?title=Kernel_embedding_of_distributions&action=edit&section=5 "Edit section: Conditional distribution embedding")]

Given a [conditional
distribution](/wiki/Conditional_distribution "Conditional distribution")
![P(Y\\mid
X)](//upload.wikimedia.org/math/a/8/c/a8c14bbd3415899e56075e656f90cb72.png),
one can define the corresponding RKHS embedding as
^[[2]](#cite_note-Song2013-2)^

![\\mu \_{{Y\\mid x}}={\\mathbb {E}}\_{{Y\\mid x}}[\\phi (Y)]=\\int
\_{\\Omega }\\phi (y)\\ {\\mathrm {d}}P(y\\mid
x)](//upload.wikimedia.org/math/d/6/2/d6280bc66bfce484f710ed17050dd9ea.png)

Note that the embedding of ![P(Y\\mid
X)](//upload.wikimedia.org/math/a/8/c/a8c14bbd3415899e56075e656f90cb72.png)
thus defines a family of points in the RKHS indexed by the values
![x](//upload.wikimedia.org/math/9/d/d/9dd4e461268c8034f5c8564e155c67a6.png)
taken by conditioning variable
![X](//upload.wikimedia.org/math/0/2/1/02129bb861061d1a052c592e2dc6b383.png).
By fixing
![X](//upload.wikimedia.org/math/0/2/1/02129bb861061d1a052c592e2dc6b383.png)
to a particular value, we obtain a single element in ![{\\mathcal
{H}}](//upload.wikimedia.org/math/c/6/d/c6d79f9e5dd728bae3a00468c627d341.png),
and thus it is natural to define the operator

![{\\mathcal {C}}\_{{Y\\mid X}}:{\\mathcal {H}}\\mapsto {\\mathcal
{H}}](//upload.wikimedia.org/math/0/8/7/0875966e99df5eb0a83e09fb5cb8a985.png)
as ![{\\mathcal {C}}\_{{Y\\mid X}}={\\mathcal {C}}\_{{YX}}{\\mathcal
{C}}\_{{XX}}\^{{-1}}](//upload.wikimedia.org/math/d/a/0/da0a20010267c28073c4203d982390f8.png)

which given the feature mapping of
![x](//upload.wikimedia.org/math/9/d/d/9dd4e461268c8034f5c8564e155c67a6.png)
outputs the conditional embedding of
![Y](//upload.wikimedia.org/math/5/7/c/57cec4137b614c87cb4e24a3d003a3e0.png)
given
![X=x](//upload.wikimedia.org/math/2/e/f/2efd923ff714e805035f50f75aeed596.png).
Assuming that for all ![g\\in {\\mathcal {H}}:\\ {\\mathbb
{E}}\_{{Y\\mid X}}[g(Y)]\\in {\\mathcal
{H}}](//upload.wikimedia.org/math/a/9/1/a917d3700bb6fef5d032c759a44130d6.png),
it can be shown that ^[[6]](#cite_note-SongCDE-6)^

![\\mu \_{{Y\\mid x}}={\\mathcal {C}}\_{{Y\\mid X}}\\phi
(x)](//upload.wikimedia.org/math/0/d/c/0dca9b01f439acdc3e87b0382a89b9e3.png)

This assumption is always true for finite domains with characteristic
kernels, but may not necessarily hold for continuous
domains.^[[2]](#cite_note-Song2013-2)^ Nevertheless, even in cases where
the assumption fails, ![{\\mathcal {C}}\_{{Y\\mid X}}\\phi
(x)](//upload.wikimedia.org/math/0/c/0/0c01e2698716883a65fb5443d6c1e25f.png)
may still be used to approximate the conditional kernel embedding ![\\mu
\_{{Y\\mid
x}}](//upload.wikimedia.org/math/8/5/5/8550cd233e2e69c8e8940c3f3dad5e92.png),
and in practice, the inversion operator is replaced with a regularized
version of itself ![({\\mathcal {C}}\_{{XX}}+\\lambda {\\mathbf
{I}})\^{{-1}}](//upload.wikimedia.org/math/8/9/8/8983fe0a638f63904e7180eb9e309148.png)
(where ![{\\mathbf
{I}}](//upload.wikimedia.org/math/8/7/1/87189fa9d70bf15da548f2de5a8b1a70.png)
denotes the [identity matrix](/wiki/Identity_matrix "Identity matrix")).

Given training examples ![\\{(x\_{1},y\_{1}),\\dots
,(x\_{n},y\_{n})\\}](//upload.wikimedia.org/math/2/6/5/26543b2c0626e475e1ac2948760dc83d.png),
the empirical kernel conditional embedding operator may be estimated as
^[[2]](#cite_note-Song2013-2)^

![\\widehat {C}\_{{Y\\mid X}}={\\boldsymbol {\\Phi }}({\\mathbf
{K}}+\\lambda {\\mathbf {I}})\^{{-1}}{\\boldsymbol {\\Upsilon
}}\^{T}](//upload.wikimedia.org/math/e/d/e/ede3d669c07cf4dfea9bca795951c4d2.png)

where ![{\\boldsymbol {\\Phi }}=\\left(\\phi (y\_{i}),\\dots
,(y\_{n})\\right),{\\boldsymbol {\\Upsilon }}=\\left(\\phi
(x\_{i}),\\dots
,(x\_{n})\\right)](//upload.wikimedia.org/math/0/5/b/05b4d9aca79d537a519da72fa1d90f17.png)
are implicitly formed feature matrices, ![{\\mathbf {K}}={\\boldsymbol
{\\Upsilon }}\^{T}{\\boldsymbol {\\Upsilon
}}](//upload.wikimedia.org/math/a/6/7/a676f01442bc3a22f85adf1aa24917c0.png)
is the Gram matrix for samples of
![X](//upload.wikimedia.org/math/0/2/1/02129bb861061d1a052c592e2dc6b383.png),
and ![\\lambda
](//upload.wikimedia.org/math/e/0/5/e05a30d96800384dd38b22851322a6b5.png)
is a
[regularization](/wiki/Regularization_(mathematics) "Regularization (mathematics)")
parameter needed to avoid
[overfitting](/wiki/Overfitting "Overfitting").

Thus, the empirical estimate of the kernel conditional embedding is
given by a weighted sum of samples of
![Y](//upload.wikimedia.org/math/5/7/c/57cec4137b614c87cb4e24a3d003a3e0.png)
in the feature space:

![\\widehat {\\mu }\_{{Y\\mid x}}=\\sum \_{{i=1}}\^{n}\\beta
\_{i}(x)\\phi (y\_{i})={\\boldsymbol {\\Phi }}{\\boldsymbol {\\beta
}}(x)](//upload.wikimedia.org/math/4/b/a/4ba8417b140284d2aaa46b71fd66e61b.png)
where ![{\\boldsymbol {\\beta }}(x)=({\\mathbf {K}}+\\lambda {\\mathbf
{I}})\^{{-1}}{\\mathbf
{K}}\_{x}](//upload.wikimedia.org/math/b/6/1/b615bbda929e84b7e042031082741392.png)
and ![{\\mathbf {K}}\_{x}=\\left(k(x\_{1},x),\\dots
,k(x\_{n},x)\\right)\^{T}](//upload.wikimedia.org/math/3/e/4/3e432cefc8de0c51c1a16f1526eae167.png)

Properties[[edit](/w/index.php?title=Kernel_embedding_of_distributions&action=edit&section=6 "Edit section: Properties")]
-------------------------------------------------------------------------------------------------------------------------

-   The expectation of any function
    ![f](//upload.wikimedia.org/math/8/f/a/8fa14cdd754f91cc6554c9e71929cce7.png)
    in the RKHS can be computed as an inner product with the kernel
    embedding:

![{\\mathbb {E}}\_{X}[f(X)]=\\langle f,\\mu \_{X}\\rangle \_{{\\mathcal
{H}}}](//upload.wikimedia.org/math/4/9/d/49dace8e278af57a4aa4a34a097d2f79.png)

-   In the presence of large sample sizes, manipulations of the
    ![n\\times
    n](//upload.wikimedia.org/math/6/0/7/607acaa73c762411b20745149a11e90b.png)
    Gram matrix may be computationally demanding. Through use of a
    low-rank approximation of the Gram matrix (such as the [incomplete
    Cholesky
    factorization](/wiki/Incomplete_Cholesky_factorization "Incomplete Cholesky factorization")),
    running time and memory requirements of kernel-embedding-based
    learning algorithms can be drastically reduced without suffering
    much loss in approximation accuracy.^[[2]](#cite_note-Song2013-2)^

### Convergence of empirical kernel mean to the true distribution embedding[[edit](/w/index.php?title=Kernel_embedding_of_distributions&action=edit&section=7 "Edit section: Convergence of empirical kernel mean to the true distribution embedding")]

-   If
    ![k](//upload.wikimedia.org/math/8/c/e/8ce4b16b22b58894aa86c421e8759df3.png)
    is defined such that ![f\\in
    [0,1]](//upload.wikimedia.org/math/1/6/3/163e77cbb453d30858045840b6cc429b.png)
    for all ![f\\in {\\mathcal
    {H}}](//upload.wikimedia.org/math/2/4/0/240fdcb31e3ff48c1414245e7e34acf5.png)
    with ![||f||\_{{\\mathcal {H}}}\\leq
    1](//upload.wikimedia.org/math/b/e/5/be58bca7bcbcae4494704b97ce34b3f6.png)
    (as is the case for the widely-used [radial basis
    function](/wiki/Radial_basis_function "Radial basis function")
    kernels), then with probability at least ![\\ 1-\\delta
    ](//upload.wikimedia.org/math/b/b/2/bb23483a8bdb5afb7306909c3b193c97.png):^[[5]](#cite_note-SongThesis-5)^\
     ![||\\mu \_{X}-\\widehat {\\mu }\_{X}||\_{{\\mathcal {H}}}=\\sup
    \_{{f\\in {\\mathcal {B}}(0,1)}}\\left|{\\mathbb
    {E}}\_{X}[f(X)]-{\\frac {1}{n}}\\sum
    \_{{i=1}}\^{n}f(x\_{i})\\right|\\leq {\\frac {2}{n}}{\\mathbb
    {E}}\_{X}\\left[{\\sqrt {{\\text{tr}}K}}\\right]+{\\sqrt {{\\frac
    {\\log(2/\\delta
    )}{2n}}}}](//upload.wikimedia.org/math/f/9/e/f9e1faf3c2387eaa37e003f9e9471deb.png)\
     where ![{\\mathcal
    {B}}(0,1)](//upload.wikimedia.org/math/c/0/8/c08cb8c278ae4948ddd547e75d54873c.png)
    denotes the unit ball in ![{\\mathcal
    {H}}](//upload.wikimedia.org/math/c/6/d/c6d79f9e5dd728bae3a00468c627d341.png)
    and ![{\\mathbf
    {K}}](//upload.wikimedia.org/math/8/f/c/8fc676647a267baca4a6d4d9144d5de6.png)
    is the Gram matrix whose
    ![i,j](//upload.wikimedia.org/math/e/e/8/ee813f0ede8664a8049b1b6720f03b60.png)th
    entry is
    ![k(x\_{i},x\_{j})](//upload.wikimedia.org/math/1/2/e/12efba6154ab7719ed9a4e756c4935ee.png).
-   The rate of convergence (in RKHS norm) of the empirical kernel
    embedding to its distribution counterpart is
    ![O(n\^{{-1/2}})](//upload.wikimedia.org/math/e/e/f/eef545f8107a92b52f2fb52083ea0f87.png)
    and does *not* depend on the dimension of
    ![X](//upload.wikimedia.org/math/0/2/1/02129bb861061d1a052c592e2dc6b383.png).
-   Statistics based on kernel embeddings thus avoid the [curse of
    dimensionality](/wiki/Curse_of_dimensionality "Curse of dimensionality"),
    and though the true underlying distribution is unknown in practice,
    one can (with high probability) obtain an approximation within
    ![O(n\^{{-1/2}})](//upload.wikimedia.org/math/e/e/f/eef545f8107a92b52f2fb52083ea0f87.png)
    of the true kernel embedding based on a finite sample of size
    ![n](//upload.wikimedia.org/math/7/b/8/7b8b965ad4bca0e41ab51de7b31363a1.png).
-   For the embedding of conditional distributions, the empirical
    estimate can be seen as a *weighted* average of feature mappings
    (where the weights ![\\beta
    \_{i}(x)](//upload.wikimedia.org/math/b/6/0/b60f7ca426ddd4fed6e37fcbf9ca6e45.png)
    depend on the value of the conditioning variable and capture the
    effect of the conditioning on the kernel embedding). In this case,
    the empirical estimate converges to the conditional distribution
    RKHS embedding with rate
    ![O\\left(n\^{{-1/4}}\\right)](//upload.wikimedia.org/math/d/2/6/d2670629e9498279ef85475f86869056.png)
    if the regularization parameter ![\\lambda
    ](//upload.wikimedia.org/math/e/0/5/e05a30d96800384dd38b22851322a6b5.png)
    is decreased as
    ![O\\left(n\^{{-1/2}}\\right)](//upload.wikimedia.org/math/b/5/9/b592b328fd294d94896f78b06fd3afc7.png),
    though faster rates of convergence may be achieved by placing
    additional assumptions on the joint
    distribution.^[[2]](#cite_note-Song2013-2)^

### Universal kernels[[edit](/w/index.php?title=Kernel_embedding_of_distributions&action=edit&section=8 "Edit section: Universal kernels")]

-   Letting ![C({\\mathcal
    {X}})](//upload.wikimedia.org/math/1/a/9/1a99722d28a00555295ac264c436b99b.png)
    denote the space of
    [continuous](/wiki/Continuous_function "Continuous function")
    [bounded](/wiki/Bounded_function "Bounded function") functions on
    [compact](/wiki/Compact_space "Compact space") domain ![{\\mathcal
    {X}}](//upload.wikimedia.org/math/1/e/2/1e2f84683f37d023e6a14e6b74602196.png),
    we call a kernel
    ![k](//upload.wikimedia.org/math/8/c/e/8ce4b16b22b58894aa86c421e8759df3.png)
    *universal* if ![k(x,\\cdot
    )](//upload.wikimedia.org/math/d/5/d/d5d49ea01f80b4a50eb353f877bc65bf.png)
    is continuous for all
    ![x](//upload.wikimedia.org/math/9/d/d/9dd4e461268c8034f5c8564e155c67a6.png)
    and the RKHS induced by
    ![k](//upload.wikimedia.org/math/8/c/e/8ce4b16b22b58894aa86c421e8759df3.png)
    is [dense](/wiki/Dense_set "Dense set") in ![C({\\mathcal
    {X}})](//upload.wikimedia.org/math/1/a/9/1a99722d28a00555295ac264c436b99b.png).

-   If
    ![k](//upload.wikimedia.org/math/8/c/e/8ce4b16b22b58894aa86c421e8759df3.png)
    induces a strictly positive definite kernel matrix for any set of
    distinct points, then it is a universal
    kernel.^[[5]](#cite_note-SongThesis-5)^ For example, the widely-used
    Gaussian RBF kernel

![k(x,x')=\\exp \\left(-{\\frac {1}{2\\sigma
\^{2}}}||x-x'||\^{2}\\right)](//upload.wikimedia.org/math/1/1/4/1140fefadf5af18952d362eeee2c46b3.png)

on compact subsets of ![{\\mathbb
{R}}\^{d}](//upload.wikimedia.org/math/5/e/8/5e8bb29789ff92c1fa703ebf600d64b9.png)
is universal.

-   If
    ![k](//upload.wikimedia.org/math/8/c/e/8ce4b16b22b58894aa86c421e8759df3.png)
    is universal, then it is *characteristic*, i.e. the kernel embedding
    is one-to-one.^[[7]](#cite_note-7)^

### Parameter selection for conditional distribution kernel embeddings[[edit](/w/index.php?title=Kernel_embedding_of_distributions&action=edit&section=9 "Edit section: Parameter selection for conditional distribution kernel embeddings")]

-   The empirical kernel conditional distribution embedding operator
    ![\\widehat {{\\mathcal
    {C}}}\_{{Y|X}}](//upload.wikimedia.org/math/4/e/2/4e2eeb2d6744ab076266d240394b2634.png)
    can alternatively be viewed as the solution of the following
    regularized least squares (function-valued) regression problem
    ^[[8]](#cite_note-8)^

![\\min \_{{{\\mathcal {C}}:{\\mathcal {H}}\\mapsto {\\mathcal
{H}}}}\\sum \_{{i=1}}\^{n}||\\phi (y\_{i})-{\\mathcal {C}}\\phi
(x\_{i})||\_{{\\mathcal {H}}}\^{2}+\\lambda ||{\\mathcal
{C}}||\_{{HS}}\^{2}](//upload.wikimedia.org/math/0/2/a/02ac240fae58b332a7d80723df585496.png)
where ![||\\cdot
||\_{{HS}}](//upload.wikimedia.org/math/d/3/0/d302f2b73cd2847bc2c03dfcebc1499f.png)
is the [Hilbert-Schmidt
norm](/wiki/Hilbert%E2%80%93Schmidt_operator "Hilbert–Schmidt operator").

-   One can thus select the regularization parameter ![\\lambda
    ](//upload.wikimedia.org/math/e/0/5/e05a30d96800384dd38b22851322a6b5.png)
    by performing
    [cross-validation](/wiki/Cross-validation_(statistics) "Cross-validation (statistics)")
    based on the squared loss function of the regression problem.

Rules of probability as operations in the RKHS[[edit](/w/index.php?title=Kernel_embedding_of_distributions&action=edit&section=10 "Edit section: Rules of probability as operations in the RKHS")]
--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

This section illustrates how basic probabilistic rules may be
reformulated as (multi)linear algebraic operations in the kernel
embedding framework and is primarily based on the work of Song et
al.^[[2]](#cite_note-Song2013-2)^^[[6]](#cite_note-SongCDE-6)^ The
following notation is adopted:

-   ![P(X,Y)=](//upload.wikimedia.org/math/1/3/c/13c947e0b75124b799d0a9d896cb042c.png)
    joint distribution over random variables
    ![X,Y](//upload.wikimedia.org/math/d/2/3/d23a4ce8bca0f4891e037439a79b45a6.png)
-   ![P(X)=\\int \_{\\Omega }P(X,{\\mathrm
    {d}}y)=](//upload.wikimedia.org/math/e/3/e/e3eee01a025f75953a3862c77ceb54a9.png)
    marginal distribution of
    ![X](//upload.wikimedia.org/math/0/2/1/02129bb861061d1a052c592e2dc6b383.png);
    ![P(Y)=](//upload.wikimedia.org/math/b/a/6/ba61dc94d631dc50128d5c4b9168379e.png)
    marginal distribution of
    ![Y](//upload.wikimedia.org/math/5/7/c/57cec4137b614c87cb4e24a3d003a3e0.png)
-   ![P(Y\\mid X)={\\frac
    {P(X,Y)}{P(X)}}=](//upload.wikimedia.org/math/4/9/5/4954b19b29a8a224156a280892ff31e5.png)
    conditional distribution of
    ![Y](//upload.wikimedia.org/math/5/7/c/57cec4137b614c87cb4e24a3d003a3e0.png)
    given
    ![X](//upload.wikimedia.org/math/0/2/1/02129bb861061d1a052c592e2dc6b383.png)
    with corresponding conditional embedding operator ![{\\mathcal
    {C}}\_{{Y\\mid
    X}}](//upload.wikimedia.org/math/d/a/9/da9ff6d87576fcc52ab9b050e1463ef4.png)
-   ![\\pi
    (Y)=](//upload.wikimedia.org/math/c/b/4/cb491be88e42bb46276ef01ef886a39a.png)
    prior distribution over
    ![Y](//upload.wikimedia.org/math/5/7/c/57cec4137b614c87cb4e24a3d003a3e0.png)
-   ![Q](//upload.wikimedia.org/math/f/0/9/f09564c9ca56850d4cd6b3319e541aee.png)
    is used to distinguish distributions which incorporate the prior
    from distributions
    ![P](//upload.wikimedia.org/math/4/4/c/44c29edb103a2872f519ad0c9a0fdaaa.png)
    which do not rely on the prior

In practice, all embeddings are empirically estimated from data
![\\{(x\_{1},y\_{1}),\\dots
,(x\_{n},y\_{n})\\}](//upload.wikimedia.org/math/2/6/5/26543b2c0626e475e1ac2948760dc83d.png)
and it assumed that a set of samples ![\\{\\widetilde {y}\_{1},\\dots
,\\widetilde {y}\_{{\\widetilde
{n}}}\\}](//upload.wikimedia.org/math/5/a/5/5a5bb0321cf6f04fee6d776c2f39771d.png)
may be used to estimate the kernel embedding of the prior distribution
![\\pi
(Y)](//upload.wikimedia.org/math/2/c/9/2c9d3e6597961778406996b23b9698f3.png).

### Kernel sum rule[[edit](/w/index.php?title=Kernel_embedding_of_distributions&action=edit&section=11 "Edit section: Kernel sum rule")]

In probability theory, the marginal distribution of
![X](//upload.wikimedia.org/math/0/2/1/02129bb861061d1a052c592e2dc6b383.png)
can be computed by integrating out
![Y](//upload.wikimedia.org/math/5/7/c/57cec4137b614c87cb4e24a3d003a3e0.png)
from the joint density (including the prior distribution on
![Y](//upload.wikimedia.org/math/5/7/c/57cec4137b614c87cb4e24a3d003a3e0.png))

![Q(X)=\\int \_{\\Omega }P(X\\mid Y){\\mathrm {d}}\\pi
(Y)](//upload.wikimedia.org/math/2/2/e/22e6bf0d803590b04c663eb65d0a24c6.png)

The analog of this rule in the kernel embedding framework states that
![\\mu \_{X}\^{\\pi
}](//upload.wikimedia.org/math/e/c/c/eccc232a500a674fbd5875f0c3449e65.png),
the RKHS embedding of
![Q(X)](//upload.wikimedia.org/math/2/6/f/26f917583380478b1ed798e53434a501.png),
can be computed via

![\\mu \_{X}\^{\\pi }={\\mathbb {E}}\_{{Y}}[{\\mathcal {C}}\_{{X\\mid
Y}}\\phi (Y)]={\\mathcal {C}}\_{{X\\mid Y}}{\\mathbb {E}}\_{{Y}}[\\phi
(Y)]={\\mathcal {C}}\_{{X\\mid Y}}\\mu \_{Y}\^{\\pi
}](//upload.wikimedia.org/math/0/b/e/0be54b48442576a96c7136962ee40481.png)
where ![\\mu \_{Y}\^{\\pi
}](//upload.wikimedia.org/math/3/3/a/33a05547db744b3aaf3fc9e409bcbb69.png)
is the kernel embedding of ![\\pi
(Y)](//upload.wikimedia.org/math/2/c/9/2c9d3e6597961778406996b23b9698f3.png)

In practical implementations, the kernel sum rule takes the following
form

![\\widehat {\\mu }\_{X}\^{\\pi }=\\widehat {{\\mathcal {C}}}\_{{X\\mid
Y}}\\widehat {\\mu }\_{Y}\^{\\pi }={\\boldsymbol {\\Upsilon
}}(G+\\lambda {\\mathbf {I}})\^{{-1}}\\widetilde {{\\boldsymbol
{G}}}{\\boldsymbol {\\alpha
}}](//upload.wikimedia.org/math/4/6/d/46d7aa9f4684fd139b3f02f18c9346e2.png)

where ![\\mu \_{Y}\^{\\pi }=\\sum \_{{i=1}}\^{{\\widetilde {n}}}\\alpha
\_{i}\\phi (\\widetilde
{y}\_{i})](//upload.wikimedia.org/math/a/9/a/a9a0340a073ce64178ca297a0a90aa5a.png)
is the empirical kernel embedding of the prior distribution,
![{\\boldsymbol {\\alpha }}=(\\alpha \_{1},\\dots ,\\alpha
\_{{\\widetilde
{n}}})\^{T}](//upload.wikimedia.org/math/f/1/d/f1d316696e5e1a2b4ae93f53cbb769d9.png),
![{\\boldsymbol {\\Upsilon }}=\\left(\\phi (x\_{1}),\\dots ,\\phi
(x\_{n})\\right)](//upload.wikimedia.org/math/a/3/3/a33317aa0e15ba2d2b7c072e9bc1bfe4.png),
and ![{\\mathbf {G}},\\widetilde {{\\mathbf
{G}}}](//upload.wikimedia.org/math/2/b/3/2b3016c473a169c0adb7eeb9ac4705fe.png)
are Gram matrices with entries ![{\\mathbf
{G}}\_{{ij}}=k(y\_{i},y\_{j}),\\widetilde {{\\mathbf
{G}}}\_{{ij}}=k(y\_{i},\\widetilde
{y}\_{j})](//upload.wikimedia.org/math/4/5/7/457f7c55ae0b726b86b5f95ae7b1f27e.png)
respectively.

### Kernel chain rule[[edit](/w/index.php?title=Kernel_embedding_of_distributions&action=edit&section=12 "Edit section: Kernel chain rule")]

In probability theory, a joint distribution can be factorized into a
product between conditional and marginal distributions

![Q(X,Y)=P(X\\mid Y)\\pi
(Y)](//upload.wikimedia.org/math/4/c/7/4c7f56f1e542eae045e67e49d3a0c227.png)

The analog of this rule in the kernel embedding framework states that
![{\\mathcal {C}}\_{{XY}}\^{\\pi
}](//upload.wikimedia.org/math/c/9/c/c9ca1a7224768afcf15ae26bb348418b.png),
the joint embedding of
![Q(X,Y)](//upload.wikimedia.org/math/8/6/1/8615d357d957a94455eba454f5448fb5.png),
can be factorized as a composition of conditional embedding operator
with the auto-covariance operator associated with ![\\pi
(Y)](//upload.wikimedia.org/math/2/c/9/2c9d3e6597961778406996b23b9698f3.png)

![{\\mathcal {C}}\_{{XY}}\^{\\pi }={\\mathcal {C}}\_{{X\\mid
Y}}{\\mathcal {C}}\_{{YY}}\^{\\pi
}](//upload.wikimedia.org/math/c/f/5/cf5809caac676943398e0b6f37738476.png)
where ![{\\mathcal {C}}\_{{XY}}\^{\\pi }={\\mathbb {E}}\_{{XY}}[\\phi
(X)\\otimes \\phi
(Y)]](//upload.wikimedia.org/math/2/f/4/2f49ff6ca52ffd14332283a42bc8ae6f.png)
and ![{\\mathcal {C}}\_{{YY}}\^{\\pi }={\\mathbb {E}}\_{Y}[\\phi
(Y)\\otimes \\phi
(Y)]](//upload.wikimedia.org/math/6/e/d/6ed51e763e5ede464e9890874fbf2d5c.png)

In practical implementations, the kernel chain rule takes the following
form

![\\widehat {{\\mathcal {C}}}\_{{XY}}\^{\\pi }=\\widehat {{\\mathcal
{C}}}\_{{X\\mid Y}}\\widehat {{\\mathcal {C}}}\_{{YY}}\^{\\pi
}={\\boldsymbol {\\Upsilon }}({\\mathbf {G}}+\\lambda {\\mathbf
{I}})\^{{-1}}\\widetilde {{\\mathbf {G}}}{\\text{diag}}({\\boldsymbol
{\\alpha }}){\\boldsymbol {\\Phi
}}\^{T}](//upload.wikimedia.org/math/7/f/f/7ffadbe0b54e0b69411af3290241ea3e.png)

### Kernel Bayes' rule[[edit](/w/index.php?title=Kernel_embedding_of_distributions&action=edit&section=13 "Edit section: Kernel Bayes' rule")]

In probability theory, a posterior distribution can be expressed in
terms of a prior distribution and a likelihood function as

![Q(Y\\mid x)={\\frac {P(x\\mid Y)\\pi
(Y)}{Q(x)}}](//upload.wikimedia.org/math/7/d/6/7d62e9d69891260212c37aa078186053.png)
where ![Q(x)=\\int \_{\\Omega }P(x\\mid y){\\mathrm {d}}\\pi
(y)](//upload.wikimedia.org/math/8/2/7/82729503e03132429b3a3a841786a457.png)

The analog of this rule in the kernel embedding framework expresses the
kernel embedding of the conditional distribution in terms of conditional
embedding operators which are modified by the prior distribution

![\\mu \_{{Y\\mid x}}\^{\\pi }={\\mathcal {C}}\_{{Y\\mid X}}\^{\\pi
}\\phi (x)={\\mathcal {C}}\_{{YX}}\^{\\pi }({\\mathcal
{C}}\_{{XX}}\^{\\pi })\^{{-1}}\\phi
(x)](//upload.wikimedia.org/math/2/a/5/2a527350f096199c82980868bd3878e2.png)
where from the chain rule: ![{\\mathcal {C}}\_{{YX}}\^{\\pi
}=\\left({\\mathcal {C}}\_{{X\\mid Y}}{\\mathcal {C}}\_{{YY}}\^{\\pi
}\\right)\^{T}](//upload.wikimedia.org/math/9/4/8/9486f8541a24e23f598a8837a8757e08.png).

In practical implementations, the kernel Bayes' rule takes the following
form

![\\widehat {\\mu }\_{{Y\\mid x}}\^{\\pi }=\\widehat {{\\mathcal
{C}}}\_{{YX}}\^{\\pi }\\left((\\widehat {{\\mathcal
{C}}}\_{{XX}})\^{2}+\\widetilde {\\lambda }{\\mathbf
{I}}\\right)\^{{-1}}\\widehat {{\\mathcal {C}}}\_{{XX}}\^{\\pi }\\phi
(x)=\\widetilde {{\\boldsymbol {\\Phi }}}{\\boldsymbol {\\Lambda
}}\^{T}\\left(({\\mathbf {D}}{\\mathbf {K}})\^{2}+\\widetilde {\\lambda
}{\\mathbf {I}}\\right)\^{{-1}}{\\mathbf {K}}{\\mathbf {D}}{\\mathbf
{K}}\_{x}](//upload.wikimedia.org/math/0/0/4/004b5529279952dcd5edf7b1223261f4.png)

where ![{\\boldsymbol {\\Lambda }}=\\left({\\mathbf {G}}+\\widetilde
{\\lambda }{\\mathbf {I}}\\right)\^{{-1}}\\widetilde {{\\mathbf
{G}}}{\\text{diag}}({\\boldsymbol {\\alpha }}),{\\mathbf
{D}}={\\text{diag}}\\left(\\left({\\mathbf {G}}+\\widetilde {\\lambda
}{\\mathbf {I}}\\right)\^{{-1}}\\widetilde {{\\mathbf {G}}}{\\boldsymbol
{\\alpha
}}\\right)](//upload.wikimedia.org/math/b/3/8/b38ffb1c2d48b311219644792bacaf30.png).
Two regularization parameters are used in this framework: ![\\lambda
](//upload.wikimedia.org/math/e/0/5/e05a30d96800384dd38b22851322a6b5.png)
for the estimation of ![\\widehat {{\\mathcal {C}}}\_{{YX}}\^{\\pi
},\\widehat {{\\mathcal {C}}}\_{{XX}}\^{\\pi }={\\boldsymbol {\\Upsilon
}}{\\mathbf {D}}{\\boldsymbol {\\Upsilon
}}\^{T}](//upload.wikimedia.org/math/4/8/9/4892df871f5e529cbb6373348e35dd19.png)
and ![\\widetilde {\\lambda
}](//upload.wikimedia.org/math/6/5/2/65206f18944503d805e7497d1a70ade1.png)
for the estimation of the final conditional embedding operator
![\\widehat {{\\mathcal {C}}}\_{{Y\\mid X}}\^{\\pi }=\\widehat
{{\\mathcal {C}}}\_{{YX}}\^{\\pi }\\left((\\widehat {{\\mathcal
{C}}}\_{{XX}}\^{\\pi })\^{2}+\\widetilde {\\lambda }{\\mathbf
{I}}\\right)\^{{-1}}\\widehat {{\\mathcal {C}}}\_{{XX}}\^{\\pi
}](//upload.wikimedia.org/math/f/f/e/ffe8be6fd7afdeafbfa0a4027b16e7ba.png).
The latter regularization is done on square of ![\\widehat {{\\mathcal
{C}}}\_{{XX}}\^{\\pi
}](//upload.wikimedia.org/math/6/5/9/65939861649786d0b67afa5b9ff556b1.png)
because
![D](//upload.wikimedia.org/math/f/6/2/f623e75af30e62bbd73d6df5b50bb7b5.png)
may not be [positive
definite](/wiki/Positive-definite_matrix "Positive-definite matrix").

Applications[[edit](/w/index.php?title=Kernel_embedding_of_distributions&action=edit&section=14 "Edit section: Applications")]
------------------------------------------------------------------------------------------------------------------------------

### Measuring distance between distributions[[edit](/w/index.php?title=Kernel_embedding_of_distributions&action=edit&section=15 "Edit section: Measuring distance between distributions")]

The **maximum mean discrepancy** (MMD) is a distance-measure between
distributions
![P(X)](//upload.wikimedia.org/math/0/c/3/0c3d72395d7576ab13b9e9389f865960.png)
and
![Q(Y)](//upload.wikimedia.org/math/f/e/8/fe8cb4db7acb95c83dd6f4b1819bbd96.png)
which is defined as the squared distance between their embeddings in the
RKHS ^[[5]](#cite_note-SongThesis-5)^

![{\\text{MMD}}(P,Q)=\\left|\\left|\\mu \_{X}-\\mu
\_{Y}\\right|\\right|\_{{{\\mathcal
{H}}}}\^{2}](//upload.wikimedia.org/math/4/9/6/4969a8b737f8d8f7298e17bdcf217355.png)

While most distance-measures between distributions such as the
widely-used [Kullback–Leibler
divergence](/wiki/Kullback%E2%80%93Leibler_divergence "Kullback–Leibler divergence")
either require density estimation (either parametrically or
nonparametrically) or space partitioning/bias correction
strategies,^[[5]](#cite_note-SongThesis-5)^ the MMD is easily estimated
as an empirical mean which is concentrated around the true value of the
MMD. The characterization of this distance as the *maximum mean
discrepancy* refers to the fact that computing the MMD is equivalent to
finding the RKHS function that maximizes the difference in expectations
between the two probability distributions

![{\\text{MMD}}(P,Q)=\\sup \_{{||f||\_{{\\mathcal {H}}}\\leq
1}}\\left({\\mathbb {E}}\_{X}[f(X)]-{\\mathbb
{E}}\_{Y}[f(Y)]\\right)](//upload.wikimedia.org/math/f/2/5/f25d395bc0e1b41b19d66bb61333b046.png)

### Kernel two sample test[[edit](/w/index.php?title=Kernel_embedding_of_distributions&action=edit&section=16 "Edit section: Kernel two sample test")]

Given *n* training examples from
![P(X)](//upload.wikimedia.org/math/0/c/3/0c3d72395d7576ab13b9e9389f865960.png)
and *m* samples from
![Q(Y)](//upload.wikimedia.org/math/f/e/8/fe8cb4db7acb95c83dd6f4b1819bbd96.png),
one can formulate a test statistic based on the empirical estimate of
the MMD

![\\widehat {{\\text{MMD}}}(P,Q)=\\left|\\left|{\\frac {1}{n}}\\sum
\_{{i=1}}\^{n}\\phi (x\_{i})-{\\frac {1}{m}}\\sum \_{{i=1}}\^{m}\\phi
(y\_{i})\\right|\\right|\_{{{\\mathcal {H}}}}\^{2}={\\frac {1}{nm}}\\sum
\_{{i=1}}\^{n}\\sum
\_{{j=1}}\^{m}\\left[k(x\_{i},x\_{j})+k(y\_{i},y\_{j})-2k(x\_{i},y\_{j})\\right]](//upload.wikimedia.org/math/e/9/f/e9fef41aae54f2a9194868199ef114db.png)

to obtain a **two-sample test** ^[[9]](#cite_note-9)^ of the null
hypothesis that both samples stem from the same distribution (i.e.
![P=Q](//upload.wikimedia.org/math/c/7/1/c714b467f91d14cf30bf304cb2479b38.png))
against the broad alternative ![P\\neq
Q](//upload.wikimedia.org/math/5/6/e/56e22bc2d415631a343d96e403ccb915.png).

### Density estimation via kernel embeddings[[edit](/w/index.php?title=Kernel_embedding_of_distributions&action=edit&section=17 "Edit section: Density estimation via kernel embeddings")]

Although learning algorithms in the kernel embedding framework
circumvent the need for intermediate density estimation, one may
nonetheless use the empirical embedding to perform density estimation
based on *n* samples drawn from an underlying distribution
![P\_{X}\^{\*}](//upload.wikimedia.org/math/7/a/1/7a137aa9ce49c6deab4d84f07783f135.png).
This can be done by solving the following optimization problem
^[[5]](#cite_note-SongThesis-5)^^[[10]](#cite_note-10)^

![\\max
\_{{P\_{X}}}H(P\_{X})](//upload.wikimedia.org/math/5/2/a/52a246eca4fbf0fd4f80dbc3fc98c414.png)
subject to ![||\\widehat {\\mu }\_{X}-\\mu \_{X}[P\_{X}]||\_{{\\mathcal
{H}}}\\leq \\epsilon
](//upload.wikimedia.org/math/e/f/9/ef93042c4f923801ee61c92f2cf3f3d1.png)

where the maximization is done over the entire space of distributions on
![\\Omega
](//upload.wikimedia.org/math/b/9/d/b9d99db8626de63193c7fe96273a6cae.png).
Here, ![\\mu
\_{X}[P\_{X}]](//upload.wikimedia.org/math/9/c/0/9c0fceb972b6d09fdfe9bcf53e6adc59.png)
is the kernel embedding of the proposed density
![P\_{X}](//upload.wikimedia.org/math/a/9/b/a9b2bfcd4375aab9370c921704fed864.png)
and
![H](//upload.wikimedia.org/math/c/1/d/c1d9f50f86825a1a2302ec2449c17196.png)
is an entropy-like quantity (e.g.
[Entropy](/wiki/Entropy_(information_theory) "Entropy (information theory)"),
[KL
divergence](/wiki/Kullback%E2%80%93Leibler_divergence "Kullback–Leibler divergence"),
[Bregman divergence](/wiki/Bregman_divergence "Bregman divergence")).
The distribution which solves this optimization may be interpreted as a
compromise between fitting the empirical kernel means of the samples
well, while still allocating a substantial portion of the probability
mass to all regions of the probability space (much of which may not be
represented in the training examples). In practice, a good approximate
solution of the difficult optimization may be found by restricting the
space of candidate densities to a mixture of *M* candidate distributions
with regularized mixing proportions. Connections between the ideas
underlying [Gaussian
processes](/wiki/Gaussian_process "Gaussian process") and [conditional
random
fields](/wiki/Conditional_random_fields "Conditional random fields") may
be drawn with the estimation of conditional probability distributions in
this fashion, if one views the feature mappings associated with the
kernel as sufficient statistics in generalized (possibly
infinite-dimensional) [exponential
families](/wiki/Exponential_family "Exponential family").^[[5]](#cite_note-SongThesis-5)^

### Measuring dependence of random variables[[edit](/w/index.php?title=Kernel_embedding_of_distributions&action=edit&section=18 "Edit section: Measuring dependence of random variables")]

A measure of the statistical dependence between random variables
![X](//upload.wikimedia.org/math/0/2/1/02129bb861061d1a052c592e2dc6b383.png)
and
![Y](//upload.wikimedia.org/math/5/7/c/57cec4137b614c87cb4e24a3d003a3e0.png)
(from any domains on which sensible kernels can be defined) can be
formulated based on the Hilbert–Schmidt Independence Criterion
^[[11]](#cite_note-11)^

![{\\text{HSIC}}(X,Y)=\\left|\\left|{\\mathcal {C}}\_{{XY}}-\\mu
\_{X}\\otimes \\mu \_{Y}\\right|\\right|\_{{{\\mathcal {H}}\\otimes
{\\mathcal
{H}}}}\^{2}](//upload.wikimedia.org/math/4/f/8/4f82426db7a72e462a8dff34451e6714.png)

and can be used as a principled replacement for [mutual
information](/wiki/Mutual_information "Mutual information"), [Pearson
correlation](/wiki/Pearson_correlation "Pearson correlation") or any
other dependence measure used in learning algorithms. Most notably, HSIC
can detect arbitrary dependencies (when a characteristic kernel is used
in the embeddings, HSIC is zero if and only if the variables are
[independent](/wiki/Independence_(probability_theory) "Independence (probability theory)")),
and can be used to measure dependence between different types of data
(e.g. images and text captions). Given *n* i.i.d. samples of each random
variable, a simple parameter-free
[unbiased](/wiki/Bias_of_an_estimator "Bias of an estimator") estimator
of HSIC which exhibits
[concentration](/wiki/Concentration_of_measure "Concentration of measure")
about the true value can be computed in
![O(n(d\_{f}\^{2}+d\_{g}\^{2}))](//upload.wikimedia.org/math/8/1/0/810c1c6a17cc9f6094a8ecde7aaf11b1.png)
time,^[[5]](#cite_note-SongThesis-5)^ where the Gram matrices of the two
datasets are approximated using ![{\\mathbf {A}}{\\mathbf
{A}}\^{T},{\\mathbf {B}}{\\mathbf
{B}}\^{T}](//upload.wikimedia.org/math/e/8/a/e8a04957f4528630c88b59aeed5e3ea3.png)
with ![{\\mathbf {A}}\\in {\\mathbb {R}}\^{{n\\times d\_{f}}},{\\mathbf
{B}}\\in {\\mathbb {R}}\^{{n\\times
d\_{g}}}](//upload.wikimedia.org/math/1/d/0/1d0acb0d8014b0591d1e79449043adc8.png).
The desirable properties of HSIC have led to the formulation of numerous
algorithms which utilize this dependence measure for a variety of common
machine learning tasks such as: [feature
selection](/wiki/Feature_selection "Feature selection") (BAHSIC
^[[12]](#cite_note-12)^),
[clustering](/wiki/Cluster_analysis "Cluster analysis") (CLUHSIC
^[[13]](#cite_note-13)^), and [dimensionality
reduction](/wiki/Dimensionality_reduction "Dimensionality reduction")
(MUHSIC ^[[14]](#cite_note-14)^).

### Kernel belief propagation[[edit](/w/index.php?title=Kernel_embedding_of_distributions&action=edit&section=19 "Edit section: Kernel belief propagation")]

[Belief propagation](/wiki/Belief_propagation "Belief propagation") is a
fundamental algorithm for inference in [graphical
models](/wiki/Graphical_models "Graphical models") in which nodes
repeatedly pass and receive messages corresponding to the evaluation of
conditional expectations. In the kernel embedding framework, the
messages may be represented as RKHS functions and the conditional
distribution embeddings can be applied to efficiently compute message
updates. Given *n* samples of random variables represented by nodes in a
[Markov Random Field](/wiki/Markov_Random_Field "Markov Random Field"),
the incoming message to node *t* from node *u* can be expressed as
![m\_{{ut}}(\\cdot )=\\sum \_{{i=1}}\^{n}\\beta \_{{ut}}\^{i}\\phi
(x\_{t}\^{i})](//upload.wikimedia.org/math/8/d/7/8d76f7a81d5be8414f7052daf5cbbf99.png)
if it assumed to lie in the RKHS. The **kernel belief propagation
update** message from *t* to node *s* is then given by
^[[2]](#cite_note-Song2013-2)^

![\\widehat {m}\_{{ts}}=\\left(\\odot \_{{u\\in N(t)\\backslash
s}}{\\mathbf {K}}\_{t}{\\boldsymbol {\\beta
}}\_{{ut}}\\right)\^{T}({\\mathbf {K}}\_{s}+\\lambda {\\mathbf
{I}})\^{{-1}}{\\boldsymbol {\\Upsilon }}\_{s}\^{T}\\phi
(x\_{s})](//upload.wikimedia.org/math/f/8/e/f8ef3a2757e8cd273cd57eab79b39b01.png)

where ![\\odot
](//upload.wikimedia.org/math/9/d/2/9d21e01ce4991f34a5811f75149e284b.png)
denotes the element-wise vector product, ![N(t)\\backslash
s](//upload.wikimedia.org/math/2/f/5/2f5c385512665878591f7511dca6e798.png)
is the set of nodes connected to *t* excluding node *s*, ![{\\boldsymbol
{\\beta }}\_{{ut}}=\\left(\\beta \_{{ut}}\^{1},\\dots ,\\beta
\_{{ut}}\^{n}\\right)](//upload.wikimedia.org/math/e/3/c/e3cc96a889ff6453e783c4c3abe1428d.png),
![{\\mathbf {K}}\_{t},{\\mathbf
{K}}\_{s}](//upload.wikimedia.org/math/7/a/1/7a1c526ed0e61a3ee061e07017e8fdf8.png)
are the Gram matrices of the samples from variables
![X\_{t},X\_{s}](//upload.wikimedia.org/math/c/f/3/cf3894b9b9f2e8f69da5cd179cb5c40f.png),
respectively, and ![{\\boldsymbol {\\Upsilon }}\_{s}=\\left(\\phi
(x\_{s}\^{1}),\\dots ,\\phi
(x\_{s}\^{n})\\right)](//upload.wikimedia.org/math/0/8/e/08ea7c3687aa040789db4ba9f922c931.png)
is the feature matrix for the samples from
![X\_{s}](//upload.wikimedia.org/math/e/2/2/e2264c415eaadcf007c9df3435082713.png).

Thus, if the incoming messages to node *t* are linear combinations of
feature mapped samples from
![X\_{t}](//upload.wikimedia.org/math/0/4/b/04b2b7370fed516962642f0b34c55b37.png),
then the outgoing message from this node is also a linear combination of
feature mapped samples from
![X\_{s}](//upload.wikimedia.org/math/e/2/2/e2264c415eaadcf007c9df3435082713.png).
This RKHS function representation of message-passing updates therefore
produces an efficient belief propagation algorithm in which the
[potentials](/wiki/Markov_Random_Field#Clique_factorization "Markov Random Field")
are nonparametric functions inferred from the data so that arbitrary
statistical relationships may be modeled.^[[2]](#cite_note-Song2013-2)^

### Nonparametric filtering in hidden Markov models[[edit](/w/index.php?title=Kernel_embedding_of_distributions&action=edit&section=20 "Edit section: Nonparametric filtering in hidden Markov models")]

In the [hidden Markov
model](/wiki/Hidden_Markov_model "Hidden Markov model") (HMM), two key
quantities of interest are the transition probabilities between hidden
states ![P(S\^{t}\\mid
S\^{{t-1}})](//upload.wikimedia.org/math/3/e/7/3e7ab0138deab994a252dbb9b4171eb2.png)
and the emission probabilities ![P(O\^{t}\\mid
S\^{t})](//upload.wikimedia.org/math/8/6/9/869b46b313a1056e91dda562497754f6.png)
for observations. Using the kernel conditional distribution embedding
framework, these quantities may be expressed in terms of samples from
the HMM. A serious limitation of the embedding methods in this domain is
the need for training samples containing hidden states, as otherwise
inference with arbitrary distributions in the HMM is not possible.

One common use of HMMs is
[filtering](/wiki/Hidden_Markov_Model#Filtering "Hidden Markov Model")
in which the goal is to estimate posterior distribution over the hidden
state
![s\^{{t}}](//upload.wikimedia.org/math/3/2/f/32f12bfd03490a6efdeff987185c0f67.png)
at time step *t* given a history of previous observations
![h\^{t}=(o\^{1},\\dots
,o\^{t})](//upload.wikimedia.org/math/d/d/f/ddf47eecc993ac0c86e4de41ae5508c5.png)
from the system. In filtering, a **belief state** ![P(S\^{{t+1}}\\mid
h\^{{t+1}})](//upload.wikimedia.org/math/a/c/a/aca14ef039ee993e839a9b03fab90b7b.png)
is recursively maintained via a prediction step (where updates
![P(S\^{{t+1}}\\mid h\^{t})={\\mathbb {E}}\_{{S\^{t}\\mid
h\^{t}}}[P(S\^{{t+1}}\\mid
S\^{t})]](//upload.wikimedia.org/math/b/5/4/b54f1ef4fd10238dceb282d01f87e06f.png)
are computed by marginalizing out the previous hidden state) followed by
a conditioning step (where updates ![P(S\^{{t+1}}\\mid
h\^{t},o\^{{t+1}})\\propto P(o\^{{t+1}}\\mid
S\^{{t+1}})P(S\^{{t+1}}\\mid
h\^{t})](//upload.wikimedia.org/math/2/4/5/2450dea5ed4f3d5159ec129623e19e5a.png)
are computed by applying Bayes' rule to condition on a new
observation).^[[2]](#cite_note-Song2013-2)^ The RKHS embedding of the
belief state at time *t+1* can be recursively expressed as

![\\mu \_{{S\^{{t+1}}\\mid h\^{{t+1}}}}={\\mathcal
{C}}\_{{S\^{{t+1}}O\^{{t+1}}}}\^{\\pi }\\left({\\mathcal
{C}}\_{{O\^{{t+1}}O\^{{t+1}}}}\^{\\pi }\\right)\^{{-1}}\\phi
(o\^{{t+1}})](//upload.wikimedia.org/math/8/9/b/89bf2c8f6ec64c8562a0d617569cc09c.png)

by computing the embeddings of the prediction step via the [kernel sum
rule](#Kernel_Sum_Rule) and the embedding of the conditioning step via
[kernel Bayes' rule](#Kernel_Bayes.27_Rule). Assuming a training sample
![(\\widetilde {s}\^{1},\\dots ,\\widetilde {s}\^{T},\\widetilde
{o}\^{1},\\dots ,\\widetilde
{o}\^{T})](//upload.wikimedia.org/math/8/2/7/827e833d193ca4340c1b69fa7ebfcc96.png)
is given, one can in practice estimate ![\\widehat {\\mu
}\_{{S\^{{t+1}}\\mid h\^{{t+1}}}}=\\sum \_{{i=1}}\^{T}\\alpha
\_{i}\^{t}\\phi (\\widetilde
{s}\^{t})](//upload.wikimedia.org/math/c/6/a/c6ad5bef2ed097b995663e779b41ba8c.png)
and filtering with kernel embeddings is thus implemented recursively
using the following updates for the weights ![{\\boldsymbol {\\alpha
}}=(\\alpha \_{1},\\dots ,\\alpha
\_{T})](//upload.wikimedia.org/math/3/1/d/31df0f1f38f32f784059c9bc65d79e0b.png)
^[[2]](#cite_note-Song2013-2)^

![{\\mathbf {D}}\^{{t+1}}={\\text{diag}}\\left((G+\\lambda {\\mathbf
{I}})\^{{-1}}\\widetilde {G}{\\boldsymbol {\\alpha
}}\^{t}\\right)](//upload.wikimedia.org/math/9/2/d/92d85f52c5b4e7ea3a4c06520fed6824.png)

![{\\boldsymbol {\\alpha }}\^{{t+1}}={\\mathbf {D}}\^{{t+1}}{\\mathbf
{K}}\\left(({\\mathbf {D}}\^{{t+1}}K)\^{2}+\\widetilde {\\lambda
}{\\mathbf {I}}\\right)\^{{-1}}{\\mathbf {D}}\^{{t+1}}{\\mathbf
{K}}\_{{o\^{{t+1}}}}](//upload.wikimedia.org/math/6/4/b/64b6294e96bbcaf9230d1cb11b076416.png)

where ![{\\mathbf {G}},{\\mathbf
{K}}](//upload.wikimedia.org/math/1/5/6/1564f129ba0572791acc18de64f72d35.png)
denote the Gram matrices of ![\\widetilde {s}\^{1},\\dots ,\\widetilde
{s}\^{T}](//upload.wikimedia.org/math/c/5/4/c543db7497c3a07a2edc063c63c40d19.png)
and ![\\widetilde {o}\^{1},\\dots ,\\widetilde
{o}\^{T}](//upload.wikimedia.org/math/2/4/7/24735b057a3911d969520321f85eff75.png)
respectively, ![\\widetilde {{\\mathbf
{G}}}](//upload.wikimedia.org/math/9/1/3/9137ae3987415e9d1d51671796aaba3b.png)
is a transfer Gram matrix defined as ![\\widetilde {{\\mathbf
{G}}}\_{{ij}}=k(\\widetilde {s}\_{i},\\widetilde
{s}\_{{j+1}})](//upload.wikimedia.org/math/c/9/1/c91198736ccc681e3a968e9133d8fc47.png),
and ![{\\mathbf {K}}\_{{o\^{{t+1}}}}=(k(\\widetilde
{o}\^{1},o\^{{t+1}}),\\dots ,k(\\widetilde
{o}\^{T},o\^{{t+1}}))\^{T}](//upload.wikimedia.org/math/1/a/2/1a22ce2bb75297bb4faf6f5384603dcf.png).

### Support measure machines[[edit](/w/index.php?title=Kernel_embedding_of_distributions&action=edit&section=21 "Edit section: Support measure machines")]

The **support measure machine** (SMM) is a generalization of the
[support vector
machine](/wiki/Support_vector_machine "Support vector machine") (SVM) in
which the training examples are probability distributions paired with
labels ![\\{P\_{i},y\_{i}\\}\_{{i=1}}\^{n},\\ y\_{i}\\in
\\{+1,-1\\}](//upload.wikimedia.org/math/b/5/8/b58eaf252232f1ce0f59634547fe2a0b.png).^[[15]](#cite_note-SMM-15)^
SMMs solve the standard SVM [dual optimization
problem](/wiki/Support_vector_machine#Dual_form "Support vector machine")
using the following **expected kernel**

![K\\left(P(X),Q(Z)\\right)=\\langle \\mu \_{X},\\mu \_{Z}\\rangle
\_{{\\mathcal {H}}}={\\mathbb
{E}}\_{{XZ}}[k(x,z)]](//upload.wikimedia.org/math/9/5/1/951c54966539db324c78a765234b3a80.png)

which is computable in closed form for many common specific
distributions
![P\_{i}](//upload.wikimedia.org/math/4/f/2/4f2cbf2c8593be1f2fb006b74764f013.png)
(such as the Gaussian distribution) combined with popular embedding
kernels
![k](//upload.wikimedia.org/math/8/c/e/8ce4b16b22b58894aa86c421e8759df3.png)
(e.g. the Gaussian kernel or polynomial kernel), or can be accurately
empirically estimated from i.i.d. samples
![\\{x\_{i}\\}\_{{i=1}}\^{n}\\sim P(X),\\{z\_{j}\\}\_{{j=1}}\^{m}\\sim
Q(Z)](//upload.wikimedia.org/math/0/3/e/03eff8aefa3b9be1fb50f4a6f63f2067.png)
via

![\\widehat {K}\\left(X,Z\\right)={\\frac {1}{nm}}\\sum
\_{{i=1}}\^{n}\\sum
\_{{j=1}}\^{m}k(x\_{i},z\_{j})](//upload.wikimedia.org/math/b/0/c/b0cfa1e5e716c30f512226aa347bbcd4.png)

Under certain choices of the embedding kernel
![k](//upload.wikimedia.org/math/8/c/e/8ce4b16b22b58894aa86c421e8759df3.png),
the SMM applied to training examples
![\\{P\_{i},y\_{i}\\}\_{{i=1}}\^{n}](//upload.wikimedia.org/math/b/d/f/bdf00fbba64d9a6a39cb64ca196a12c6.png)
is equivalent to a SVM trained on samples
![\\{x\_{i},y\_{i}\\}\_{{i=1}}\^{n}](//upload.wikimedia.org/math/a/6/6/a6619eb2236f78e358182afaacaf85ae.png),
and thus the SMM can be viewed as a *flexible* SVM in which a different
data-dependent kernel (specified by the assumed form of the distribution
![P\_{i}](//upload.wikimedia.org/math/4/f/2/4f2cbf2c8593be1f2fb006b74764f013.png))
may be placed on each training point.^[[15]](#cite_note-SMM-15)^

### Domain adaptation under covariate, target, and conditional shift[[edit](/w/index.php?title=Kernel_embedding_of_distributions&action=edit&section=22 "Edit section: Domain adaptation under covariate, target, and conditional shift")]

The goal of **domain adaptation** is the formulation of learning
algorithms which generalize well when the training and test data have
different distributions. Given training examples
![\\{(x\_{i}\^{{tr}},y\_{i}\^{{tr}})\\}\_{{i=1}}\^{n}](//upload.wikimedia.org/math/1/4/9/1498ed515f54e0039dc0a3d5e8bb96e6.png)
and a test set
![\\{(x\_{j}\^{{te}},y\_{j}\^{{te}})\\}\_{{j=1}}\^{m}](//upload.wikimedia.org/math/8/b/3/8b3dfa9fb6502929339dd1fbdd5fd592.png)
where the
![y\_{j}\^{{te}}](//upload.wikimedia.org/math/2/9/5/2956d7e98fc21f02674055b681e406a6.png)
are unknown, three types of differences are commonly assumed between the
distribution of the training examples
![P\^{{tr}}(X,Y)](//upload.wikimedia.org/math/d/0/5/d053b13ec6799bd858fdfc69a6d4bf26.png)
and the test distribution
![P\^{{te}}(X,Y)](//upload.wikimedia.org/math/e/d/9/ed94a3820464b83392ab6768c9d6d376.png):^[[16]](#cite_note-DA-16)^^[[17]](#cite_note-CovS-17)^

1.  **Covariate Shift** in which the marginal distribution of the
    covariates changes across domains: ![P\^{{tr}}(X)\\neq
    P\^{{te}}(X)](//upload.wikimedia.org/math/1/c/2/1c224cc389b89371c1f374512dbb73a9.png)
2.  **Target Shift** in which the marginal distribution of the outputs
    changes across domains: ![P\^{{tr}}(Y)\\neq
    P\^{{te}}(Y)](//upload.wikimedia.org/math/b/e/6/be645dab4deca4286ef383593a045508.png)
3.  **Conditional Shift** in which
    ![P(Y)](//upload.wikimedia.org/math/c/3/6/c362a34d73aad0d7fba4dd1bf1e21b96.png)
    remains the same across domains, but the conditional distributions
    differ: ![P\^{{tr}}(X\\mid Y)\\neq P\^{{te}}(X\\mid
    Y)](//upload.wikimedia.org/math/c/5/7/c57a6fcc48a31f6f1d0c56b955a700fe.png).
    In general, the presence of conditional shift leads to an
    [ill-posed](/wiki/Well-posed_problem "Well-posed problem") problem,
    and the additional assumption that ![P(X\\mid
    Y)](//upload.wikimedia.org/math/2/7/b/27b9800817b431469d34c21996c54054.png)
    changes only under
    [location](/wiki/Location_parameter "Location parameter")-[scale](/wiki/Scale_parameter "Scale parameter")
    (LS) transformations on
    ![X](//upload.wikimedia.org/math/0/2/1/02129bb861061d1a052c592e2dc6b383.png)
    is commonly imposed to make the problem tractable.

By utilizing the kernel embedding of marginal and conditional
distributions, practical approaches to deal with the presence of these
types of differences between training and test domains can be
formulated. Covariate shift may be accounted for by reweighting examples
via estimates of the ratio
![P\^{{te}}(X)/P\^{{tr}}(X)](//upload.wikimedia.org/math/5/2/7/527cf7211cd33995906d6b7d51839fe8.png)
obtained directly from the kernel embeddings of the marginal
distributions of
![X](//upload.wikimedia.org/math/0/2/1/02129bb861061d1a052c592e2dc6b383.png)
in each domain without any need for explicit estimation of the
distributions.^[[17]](#cite_note-CovS-17)^ Target shift, which cannot be
similarly dealt with since no samples from
![Y](//upload.wikimedia.org/math/5/7/c/57cec4137b614c87cb4e24a3d003a3e0.png)
are available in the test domain, is accounted for by weighting training
examples using the vector ![{\\boldsymbol {\\beta }}\^{\*}({\\mathbf
{y}}\^{{tr}})](//upload.wikimedia.org/math/7/d/3/7d3bb947fb87ccbcd0e5c01734afe996.png)
which solves the following optimization problem (where in practice,
empirical approximations must be used) ^[[16]](#cite_note-DA-16)^

![\\min \_{{{\\boldsymbol {\\beta }}(y)}}\\left|\\left|{\\mathcal
{C}}\_{{{(X\\mid Y)}\^{{tr}}}}{\\mathbb
{E}}\_{{Y\^{{tr}}}}[{\\boldsymbol {\\beta }}(y)\\phi (y)]-\\mu
\_{{X\^{{te}}}}\\right|\\right|\_{{\\mathcal
{H}}}\^{2}](//upload.wikimedia.org/math/d/4/0/d40e9166a3171f40eb07635a97c5552d.png)
subject to ![{\\boldsymbol {\\beta }}(y)\\geq 0,{\\mathbb
{E}}\_{{Y\^{{tr}}}}[{\\boldsymbol {\\beta
}}(y)]=1](//upload.wikimedia.org/math/b/d/3/bd31493d971c0364fe6f0064a4a02da0.png)

To deal with location scale conditional shift, one can perform a LS
transformation of the training points to obtain new transformed training
data ![{\\mathbf {X}}\^{{new}}={\\mathbf {X}}\^{{tr}}\\odot {\\mathbf
{W}}+{\\mathbf
{B}}](//upload.wikimedia.org/math/c/d/e/cde7db48371973960fd702b537b17d36.png)
(where ![\\odot
](//upload.wikimedia.org/math/9/d/2/9d21e01ce4991f34a5811f75149e284b.png)
denotes the element-wise vector product). To ensure similar
distributions between the new transformed training samples and the test
data, ![{\\mathbf {W}},{\\mathbf
{B}}](//upload.wikimedia.org/math/4/1/7/41757fc17c69694a24f3159da03b4668.png)
are estimated by minimizing the following empirical kernel embedding
distance ^[[16]](#cite_note-DA-16)^

![\\left|\\left|\\widehat {\\mu }\_{{X\^{{new}}}}-\\widehat {\\mu
}\_{{X\^{{te}}}}\\right|\\right|\_{{{\\mathcal
{H}}}}\^{2}=\\left|\\left|\\widehat {{\\mathcal {C}}}\_{{(X\\mid
Y)\^{{new}}}}\\widehat {\\mu }\_{{Y\^{{tr}}}}-\\widehat {\\mu
}\_{{X\^{{te}}}}\\right|\\right|\_{{{\\mathcal
{H}}}}\^{2}](//upload.wikimedia.org/math/a/4/4/a4479139be207b7d4ff41d46953d0f30.png)

In general, the kernel embedding methods for dealing with LS conditional
shift and target shift may be combined to find a reweighted
transformation of the training data which mimics the test distribution,
and these methods may perform well even in the presence of conditional
shifts other than location-scale changes.^[[16]](#cite_note-DA-16)^

### Domain generalization via invariant feature representation[[edit](/w/index.php?title=Kernel_embedding_of_distributions&action=edit&section=23 "Edit section: Domain generalization via invariant feature representation")]

Given *N* sets of training examples sampled i.i.d. from distributions
![P\^{{(1)}}(X,Y),P\^{{(2)}}(X,Y),\\dots
,P\^{{(N)}}(X,Y)](//upload.wikimedia.org/math/f/a/f/faf28914daa9737355fc1be362366922.png),
the goal of **domain generalization** is to formulate learning
algorithms which perform well on test examples sampled from a previously
unseen domain
![P\^{\*}(X,Y)](//upload.wikimedia.org/math/4/d/d/4dd7470aec760df7c1dcd7ed7f18a589.png)
where no data from the test domain is available at training time. If
conditional distributions ![P(Y\\mid
X)](//upload.wikimedia.org/math/a/8/c/a8c14bbd3415899e56075e656f90cb72.png)
are assumed to be relatively similar across all domains, then a learner
capable of domain generalization must estimate a functional relationship
between the variables which is robust to changes in the marginals
![P(X)](//upload.wikimedia.org/math/0/c/3/0c3d72395d7576ab13b9e9389f865960.png).
Based on kernel embeddings of these distributions, Domain Invariant
Component Analysis (DICA) is a method which determines the
transformation of the training data that minimizes the difference
between marginal distributions while preserving a common conditional
distribution shared between all training
domains.^[[18]](#cite_note-DICA-18)^ DICA thus extracts *invariants*,
features that transfer across domains, and may be viewed as a
generalization of many popular dimension-reduction methods such as
[kernel principal component
analysis](/wiki/Kernel_principal_component_analysis "Kernel principal component analysis"),
transfer component analysis, and covariance operator inverse
regression.^[[18]](#cite_note-DICA-18)^

Defining a probability distribution ![{\\mathcal
{P}}](//upload.wikimedia.org/math/d/1/0/d10474a5bd9c2ef0d456700a201d5dd5.png)
on the RKHS ![{\\mathcal
{H}}](//upload.wikimedia.org/math/c/6/d/c6d79f9e5dd728bae3a00468c627d341.png)
with ![{\\mathcal {P}}(\\mu \_{{X\^{{(i)}}Y\^{{(i)}}}})=1/N{\\text{ for
}}i=1,\\dots
,N](//upload.wikimedia.org/math/5/a/3/5a3a05933a933d7b531b44dd3f54f6b0.png),
DICA measures dissimilarity between domains via **distributional
variance** which is computed as

![V\_{{\\mathcal {H}}}({\\mathcal {P}})={\\frac
{1}{N}}{\\text{tr}}({\\mathbf {G}})-{\\frac {1}{N\^{2}}}\\sum
\_{{i,j=1}}\^{N}{\\mathbf
{G}}\_{{ij}}](//upload.wikimedia.org/math/a/1/9/a192ac00b6ed66ae3b44b41308e49696.png)
where ![{\\mathbf {G}}\_{{ij}}=\\langle \\mu \_{{X\^{{(i)}}}},\\mu
\_{{X\^{{(j)}}}}\\rangle \_{{\\mathcal
{H}}}](//upload.wikimedia.org/math/1/5/3/15327a769ce47657f04135b099f99f7d.png)

so ![{\\mathbf
{G}}](//upload.wikimedia.org/math/c/4/4/c4451413b80e95dc3fecb7312438f6cf.png)
is a ![N\\times
N](//upload.wikimedia.org/math/3/9/5/395a9af17f8642e02a32af8637542947.png)
Gram matrix over the distributions from which the training data are
sampled. Finding an [orthogonal
transform](/wiki/Orthogonal_matrix "Orthogonal matrix") onto a
low-dimensional [subspace](/wiki/Linear_subspace "Linear subspace") *B*
(in the feature space) which minimizes the distributional variance, DICA
simultaneously ensures that *B* aligns with the
[bases](/wiki/Basis_function "Basis function") of a **central subspace**
*C* for which
![Y](//upload.wikimedia.org/math/5/7/c/57cec4137b614c87cb4e24a3d003a3e0.png)
becomes independent of
![X](//upload.wikimedia.org/math/0/2/1/02129bb861061d1a052c592e2dc6b383.png)
given
![C\^{T}X](//upload.wikimedia.org/math/d/d/0/dd0f6161904a7a79e4a8d03094f5b652.png)
across all domains. In the absence of target values
![Y](//upload.wikimedia.org/math/5/7/c/57cec4137b614c87cb4e24a3d003a3e0.png),
an unsupervised version of DICA may be formulated which finds a
low-dimensional subspace that minimizes distributional variance while
simultaneously maximizing the variance of
![X](//upload.wikimedia.org/math/0/2/1/02129bb861061d1a052c592e2dc6b383.png)
(in the feature space) across all domains (rather than preserving a
central subspace).^[[18]](#cite_note-DICA-18)^

Example[[edit](/w/index.php?title=Kernel_embedding_of_distributions&action=edit&section=24 "Edit section: Example")]
--------------------------------------------------------------------------------------------------------------------

In this simple example, which is taken from Song et
al.,^[[2]](#cite_note-Song2013-2)^
![X,Y](//upload.wikimedia.org/math/d/2/3/d23a4ce8bca0f4891e037439a79b45a6.png)
are assumed to be [discrete random
variables](/wiki/Probability_distribution#Discrete_probability_distribution "Probability distribution")
which take values in the set ![\\{1,\\dots
,K\\}](//upload.wikimedia.org/math/1/2/a/12a8e751e90b1a6a7be70137855cc529.png)
and the kernel is chosen to be the [Kronecker
delta](/wiki/Kronecker_delta "Kronecker delta") function, so
![k(x,x')=\\delta
(x,x')](//upload.wikimedia.org/math/6/2/0/62072c22e99c521ec2a6c28799a2298a.png).
The feature map corresponding to this kernel is the [standard
basis](/wiki/Standard_basis "Standard basis") vector ![\\phi
(x)={\\mathbf
{e}}\_{x}](//upload.wikimedia.org/math/c/2/3/c23453d5ccaadf711ea2b0143ff60d90.png).
The kernel embeddings of such a distributions are thus vectors of
marginal probabilities while the embeddings of joint distributions in
this setting are ![K\\times
K](//upload.wikimedia.org/math/8/a/1/8a15d79635e907ab88124e1c915e0ae5.png)
matrices specifying joint probability tables, and the explicit form of
these embeddings is

![\\mu \_{X}={\\mathbb {E}}\_{X}[{\\mathbf
{e}}\_{x}]=\\left({\\begin{array}{c}P(X=1)\\\\\\vdots
\\\\P(X=K)\\\\\\end{array}}\\right)](//upload.wikimedia.org/math/6/d/7/6d7c7b843b3a119efc12796989ad3fb7.png)

![{\\mathcal {C}}\_{{XY}}={\\mathbb {E}}\_{{XY}}[{\\mathbf
{e}}\_{X}\\otimes e\_{Y}]={\\bigg (}P(X=s,Y=t){\\bigg )}\_{{s,t\\in
\\{1,\\dots
,K\\}}}](//upload.wikimedia.org/math/b/4/c/b4c526cf179daadd723082c5b25a6e6b.png)

The conditional distribution embedding operator ![{\\mathcal
{C}}\_{{Y\\mid X}}={\\mathcal {C}}\_{{YX}}{\\mathcal
{C}}\_{{XX}}\^{{-1}}](//upload.wikimedia.org/math/d/a/0/da0a20010267c28073c4203d982390f8.png)
is in this setting a conditional probability table

![{\\mathcal {C}}\_{{Y\\mid X}}={\\bigg (}P(Y=s\\mid X=t){\\bigg
)}\_{{s,t\\in \\{1,\\dots
,K\\}}}](//upload.wikimedia.org/math/c/b/4/cb463299efc577acfcd35d67a3b5316f.png)

and ![{\\mathcal {C}}\_{{XX}}=\\left({\\begin{array}{ccc}P(X=1)&\\dots
&0\\\\\\vdots &\\ddots &\\vdots \\\\0&\\dots
&P(X=K)\\\\\\end{array}}\\right)](//upload.wikimedia.org/math/8/b/8/8b890f4ef6e384262b41e8d861e73ba7.png)

Thus, the embeddings of the conditional distribution under a fixed value
of
![X](//upload.wikimedia.org/math/0/2/1/02129bb861061d1a052c592e2dc6b383.png)
may be computed as

![\\mu \_{{Y\\mid x}}={\\mathcal {C}}\_{{Y\\mid X}}\\phi
(x)=\\left({\\begin{array}{c}P(Y=1\\mid X=x)\\\\\\vdots \\\\P(Y=K\\mid
X=x)\\\\\\end{array}}\\right)](//upload.wikimedia.org/math/f/5/f/f5fc0a58904776e4769d21672614ea38.png)

In this discrete-valued setting with the Kronecker delta kernel, the
[kernel sum rule](#Rules_of_probability_as_operations_in_the_RKHS)
becomes

![\\underbrace {\\left({\\begin{array}{c}Q(X=1)\\\\\\vdots
\\\\P(X=N)\\\\\\end{array}}\\right)}\_{{\\mu \_{Y}\^{\\pi
}}}=\\underbrace {\\left({\\begin{array}{c}\\\\P(X=s\\mid
Y=t)\\\\\\\\\\end{array}}\\right)}\_{{{\\mathcal {C}}\_{{X\\mid
Y}}}}\\underbrace {\\left({\\begin{array}{c}\\pi (Y=1)\\\\\\vdots
\\\\pi(Y=N)\\\\\\end{array}}\\right)}\_{{\\mu \_{Y}\^{\\pi
}}}](//upload.wikimedia.org/math/3/4/7/3472b427fd13bbabe73b796d6892c9f4.png)

The [kernel chain rule](#Rules_of_probability_as_operations_in_the_RKHS)
in this case is given by

![\\underbrace
{\\left({\\begin{array}{c}\\\\Q(X=s,Y=t)\\\\\\\\\\end{array}}\\right)}\_{{{\\mathcal
{C}}\_{{XY}}\^{\\pi }}}=\\underbrace
{\\left({\\begin{array}{c}\\\\P(X=s\\mid
Y=t)\\\\\\\\\\end{array}}\\right)}\_{{{\\mathcal {C}}\_{{X\\mid
Y}}}}\\underbrace {\\left({\\begin{array}{ccc}\\pi (Y=1)&\\dots
&0\\\\\\vdots &\\ddots &\\vdots \\\\0&\\dots &\\pi
(Y=K)\\\\\\end{array}}\\right)}\_{{{\\mathcal {C}}\_{{YY}}\^{\\pi
}}}](//upload.wikimedia.org/math/6/b/d/6bdd7014cfe57480d8e1de1db23509c0.png)

References[[edit](/w/index.php?title=Kernel_embedding_of_distributions&action=edit&section=25 "Edit section: References")]
--------------------------------------------------------------------------------------------------------------------------

1.  \^ [^***a***^](#cite_ref-Smola2007_1-0)
    [^***b***^](#cite_ref-Smola2007_1-1) A. Smola, A. Gretton, L. Song,
    B. Schölkopf. (2007). [A Hilbert Space Embedding for
    Distributions](http://eprints.pascal-network.org/archive/00003987/01/SmoGreSonSch07.pdf).
    *Algorithmic Learning Theory: 18th International Conference*.
    Springer: 13–31.
2.  \^ [^***a***^](#cite_ref-Song2013_2-0)
    [^***b***^](#cite_ref-Song2013_2-1)
    [^***c***^](#cite_ref-Song2013_2-2)
    [^***d***^](#cite_ref-Song2013_2-3)
    [^***e***^](#cite_ref-Song2013_2-4)
    [^***f***^](#cite_ref-Song2013_2-5)
    [^***g***^](#cite_ref-Song2013_2-6)
    [^***h***^](#cite_ref-Song2013_2-7)
    [^***i***^](#cite_ref-Song2013_2-8)
    [^***j***^](#cite_ref-Song2013_2-9)
    [^***k***^](#cite_ref-Song2013_2-10)
    [^***l***^](#cite_ref-Song2013_2-11)
    [^***m***^](#cite_ref-Song2013_2-12)
    [^***n***^](#cite_ref-Song2013_2-13) L. Song, K. Fukumizu, F.
    Dinuzzo, A. Gretton (2013). [Kernel Embeddings of Conditional
    Distributions: A unified kernel framework for nonparametric
    inference in graphical
    models](http://www.gatsby.ucl.ac.uk/~gretton/papers/SonFukGre13.pdf).
    *IEEE Signal Processing Magazine* **30**: 98–111.
3.  **[\^](#cite_ref-3)** J. Shawe-Taylor, N. Christianini. (2004).
    *Kernel Methods for Pattern Analysis*. Cambridge University Press,
    Cambridge, UK.
4.  **[\^](#cite_ref-4)** T. Hofmann, B. Schölkopf, A. Smola. (2008).
    [Kernel Methods in Machine
    Learning](http://projecteuclid.org/DPubS?service=UI&version=1.0&verb=Display&handle=euclid.aos/1211819561).
    *The Annals of Statistics* **36**(3):1171–1220.
5.  \^ [^***a***^](#cite_ref-SongThesis_5-0)
    [^***b***^](#cite_ref-SongThesis_5-1)
    [^***c***^](#cite_ref-SongThesis_5-2)
    [^***d***^](#cite_ref-SongThesis_5-3)
    [^***e***^](#cite_ref-SongThesis_5-4)
    [^***f***^](#cite_ref-SongThesis_5-5)
    [^***g***^](#cite_ref-SongThesis_5-6)
    [^***h***^](#cite_ref-SongThesis_5-7)
    [^***i***^](#cite_ref-SongThesis_5-8) L. Song. (2008) [Learning via
    Hilbert Space Embedding of
    Distributions](http://www.cc.gatech.edu/~lsong/papers/lesong_thesis.pdf).
    PhD Thesis, University of Sidney.
6.  \^ [^***a***^](#cite_ref-SongCDE_6-0)
    [^***b***^](#cite_ref-SongCDE_6-1)
    [^***c***^](#cite_ref-SongCDE_6-2) L. Song, J. Huang, A. J. Smola,
    K. Fukumizu. (2009). [Hilbert space embeddings of conditional
    distributions](http://www.stanford.edu/~jhuang11/research/pubs/icml09/icml09.pdf).
    *Proc. Int. Conf. Machine Learning*. Montreal, Canada: 961-968.
7.  **[\^](#cite_ref-7)** A. Gretton, K. Borgwardt, M. Rasch, B.
    Schölkopf, A. Smola. (2007). [A kernel method for the
    two-sample-problem](http://www.gatsby.ucl.ac.uk/~gretton/papers/GreBorRasSchSmo07.pdf).
    *Advances in Neural Information Processing Systems* **19**, MIT
    Press, Cambridge, MA.
8.  **[\^](#cite_ref-8)** S. Grunewalder, G. Lever, L. Baldassarre, S.
    Patterson, A. Gretton, M. Pontil. (2012). [Conditional mean
    embeddings as regressors](http://icml.cc/2012/papers/898.pdf).
    *Proc. Int. Conf. Machine Learning*: 1823–1830.
9.  **[\^](#cite_ref-9)** A. Gretton, K. Borgwardt, M. Rasch, B.
    Schölkopf, A. Smola. (2012). [A kernel two-sample
    test](http://jmlr.org/papers/volume13/gretton12a/gretton12a.pdf).
    *Journal of Machine Learning Research*, **13**: 723-773.
10. **[\^](#cite_ref-10)** M. Dudík, S. J. Phillips, R. E. Schapire.
    (2007). [Maximum Entropy Distribution Estimation with Generalized
    Regularization and an Application to Species Distribution
    Modeling](http://classes.soe.ucsc.edu/cmps242/Winter08/lect/15/maxent_genreg_jmlr.pdf).
    *Journal of Machine Learning Research*, **8**: 1217-1260.
11. **[\^](#cite_ref-11)** A. Gretton, O. Bousquet, A. Smola, B.
    Schölkopf. (2005). [Measuring statistical dependence with
    Hilbert–Schmidt
    norms](http://www.gatsby.ucl.ac.uk/~gretton/papers/GreBouSmoSch05.pdf).
    *Proc. Intl. Conf. on Algorithmic Learning Theory*: 63–78.
12. **[\^](#cite_ref-12)** L. Song, A. Smola , A. Gretton, K. Borgwardt,
    J. Bedo. (2007). [Supervised feature selection via dependence
    estimation](http://www.machinelearning.org/proceedings/icml2007/papers/244.pdf).
    *Proc. Intl. Conf. Machine Learning*, Omnipress: 823–830.
13. **[\^](#cite_ref-13)** L. Song, A. Smola, A. Gretton, K. Borgwardt.
    (2007). [A dependence maximization view of
    clustering](http://machinelearning.wustl.edu/mlpapers/paper_files/icml2007_SongSGB07.pdf).
    *Proc. Intl. Conf. Machine Learning*. Omnipress: 815–822.
14. **[\^](#cite_ref-14)** L. Song, A. Smola, K. Borgwardt, A. Gretton.
    (2007). [Colored maximum variance
    unfolding](http://machinelearning.wustl.edu/mlpapers/paper_files/NIPS2007_492.pdf).
    *Neural Information Processing Systems*.
15. \^ [^***a***^](#cite_ref-SMM_15-0) [^***b***^](#cite_ref-SMM_15-1)
    K. Muandet, K. Fukumizu, F. Dinuzzo, B. Schölkopf. (2012). [Learning
    from Distributions via Support Measure
    Machines](http://books.nips.cc/papers/files/nips25/NIPS2012_0015.pdf).
    *Advances in Neural Information Processing Systems*: 10–18.
16. \^ [^***a***^](#cite_ref-DA_16-0) [^***b***^](#cite_ref-DA_16-1)
    [^***c***^](#cite_ref-DA_16-2) [^***d***^](#cite_ref-DA_16-3) K.
    Zhang, B. Schölkopf, K. Muandet, Z. Wang. (2013). [Domain adaptation
    under target and conditional
    shift](http://jmlr.org/proceedings/papers/v28/zhang13d.pdf).
    *Journal of Machine Learning Research, **28**(3): 819–827.*
17. \^ [^***a***^](#cite_ref-CovS_17-0) [^***b***^](#cite_ref-CovS_17-1)
    A. Gretton, A. Smola, J. Huang, M. Schmittfull, K. Borgwardt, B.
    Schölkopf. (2008). Covariate shift and local learning by
    distribution matching. *In J. Quinonero-Candela, M. Sugiyama, A.
    Schwaighofer, N. Lawrence (eds.). Dataset shift in machine
    learning*, MIT Press, Cambridge, MA: 131–160.
18. \^ [^***a***^](#cite_ref-DICA_18-0) [^***b***^](#cite_ref-DICA_18-1)
    [^***c***^](#cite_ref-DICA_18-2) K. Muandet, D. Balduzzi, B.
    Schölkopf. (2013).[Domain Generalization Via Invariant Feature
    Representation](http://jmlr.org/proceedings/papers/v28/muandet13.pdf).
    *30th International Conference on Machine Learning*.

![image](//en.wikipedia.org/wiki/Special:CentralAutoLogin/start?type=1x1)

Retrieved from
"[http://en.wikipedia.org/w/index.php?title=Kernel\_embedding\_of\_distributions&oldid=590862106](http://en.wikipedia.org/w/index.php?title=Kernel_embedding_of_distributions&oldid=590862106)"

[Categories](/wiki/Help:Category "Help:Category"):

-   [Machine
    learning](/wiki/Category:Machine_learning "Category:Machine learning")
-   [Statistics](/wiki/Category:Statistics "Category:Statistics")

Navigation menu
---------------

### Personal tools

-   [Create
    account](/w/index.php?title=Special:UserLogin&returnto=Kernel+embedding+of+distributions&type=signup)
-   [Log
    in](/w/index.php?title=Special:UserLogin&returnto=Kernel+embedding+of+distributions "You're encouraged to log in; however, it's not mandatory. [o]")

### Namespaces

-   [Article](/wiki/Kernel_embedding_of_distributions "View the content page [c]")
-   [Talk](/w/index.php?title=Talk:Kernel_embedding_of_distributions&action=edit&redlink=1 "Discussion about the content page [t]")

### 

### Variants[](#)

### Views

-   [Read](/wiki/Kernel_embedding_of_distributions)
-   [Edit](/w/index.php?title=Kernel_embedding_of_distributions&action=edit "You can edit this page. 
    Please review your changes before saving. [e]")
-   [View
    history](/w/index.php?title=Kernel_embedding_of_distributions&action=history "Past versions of this page [h]")

### Actions[](#)

### Search

[](/wiki/Main_Page "Visit the main page")

### Navigation

-   [Main page](/wiki/Main_Page "Visit the main page [z]")
-   [Contents](/wiki/Portal:Contents "Guides to browsing Wikipedia")
-   [Featured
    content](/wiki/Portal:Featured_content "Featured content – the best of Wikipedia")
-   [Current
    events](/wiki/Portal:Current_events "Find background information on current events")
-   [Random article](/wiki/Special:Random "Load a random article [x]")
-   [Donate to
    Wikipedia](https://donate.wikimedia.org/wiki/Special:FundraiserRedirector?utm_source=donate&utm_medium=sidebar&utm_campaign=C13_en.wikipedia.org&uselang=en "Support us")
-   [Wikimedia Shop](//shop.wikimedia.org "Visit the Wikimedia Shop")

### Interaction

-   [Help](/wiki/Help:Contents "Guidance on how to use and edit Wikipedia")
-   [About Wikipedia](/wiki/Wikipedia:About "Find out about Wikipedia")
-   [Community
    portal](/wiki/Wikipedia:Community_portal "About the project, what you can do, where to find things")
-   [Recent
    changes](/wiki/Special:RecentChanges "A list of recent changes in the wiki [r]")
-   [Contact page](//en.wikipedia.org/wiki/Wikipedia:Contact_us)

### Tools

-   [What links
    here](/wiki/Special:WhatLinksHere/Kernel_embedding_of_distributions "List of all English Wikipedia pages containing links to this page [j]")
-   [Related
    changes](/wiki/Special:RecentChangesLinked/Kernel_embedding_of_distributions "Recent changes in pages linked from this page [k]")
-   [Upload file](/wiki/Wikipedia:File_Upload_Wizard "Upload files [u]")
-   [Special
    pages](/wiki/Special:SpecialPages "A list of all special pages [q]")
-   [Permanent
    link](/w/index.php?title=Kernel_embedding_of_distributions&oldid=590862106 "Permanent link to this revision of the page")
-   [Page
    information](/w/index.php?title=Kernel_embedding_of_distributions&action=info)
-   [Cite this
    page](/w/index.php?title=Special:Cite&page=Kernel_embedding_of_distributions&id=590862106 "Information on how to cite this page")

### Print/export

-   [Create a
    book](/w/index.php?title=Special:Book&bookcmd=book_creator&referer=Kernel+embedding+of+distributions)
-   [Download as
    PDF](/w/index.php?title=Special:Book&bookcmd=render_article&arttitle=Kernel+embedding+of+distributions&oldid=590862106&writer=rl)
-   [Printable
    version](/w/index.php?title=Kernel_embedding_of_distributions&printable=yes "Printable version of this page [p]")

-   This page was last modified on 15 January 2014 at 19:43.\
-   Text is available under the [Creative Commons Attribution-ShareAlike
    License](//en.wikipedia.org/wiki/Wikipedia:Text_of_Creative_Commons_Attribution-ShareAlike_3.0_Unported_License)[](//creativecommons.org/licenses/by-sa/3.0/);
    additional terms may apply. By using this site, you agree to the
    [Terms of Use](//wikimediafoundation.org/wiki/Terms_of_Use) and
    [Privacy Policy.](//wikimediafoundation.org/wiki/Privacy_policy) \
     Wikipedia® is a registered trademark of the [Wikimedia Foundation,
    Inc.](//www.wikimediafoundation.org/), a non-profit organization.

-   [Privacy
    policy](//wikimediafoundation.org/wiki/Privacy_policy "wikimedia:Privacy policy")
-   [About Wikipedia](/wiki/Wikipedia:About "Wikipedia:About")
-   [Disclaimers](/wiki/Wikipedia:General_disclaimer "Wikipedia:General disclaimer")
-   [Contact Wikipedia](//en.wikipedia.org/wiki/Wikipedia:Contact_us)
-   [Developers](https://www.mediawiki.org/wiki/Special:MyLanguage/How_to_contribute)
-   [Mobile
    view](//en.m.wikipedia.org/wiki/Kernel_embedding_of_distributions)

-   [![Wikimedia
    Foundation](//bits.wikimedia.org/images/wikimedia-button.png)](//wikimediafoundation.org/)
-   [![Powered by
    MediaWiki](//bits.wikimedia.org/static-1.23wmf13/skins/common/images/poweredby_mediawiki_88x31.png)](//www.mediawiki.org/)


This markdown document has been converted from the html document located at:
https://en.wikipedia.org/wiki/Kernel_embedding_of_distributions
