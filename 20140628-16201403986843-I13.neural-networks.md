[FastML](/)
===========

Machine learning made easy
--------------------------

-   [RSS](/atom.xml "subscribe via RSS")

-   [Home](/)
-   [Contents](/contents/)
-   [Popular](/popular/)
-   [Links](/links/)
-   [About](/about/)
-   [Backgrounds](/backgrounds/)

Extreme Learning Machines and optimizing hyperparams with hyperopt
==================================================================

2014-06-25 18:24

What do you get when you take out backpropagation out of a multilayer
perceptron? You get an extreme learning machine, a non-linear model with
the speed of a linear one. It has a few hyperparameters to tune and this
gives us an opportunity to use hyperopt, a Python package for hyperparam
optimization.

Extreme Learning Machines
-------------------------

ELM is a [Chinese invention](http://www.ntu.edu.sg/home/egbhuang/).
Imagine a classic feed-forward neural network with one hidden layer,
subtract backpropagation and you have an ELM. The input-hidden weights
are constant - they are apparently initialized analytically so even
though they are semi-random the thing works. The model learns only the
hidden-output weights, which amounts to learning a linear model. Hence
the speed.

Actually the perceptron model is only half the solution, at least in
[Python-ELM](https://github.com/zygmuntz/Python-ELM), the software we’ll
be using. The other half is a radial basis function network (see [The
Secret of The Big Guys](/the-secret-of-the-big-guys/) based on
clustering and distance measures. Normally Python-ELM employs both parts
and there’s a mixing parameter *alpha* controlling contribution from one
vs the other. You can use just one if you want to.

Normal ELM features quite a few activation functions, including standard
sigmoid and tanh. We have [added rectified
linear](https://github.com/zygmuntz/Python-ELM/commit/e9b4bfebd1c6529f1aaaf291058958d0a4ff15a5)
activation, that is *max( 0, x )* for good measure.

There is also a version called GRBF, which according to the paper
authors gives strictly better results on the datasets they tested. With
the data in hand it works worse and runs slower. The upside is that it
has just two hyperparams: the number of hidden units and another
parameter, which the paper authors say is best set to 0.05 (*MELM-GRBF:
a modified version of the extreme learning machine for generalized
radial basis function neural networks*
[[PDF](http://sci2s.ugr.es/keel/pdf/keel/articulo/2011-Neurocomputing1.pdf)]).

ELM consists of a hidden layer and a linear output layer. Python-ELM is
a complete solution, but instead of standard least squares you can use
the `Pipeline` from scikit-learn to put regularized linear model (or in
fact any regressor/classifier) on top of the hidden layer. You can even
stack two or more hidden layers, although we’re not sure if it makes
sense theoretically. Here’s a basic regression example:

    rl = RandomLayer( n_hidden = n_hidden, alpha = alpha, 
        rbf_width = rbf_width, activation_func = activation_func )

    elmr = GenELMRegressor( hidden_layer = rl )

    elmr.fit( x_train, y_train )
    p = elmr.predict( x_test )

The same thing, but with Ridge on top:

    rl = RandomLayer( n_hidden = n_hidden, alpha = alpha, 
        rbf_width = rbf_width, activation_func = activation_func )

    ridge = Ridge( alpha = ridge_alpha )

    elmr = pipeline.Pipeline( [( 'rl', rl ), ( 'ridge', ridge )] )  

    elmr.fit( x_train, y_train )
    p = elmr.predict( x_test )

[Complete code](https://github.com/zygmuntz/kaggle-burn-cpu) is
available at GitHub. There are more examples in
[elm\_notebook.py](https://github.com/zygmuntz/Python-ELM/blob/master/elm_notebook.py).

If you’re further interested in the matter, check out a [Reddit thread
on
ELM](http://www.reddit.com/r/MachineLearning/comments/28go2e/downsides_to_extreme_learning_machines/)
and this recently published article on [Extreme Learning Machines with
Julia](http://lepisma.svbtle.com/extreme-learning-machines-with-julia).
Unfortunately the author uses a rather trivial dataset, both in size and
difficulty. Which brings us to…

The data
--------

We will use ELM on the data from [Burn CPU
burn](http://inclass.kaggle.com/c/model-t4) competition at Kaggle. The
goal is to predict CPU load on a cluster of computers. There are less
than 100 columns, mostly numeric, with the exception of timestamps and
machine IDs (there’s seven computers). We treat the machine ID as a
categorical variable. A more complicated alternative would be to build a
separate model for each computer, however some quick tests of this
approach showed worse results.

There are roughy 180k examples, sorted by time. We take the first 93170
as the training set and set aside the remaining 85610 points for
validation. This is meant to mimic the original train/test split.

Software for optimizing hyperparams
-----------------------------------

Now that we have the method and the data, let’s take a look at software
for optimizing hyperparams. These are the notable packages that we know
of (we covered [Spearmint](/blog/categories/Spearmint) a while ago):

-   [Spearmint](https://github.com/JasperSnoek/spearmint) (Python)
-   [BayesOpt](http://rmcantin.bitbucket.org/html/) (C++ with Python and
    Matlab/Octave interfaces)
-   [hyperopt](http://hyperopt.github.io/hyperopt/) (Python)
-   [SMAC](http://www.cs.ubc.ca/labs/beta/Projects/SMAC/) (Java)

The first two use Gaussian processes. The issue with GPs is that you
have to choose priors. This is especially pronounced in case of
BayesOpt: it looks like you need to tune hyperparams for the hyperparam
tuner. Spearmint takes care of this problem, but is slow: it takes a few
minutes to tune the benchmark Bramin function, while hyperopt takes just
a few seconds. On the other hand, Spearmint’s overhead matters less with
objective functions which run longer, like an hour. And Spearmint finds
the optimal solution for Bramin in roughly 35 function evaluations,
while hyperopt needs twice as much.

Hyperopt also uses a form of Bayesian optimization, specifically TPE,
that is a *tree of Parzen estimators*. If you’re interested, details of
the algorithm are in the [Making a Science of Model
Search](http://arxiv.org/abs/1209.5111) paper.

The process
-----------

The main things to do are:

1.  describe the search space, that is hyperparams and possible values
    for them
2.  implement the function to minimize
3.  optionally, take care of logging values and scores for analysis

The function to minimize takes hyperparam values as input and returns a
score, that is a value for error (since we’re minimizing). This means
that each time the optimizer decides on which parameter values it likes
to check, we train the model and predict targets for validation set (or
do cross-validation). Then we compute the prediction error and give it
back to the optimizer. Then again it decides which values to check and
the cycle starts over.

Spearmint will search indefinitely, so it’s your call to stop. With
hyperopt, on the other hand, you decide how many iterations you’d like
to run. One needs to give the optimizer enough tries to find
near-optimal results. 50-100 iterations seems like a good initial guess,
depending on the number of hyperparams.

hyperopt’s choice
-----------------

Perhaps the most interesting feature of hyperopt is conditional choice.
Consider for example that you’d like to tune a neural network. You’d
like one or two hidden layers, but are not sure. With software like
Spearmint, which doesn’t allow conditionals, you’d be forced to run a
separate experiment for both options. That’s because parameters for the
second layer will be meaningless if there’s only one layer in the
particular test. It could confuse the optimizer.

With `choice` you can specify sophisticated structures, for example test
two classifiers, each with its own set of hyperparams. This example is
adapted from the [hyperopt’s
docs](https://github.com/hyperopt/hyperopt/wiki/FMin):

    space = hp.choice('classifier_type', [
        {
            'type': 'naive_bayes',
        },
        {
            'type': 'svm',
            'C': hp.lognormal('svm_C', 0, 1),
            'kernel': hp.choice('svm_kernel', [
                {'ktype': 'linear'},
                {'ktype': 'RBF', 'width': hp.lognormal('svm_rbf_width', 0, 1)},
            ]),
        }
    ])

Naive Bayes has no hyperparams to tune, while SVM have a few, including
the choice of a kernel.

Parameter ranges
----------------

When you have a numeric parameter to optimize, the first question is:
discrete or continuous? For example, a number of units in a neural
network layer is an integer, while amount of L2 regularization is a real
number.

A more subtle and maybe more important issue is how to probe values, or
in other words, what’s their distribution. Hyperopt offers four options
here: uniform, normal, log-uniform and
[log-normal](http://en.wikipedia.org/wiki/Log-normal_distribution).

Let’s use an example to understand the importance of log distributions:
for some params, like regularization, the distinction among small values
is important. Maybe we think the range for regularization param should
be 0 to 1, but most likely it will be a small value like 0.01 or 0.001.
Therefore we don’t want the optimizer to probe the range uniformly,
because it would waste time trying to distinguish between 0.4 and 0.6
when in fact there’s not much difference, because both these values are
too big.

Instead we use the log-uniform, or possibly log-normal distribution so
that the optimizer could make fine distinctions with small values. We
describe the log-uniform distibution below in more detail.

Log-uniform distribution
------------------------

Here’s some Matlab/Octave code to visualize a log-uniform distribution:

    u = rand( 100000, 1 );
    bins = 20;
    hist( u, bins )
    hist( exp( u ), bins )
    hist( exp( u * 5 ), bins )

![uniform](/images/hyperopt/thumbs/histogram_uniform.png)
![log-uniform](/images/hyperopt/thumbs/histogram_exp.png) ![log-uniform
times five](/images/hyperopt/thumbs/histogram_exp_times_five.png)\

You can see that exponential/log-uniform distribution can have a heavier
or a lighter tail, but either way smaller values are more common.

The search space
----------------

This table of hyperparams to optimize might give you an idea when to use
each distribution:

![a table of hyperparams](/images/hyperopt/hyperparams_table.png)
Source: *Algorithms for Hyper-Parameter Optimization*
[[PDF](books.nips.cc/papers/files/nips24/NIPS2011_1385.pdf)].

And here’s our search space definition for ELM:

    space = ( 
        hp.qloguniform( 'n_hidden', log( 10 ), log( 1000 ), 1 ),
        hp.uniform( 'alpha', 0, 1 ),
        hp.loguniform( 'rbf_width', log( 1e-5 ), log( 100 )),
        hp.choice( 'activation_func', [ 'tanh', 'sine', 'tribas', 'inv_tribas', 'sigmoid', \
            'hardlim', 'softlim', 'gaussian', 'multiquadric','inv_multiquadric' ] )
    )

With log-distributions we need to supply `log()`s of parameter value
limits - doing so explicitly allows us to think in natural ranges of
values. The number of hidden units is a discrete quantity so we use the
`qloguniform` option.

Logging the results
-------------------

After optimization is complete we’d like to look at settings explored
and scores achieved, so we save them to a file. We have a wrapper
function which takes care of that. We pass the wrapper function to
hyperopt, the wrapper calls the proper function, saves the params and
the result and returns the error value to hyperopt.

Lessons learned
---------------

ELM involve randomness, meaning that with the same hyperparams the error
will differ - hopefully slightly - from run to run. We need to account
for this in optimizing the parameters and the way to do this is to run
each setting a few times and average the results. We missed this point
initially so the scripts run each setting just once. Luckily it doesn’t
seem to make that much difference in this case.

Here are a few more points (regarding this specific dataset, possibly
generalizing to other):

-   Standardize data first. We were reluctant to standardize one-hot
    indicator columns for categorical variables, but it seems it’s
    better to standardize them too.

-   Pipeline setup runs faster than normal ELM, probably because Ridge
    is faster than ELM’s built-in linear model.

-   Ridge also offers a better performance in terms of score.

-   Rectified linear activation seems to work just as well as other
    functions (RMSE 4.8 in validation)

-   The results from two hidden layers are basically the same as with
    one hidden layer.

-   There are different hyperparam combinations resulting in similiar
    performance. You can bag the models. We bagged a few models and got
    a slight improvement, from RMSE roughly 4.8 to 4.4 in validation.

Predictions from the best pipeline/Ridge model score 5.13 on the public
leaderboard.

Posted by Zygmunt Z. 2014-06-25 18:24
[Kaggle](/blog/categories/kaggle/), [code](/blog/categories/code/),
[hyperparams](/blog/categories/hyperparams/),
[software](/blog/categories/software/)

[Tweet](http://twitter.com/share)

[« How a Russian mathematician constructed a decision tree - by hand -
to solve a medical
problem](/how-a-russian-mathematician-constructed-a-decision-tree-by-hand-to-solve-a-medical-problem/ "Previous Post: How a Russian mathematician constructed a decision tree - by hand - to solve a medical problem")

Comments
========

Please enable JavaScript to view the [comments powered by
Disqus.](http://disqus.com/?ref_noscript)

Recent Posts
============

-   [Extreme Learning Machines and optimizing hyperparams with
    hyperopt](/extreme-learning-machines-and-optimizing-hyperparams-with-hyperopt/)
-   [How a Russian mathematician constructed a decision tree - by hand -
    to solve a medical
    problem](/how-a-russian-mathematician-constructed-a-decision-tree-by-hand-to-solve-a-medical-problem/)
-   [Yann LeCun's answers from the Reddit
    AMA](/yann-lecuns-answers-from-the-reddit-ama/)
-   [Impute missing values with
    Amelia](/impute-missing-values-with-amelia/)
-   [Converting categorical data into numbers with Pandas and
    Scikit-learn](/converting-categorical-data-into-numbers-with-pandas-and-scikit-learn/)
-   [Predicting happiness from demographics and poll
    answers](/predicting-happiness-from-demographics-and-poll-answers/)
-   [Deep learning these days](/deep-learning-these-days/)

Twitter
=======

Follow [@fastml](http://twitter.com/fastml) for notifications about new
posts.

-   Status updating...

[Follow @fastml](http://twitter.com/fastml) \
\
 Also check out [@fastml\_extra](https://twitter.com/fastml_extra) for
things related to machine learning and data science in general.

GitHub
======

Most articles come with some [code](/blog/categories/code/). We push it
to Github.

[https://github.com/zygmuntz](https://github.com/zygmuntz)

Copyright © 2014 - Zygmunt Z. - Powered by
[Octopress](http://octopress.org)

This markdown document has been converted from the html document located at:
http://fastml.com/extreme-learning-machines-and-optimizing-hyperparams-with-hyperopt/
