[The Tokenizer](http://thetokenizer.com/ "The Tokenizer")
=========================================================

Here’s a few things you might need to know, or maybe you just forgot…
---------------------------------------------------------------------

Menu
====

[Skip to content](#content "Skip to content")

-   [Home](http://thetokenizer.com/)

Post navigation
===============

[← Practicing
Backtracking](http://thetokenizer.com/2013/01/13/practicing-backtracking/)

[An Efficient Way to Extract the Main Topics from a Sentence
→](http://thetokenizer.com/2013/05/09/efficient-way-to-extract-the-main-topics-of-a-sentence/)

[April 28,
2013](http://thetokenizer.com/2013/04/28/build-your-own-summary-tool/ "1:49 PM")

Build your own summary tool!
============================

By [Shlomi
Babluki](http://thetokenizer.com/author/shlomibabluki/ "View all posts by Shlomi Babluki")
¶ ¶ Tagged [auto
summarization](http://thetokenizer.com/tag/auto-summarization/),
[nlp](http://thetokenizer.com/tag/nlp/),
[nltk](http://thetokenizer.com/tag/nltk/),
[opennlp](http://thetokenizer.com/tag/opennlp/),
[python](http://thetokenizer.com/tag/python/),
[summarization](http://thetokenizer.com/tag/summarization/),
[summary](http://thetokenizer.com/tag/summary/),
[summly](http://thetokenizer.com/tag/summly/) ¶ [21
Comments](http://thetokenizer.com/2013/04/28/build-your-own-summary-tool/#comments "Comment on Build your own summary tool!")

After Yahoo! acquired Summly and Google acquired Wavii, there is no
doubt that auto summarization technologies are a hot topic in the
industry. So as a NLP freak, I decided to give a quick overview and
hands-on experience on how these technologies actually work.

Since some of you might not read the entire post, I decided to start
with the bottom line –
**[Here](https://gist.github.com/shlomibabluki/5473521)** you can find
my Python implementation for a very naive summarization algorithm. This
algorithm extracts the key sentence from each paragraph in the text. The
algorithm works nicely on news & blog articles, and it usually cuts
around 60-70% of the original text. As an example I ran it on [this
article](http://thenextweb.com/apps/2013/03/21/swayy-discover-curate-content/)(about
my company, [Swayy](http://www.swayy.co/)), and got the following
results:

* * * * *

Swayy is a beautiful new dashboard for discovering and curating online
content [Invites]

Lior Degani, the Co-Founder and head of Marketing of Swayy, pinged me
last week when I was in California to tell me about his startup and give
me beta access.\
 One week later, I’m totally addicted to Swayy and glad I said nothing
about the spam (it doesn’t send out spam tweets but I liked the line too
much to not use it for this article).\
 What is Swayy? It’s like Percolate and LinkedIn recommended articles,
mixed with trending keywords for the topics you find interesting,
combined with an analytics dashboard that shows the trends of what you
do and how people react to it.\
 After I decided that I trusted the service, I added my Facebook and
LinkedIn accounts.\
 The analytics side isn’t as interesting for me right now, but that
could be due to the fact that I’ve barely been online since I came back
from the US last weekend.\
 It was the suggested content that impressed me the most.\
 Yes, we’re in the process of filing a patent for it.\
 Ohad Frankfurt is the CEO, Shlomi Babluki is the CTO and Oz Katz does
Product and Engineering, and I [Lior Degani] do Marketing.\
 ➤ Want to try Swayy out without having to wait? Go to this secret URL
and enter the promotion code thenextweb

**\
 Original Length 4529**\
 **Summary Length 1307**\
 **Summary Ratio: 71.1415323471**

* * * * *

**Summarization Technologies:**

Today there are two common approaches to “attacking” the summarization
mission. The first approach tries to analyze the text, and to rewrite or
rephrase it in a short way. As far as I know, until today this approach
didn’t achieve any substantial results. The second approach ,which is
similar to my naive algorithm, tries to extract the key sentences from
the text, and then tries to put them together properly. One famous
algorithm that implements this approach is
[TextRank](http://en.wikipedia.org/wiki/Automatic_summarization#Unsupervised_keyphrase_extraction:_TextRank).

**Our Summarization Algorithm**

I’m going to explain step-by-step my naive algorithm. I’ll use both
programming and computer science terminology. Before you continue, in
case you didn’t do it already, I suggest to to take a quick look at [the
code](https://gist.github.com/shlomibabluki/5473521).

The intersection function:

This function receives two sentences, and returns a score for the
intersection between them.\
 We just split each sentence into words/tokens, count how many common
tokens we have, and then we normalize the result with the average length
of the two sentences.\
 *Computer Science**:* f(s1, s2) = |{w | w in s1 and w in s2}| / ((|s1|
+ |s2|) / 2)\
 **\
**

The sentences dictionary:

This part is actually the “Heart” of the algorithm. It receives our text
as input, and calculates a score for each sentence. The calculations is
composed of two steps:

In the first step we split the text into sentences, and store the
intersection value between each two sentences in a matrix
(two-dimensional array). So values[0][2] will hold the intersection
score between sentence \#1 and sentence \#3.\
 *Computer Science:* In fact, we just converted our text into a
fully-connected weighted graph! Each sentence is a node in the graph and
the two-dimensional array holds the weight of each edge!

In the second step we calculate an individual score for each sentence
and store it in a key-value dictionary, where the sentence itself is the
key and the value is the total score. We do that just by summing up all
its intersections with the other sentences in the text (not including
itself).\
 *Computer Science:*We calculates the score for each node in our graph.
We simply do that by summing all the edges that are connected to the
node.

Building the summary:

Obviously, the final step of our algorithm is generating the final
summary. We do that by splitting our text into paragraphs, and then we
choose the best sentence from each paragraph according to our sentences
dictionary.\
 *Computer Science:*The Idea here is that every paragraph in the text
represents some **logical** subset of our graph, so we just pick the
most valuable node from each subset!

**Why this works**

There are two main reasons why this algorithm works: The first (and
obvious) reason is that a paragraph is a logical atomic unit of the
text. In simple words – there is probably a very good reason why the
author decided to split his text that way. The second (and maybe less
obvious..) reason is that if two sentences have a good intersection,
they probably holds the same information. So if one sentence has a good
intersection with many other sentences, it probably holds some
information from each one of them- or in other words, this is probably a
key sentence in our text!

**Your turn!**

If you read until here, you probably want to play a little bit with the
algorithm. Here are a few things you can try to do:

\1. In order to make it more useful, you may wrap it with some tool that
extracts content from a URL. I personally like
[Goose](https://github.com/xgdlm/python-goose), but you may use other
tools like Readability,
[boilerpipe](https://code.google.com/p/boilerpipe/) or others. It
obviously won’t improve your algorithm, but it can be really nice just
to pick a URL and see the resulting summary.

\2. In my code I intentionally didn’t use any other packages. You can
explore the [NLTK](http://nltk.org/) or
[OpenNLP](http://opennlp.apache.org/) packages and use their methods for
splitting and tokenizing text. They usually provide much better methods
for that.

\3. Play with the intersection function. As I already wrote, you may use
stopwords or stemming (these both tools are included in NLTK!) and see
how they change the result. You can also play with the normalization
part of the equation (try to divide the result with different factors).

\4. Create a new variation of the algorithm. For example, instead of
picking the best sentence from each paragraph, try and pick the 2-3 most
important paragraphs (In this case- each node of your graph is a full
paragraph, instead of a single sentence!)

\5. Use the title! – In my code I just print it at the top of the
summary, but I’m sure it can be useful (For example – try to add it with
some factor in your intersection function).

\6. Check it on other input languages – Although I tested this code only
on English, theoretically it should work in any other language!

Obviously this algorithm is just a simple Proof of concept, but I hope
it gave you some general knowledge about how summarization technologies
works. I also have to mention that in this post I introduced only one
approach to complete this task, while indeed there are a few others!

Leave your comments below, or just contact me directly on
[Twitter](https://twitter.com/ShlomiBabluki).

Shlomi Babluki.

[About these ads](http://en.wordpress.com/about-these-ads/)

### Share this:

-   -   -   -   -   -   

### Like this:

Like Loading...

21 thoughts on “Build your own summary tool!”
---------------------------------------------

1.  ![image](http://1.gravatar.com/avatar/acbf14140b62a5fe73217d9e6d67badf?s=68&d=identicon&r=G)
    jslo says:

    [April 28, 2013 at 6:24
    PM](http://thetokenizer.com/2013/04/28/build-your-own-summary-tool/comment-page-1/#comment-68)

    Great post! That reminded me what computer science is all about.

    [Reply](/2013/04/28/build-your-own-summary-tool/?replytocom=68#respond)

2.  ![image](http://i0.wp.com/a0.twimg.com/profile_images/2400957335/rystyrhbjp2y0eh4cbg5_normal.jpeg?resize=68%2C68)
    [John Berryman (@JnBrymn)](http://twitter.com/JnBrymn) says:

    [April 29, 2013 at 4:17
    AM](http://thetokenizer.com/2013/04/28/build-your-own-summary-tool/comment-page-1/#comment-76)

    Interesting! I didn’t know that the main idea was so simple. I’ve
    been playing with similar stuff with Solr and Mahout
    ([http://bit.ly/14F3KLz](http://bit.ly/14F3KLz)). This all makes me
    wonder: if you took all the sentences in a large corpus (you name
    it, all of the New York Post, the complete works of Shakespeare, the
    Bible), and applied this strategy. What would be the “central
    sentence” in the entire collection. What’s more, I would be
    interested in different centrality metrics. (In particular, I’m
    thinking about the eigenvector centrality metric.)

    [Reply](/2013/04/28/build-your-own-summary-tool/?replytocom=76#respond)

    -   ![image](http://0.gravatar.com/avatar/6d8a0bc2be27a2d1f25dc44164ae7757?s=39&d=identicon&r=G)
        Marcus says:

        [May 8, 2013 at 1:58
        PM](http://thetokenizer.com/2013/04/28/build-your-own-summary-tool/comment-page-1/#comment-117)

        Putting this over a complete corpus might lead to interesting
        results. Interesting, not necessarily useful, though.

        It might be fun to look at chapter basis (some deliminator for
        chapters would have to be found).

        Iterating the whole thing to smaller and smaller sizes might
        also work.

        i.e. Summarize each chapter, then summarize the corpus of
        chapter summaries.

        Hope I am making sense.

        [Reply](/2013/04/28/build-your-own-summary-tool/?replytocom=117#respond)

3.  ![image](http://i2.wp.com/a0.twimg.com/profile_images/3018159608/48b7f58fc5da9f1c7ee094bfe02d1c57_normal.jpeg?resize=68%2C68)
    [Chris Ismael (@craftshape)](http://twitter.com/craftshape) says:

    [April 29, 2013 at 6:14
    AM](http://thetokenizer.com/2013/04/28/build-your-own-summary-tool/comment-page-1/#comment-77)

    Hey man this is awesome! I turned it into an API here –
    [https://www.mashape.com/ismaelc/summarizer-tool\#!documentation](https://www.mashape.com/ismaelc/summarizer-tool#!documentation)
    I will blog tomorrow on how I turned it into an API, and will turn
    it into another API that accepts just a URL as you suggested.

    [Reply](/2013/04/28/build-your-own-summary-tool/?replytocom=77#respond)

4.  ![image](http://0.gravatar.com/avatar/92f0709dd48cc05bab567064973074d2?s=68&d=identicon&r=G)
    [phaniroy](http://phaniroy.wordpress.com) says:

    [April 29, 2013 at 2:27
    PM](http://thetokenizer.com/2013/04/28/build-your-own-summary-tool/comment-page-1/#comment-79)

    kudos to you!!

    [Reply](/2013/04/28/build-your-own-summary-tool/?replytocom=79#respond)

5.  ![image](http://2.gravatar.com/avatar/2a28c28368abb3b64141d4548c9eb2a6?s=68&d=identicon&r=G)
    [James Brooks](http://james.brooks.so) says:

    [April 29, 2013 at 4:33
    PM](http://thetokenizer.com/2013/04/28/build-your-own-summary-tool/comment-page-1/#comment-80)

    I created a Node.js module using the same algorithm and
    Underscore.js
    [https://npmjs.org/package/node-summary](https://npmjs.org/package/node-summary)

    It doesn’t bring back quite the same results, but for the little
    amount of time I spent on it, it seems to work well enough.

    You can check the source at
    [https://github.com/jbrooksuk/node-summary](https://github.com/jbrooksuk/node-summary)

    [Reply](/2013/04/28/build-your-own-summary-tool/?replytocom=80#respond)

    -   ![image](http://2.gravatar.com/avatar/2a28c28368abb3b64141d4548c9eb2a6?s=39&d=identicon&r=G)
        [jbrooksuk](http://www.james-brooks.net) says:

        [May 7, 2013 at 10:27
        AM](http://thetokenizer.com/2013/04/28/build-your-own-summary-tool/comment-page-1/#comment-110)

        Just a quick reply. I updated the Node.js module to combine the
        two “public” facing methods and renamed it. It’s also now fully
        asynchronous.

        [Reply](/2013/04/28/build-your-own-summary-tool/?replytocom=110#respond)

6.  Pingback: [Python News Roundup – 5/3 |
    Docstrings](http://docstrings.com/2013/05/03/python-news-roundup-53/)

7.  ![image](http://2.gravatar.com/avatar/b98f52ee9944357a8ef16191460ee393?s=68&d=identicon&r=G)
    anton says:

    [May 4, 2013 at 3:53
    AM](http://thetokenizer.com/2013/04/28/build-your-own-summary-tool/comment-page-1/#comment-102)

    Thanks for the insight. That was a really nice and informative
    article.

    [Reply](/2013/04/28/build-your-own-summary-tool/?replytocom=102#respond)

8.  Pingback: [Web-enable your Research/Project with an API |
    Chrispogeek](http://chrispogeek.wordpress.com/2013/05/08/web-enable-your-researchproject-with-an-api/)

9.  Pingback: [An Efficient Way to Extract the Main Topics from a
    Sentence | The
    Tokenizer](http://thetokenizer.com/2013/05/09/efficient-way-to-extract-the-main-topics-of-a-sentence/)

10. ![image](http://0.gravatar.com/avatar/0ffcaf053e362c51a2e495ada17de968?s=68&d=identicon&r=G)
    [Eugene](http://eugeneciurana.com) says:

    [May 10, 2013 at 8:00
    PM](http://thetokenizer.com/2013/04/28/build-your-own-summary-tool/comment-page-1/#comment-136)

    Very cool article! The naïve algorithm you implemented is quite
    effective. I like the legibility of Python as well. Thanks for
    sharing, Shlomi!

    [Reply](/2013/04/28/build-your-own-summary-tool/?replytocom=136#respond)

11. ![image](http://2.gravatar.com/avatar/e5238cd1858a12798659f5b2fdf64d4b?s=68&d=identicon&r=G)
    [ramya vemuganti](http://gravatar.com/ramyavemuganti) says:

    [July 13, 2013 at 4:32
    AM](http://thetokenizer.com/2013/04/28/build-your-own-summary-tool/comment-page-1/#comment-355)

    Hi, can I know why you are considering normalizing the sentence
    intersection, instead of considering just the intersection as a
    metric?What is the significance of dividing it by the average length
    of sentences?

    [Reply](/2013/04/28/build-your-own-summary-tool/?replytocom=355#respond)

12. ![image](http://2.gravatar.com/avatar/53f7650703829804b3ea28c40745d74d?s=68&d=identicon&r=G)
    [SUEM](http://gravatar.com/shehanmunasinghe) says:

    [July 24, 2013 at 12:53
    PM](http://thetokenizer.com/2013/04/28/build-your-own-summary-tool/comment-page-1/#comment-370)

    Is this working?

    [Reply](/2013/04/28/build-your-own-summary-tool/?replytocom=370#respond)

13. Pingback: [The Importance of sentences in articles | Something
    Catchy](http://jackdschultz.com/index.php/2013/08/28/the-importance-of-sentences-in-articles/)

14. Pingback: [GPS Trivia | -(Lab \*)
    oneTwoClick](http://lab.onetwoclick.com/gps-trivia/)

15. ![image](http://0.gravatar.com/avatar/cf673bf1ce8d3e412110432969ca7af0?s=68&d=identicon&r=G)
    nima says:

    [September 20, 2013 at 9:28
    AM](http://thetokenizer.com/2013/04/28/build-your-own-summary-tool/comment-page-1/#comment-403)

    hey can any one write the above code in perl language plz post it
    urgently

    [Reply](/2013/04/28/build-your-own-summary-tool/?replytocom=403#respond)

16. ![image](http://0.gravatar.com/avatar/04a3ca8cd41bf8e170b901072fcc667f?s=68&d=identicon&r=G)
    matansa says:

    [October 19, 2013 at 11:20
    PM](http://thetokenizer.com/2013/04/28/build-your-own-summary-tool/comment-page-1/#comment-415)

    I think there’s a third reason to “why this ‘works’” beyond the
    mentioned two. It looks like this ‘works’ because as all picked
    sentences are the highest score in their paragraph (following the
    naive algorithm, not necessarilty the suggested variations), they
    hold a lot of commonality among themselves. This makes the
    conglomeration of them as one sequential piece of output, have a
    feel of context persistency, and thus the result reads easily, even
    if it isn’t a good semantic summary of the original text. I think
    that’s an important distinction to make about this algorithm.

    What do you think?

    [Reply](/2013/04/28/build-your-own-summary-tool/?replytocom=415#respond)

17. Pingback: [自动摘要算法 |
    isnowfy](http://www.isnowfy.com/automatic-summarization/)

18. Pingback: [自动摘要算法 | 极客我爱你
    geek521.com](http://www.geek521.com/?p=3780)

19. ![image](http://0.gravatar.com/avatar/97ff44cc113b9eab71005c3caf873d38?s=68&d=identicon&r=G)
    [Phil Plückthun](http://gravatar.com/philplckthun) says:

    [December 16, 2013 at 2:20
    PM](http://thetokenizer.com/2013/04/28/build-your-own-summary-tool/comment-page-1/#comment-421)

    Based on James Brook’s code I created a new node-module summarizing
    web articles.\
     The algorithm is a bit different: It now compares the sentences
    across paragraphs.\

    [https://github.com/philplckthun/node-sumuparticles](https://github.com/philplckthun/node-sumuparticles)

    [Reply](/2013/04/28/build-your-own-summary-tool/?replytocom=421#respond)

### Leave a Reply [Cancel reply](/2013/04/28/build-your-own-summary-tool/#respond)

Enter your comment here...

Fill in your details below or click an icon to log in:

-   [](#comment-form-guest "Guest")
-   [](#comment-form-load-service:WordPress.com "WordPress.com")
-   [](#comment-form-load-service:Twitter "Twitter")
-   [](#comment-form-load-service:Facebook "Facebook")
-   

[![Gravatar](http://1.gravatar.com/avatar/ad516503a11cd5ca435acc9bb6523536?s=25&d=identicon&forcedefault=y&r=G)](https://gravatar.com/site/signup/)

Email (required) (Address never made public)

Name (required)

Website

![WordPress.com
Logo](http://s2.wp.com/wp-content/mu-plugins/highlander-comments/images/wplogo.png?m=1391188133g)

**** You are commenting using your WordPress.com account. ( [Log
Out](javascript:HighlanderComments.doExternalLogout(%20'wordpress'%20);)
/ [Change](#) )

![Twitter
picture](http://1.gravatar.com/avatar/ad516503a11cd5ca435acc9bb6523536?s=25&d=identicon&forcedefault=y&r=G)

**** You are commenting using your Twitter account. ( [Log
Out](javascript:HighlanderComments.doExternalLogout(%20'twitter'%20);) /
[Change](#) )

![Facebook
photo](http://1.gravatar.com/avatar/ad516503a11cd5ca435acc9bb6523536?s=25&d=identicon&forcedefault=y&r=G)

**** You are commenting using your Facebook account. ( [Log
Out](javascript:HighlanderComments.doExternalLogout(%20'facebook'%20);)
/ [Change](#) )

![Google+
photo](http://1.gravatar.com/avatar/ad516503a11cd5ca435acc9bb6523536?s=25&d=identicon&forcedefault=y&r=G)

**** You are commenting using your Google+ account. ( [Log
Out](javascript:HighlanderComments.doExternalLogout(%20'googleplus'%20);)
/ [Change](#) )

[Cancel](javascript:HighlanderComments.cancelExternalWindow();)

Connecting to %s

Notify me of follow-up comments via email.

### Follow me on Twitter

My Tweets

### Recent Posts

-   [An Algorithm For Generating Automatic
    Hashtags](http://thetokenizer.com/2013/09/20/an-algorithm-for-generating-automatic-hashtags/)
-   [How To Easily Recognize People’s Names In A
    Text](http://thetokenizer.com/2013/08/25/how-to-easily-recognize-peoples-names-in-a-text/)
-   [An Efficient Way to Extract the Main Topics from a
    Sentence](http://thetokenizer.com/2013/05/09/efficient-way-to-extract-the-main-topics-of-a-sentence/)
-   [Build your own summary
    tool!](http://thetokenizer.com/2013/04/28/build-your-own-summary-tool/)
-   [Practicing
    Backtracking](http://thetokenizer.com/2013/01/13/practicing-backtracking/)

-   [RSS - Posts](http://thetokenizer.com/feed/ "Subscribe to Posts")

[Blog at WordPress.com](http://wordpress.com/?ref=footer). | [The Runo
Lite
Theme](http://theme.wordpress.com/themes/runo-lite/ "Learn more about this theme").

[Follow](javascript:void(0))

### Follow “The Tokenizer”

Get every new post delivered to your Inbox.

Join 187 other followers

[Powered by WordPress.com](http://wordpress.com/signup/?ref=lof)

%d bloggers like this:

![image](http://stats.wordpress.com/b.gif?v=noscript)

This markdown document has been converted from the html document located at:
http://thetokenizer.com/2013/04/28/build-your-own-summary-tool/
