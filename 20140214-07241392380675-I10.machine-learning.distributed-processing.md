[![The Netflix Tech
Blog](http://3.bp.blogspot.com/-Kivs7N-IASs/Tte7CoF7noI/AAAAAAAAALQ/MSSIzJcOuXs/s1600/techblog-header.png)](http://techblog.netflix.com/)

Monday, February 10, 2014
-------------------------

### Distributed Neural Networks with GPUs in the AWS Cloud

by Alex Chen, [Justin Basilico](https://twitter.com/JustinBasilico), and
[Xavier Amatriain](https://twitter.com/xamat)\
 \
 As we have described
[previously](http://techblog.netflix.com/2012/04/netflix-recommendations-beyond-5-stars.html)
on this blog, at Netflix we are constantly innovating by looking for
better ways to find the best movies and TV shows for our members. When a
new algorithmic technique such as Deep Learning shows promising results
in other domains (e.g. [Image
Recognition](http://www.nytimes.com/2012/06/26/technology/in-a-big-network-of-computers-evidence-of-machine-learning.html),
[Neuro-imaging](http://yann.lecun.com/exdb/publis/pdf/mirowski-mlsp-08.pdf),
[Language
Models](http://www.iro.umontreal.ca/~bengioy/ift6266/H13/nlp.pdf), and
[Speech
Recognition](https://www.cs.toronto.edu/~hinton/absps/DNN-2012-proof.pdf)),
it should not come as a surprise that we would try to figure out how to
apply such techniques to improve our product. In this post, we will
focus on what we have learned while building infrastructure for
experimenting with these approaches at Netflix. We hope that this will
be useful for others working on similar algorithms, especially if they
are also leveraging the Amazon Web Services (AWS) infrastructure.
However, we will not detail how we are using variants of Artificial
Neural Networks for personalization, since it is an active area of
research.\
 \
 Many researchers have pointed out that most of the algorithmic
techniques used in the trendy Deep Learning approaches have been known
and available for some time. Much of the more recent innovation in this
area has been around making these techniques feasible for real-world
applications. This involves designing and implementing architectures
that can execute these techniques using a reasonable amount of resources
in a reasonable amount of time. The [first successful
instance](http://www.cs.toronto.edu/~ranzato/publications/DistBeliefNIPS2012_withAppendix.pdf)
of large-scale Deep Learning made use of 16000 CPU cores in 1000
machines in order to train an Artificial Neural Network in a matter of
days. While that was a remarkable milestone, the required
infrastructure, cost, and computation time are still not practical.\
 \
 Andrew Ng and his team addressed this issue in [follow up
work](http://www.wired.com/wiredenterprise/2013/06/andrew_ng/). [Their
implementation](http://stanford.edu/~acoates/papers/CoatesHuvalWangWuNgCatanzaro_icml2013.pdf)
used GPUs as a powerful yet cheap alternative to large clusters of CPUs.
Using this architecture, they were able to train a model 6.5 times
larger in a few days using only 3 machines. In [another
study](http://www-lium.univ-lemans.fr/~schwenk/papers/Schwenk.naacl-lm2012.pdf),
Schwenk et al. showed that training these models on GPUs can improve
performance dramatically, even when comparing to high-end multicore
CPUs.\
 \
 Given our well-known approach and
[leadership](http://www.networkworld.com/news/2013/072413-netflix-cloud-272141.html)
in cloud computing, we sought out to implement a large-scale Neural
Network training system that leveraged both the advantages of GPUs and
the AWS cloud. We wanted to use a reasonable number of machines to
implement a powerful machine learning solution using a Neural Network
approach. We also wanted to avoid needing special machines in a
dedicated data center and instead leverage the full, on-demand computing
power we can obtain from AWS.\
 \
 In architecting our approach for leveraging computing power in the
cloud, we sought to strike a balance that would make it fast and easy to
train Neural Networks by looking at the entire training process. For
computing resources, we have the capacity to use many GPU cores, CPU
cores, and AWS instances, which we would like to use efficiently. For an
application such as this, we typically need to train not one, but
multiple models either from different datasets or configurations (e.g.
different international regions). For each configuration we need to
perform hyperparameter tuning, where each combination of parameters
requires training a separate Neural Network. In our solution, we take
the approach of using GPU-based parallelism for training and using
distributed computation for handling hyperparameter tuning and different
configurations.\
 \

### Distributing Machine Learning: At what level?

\
 Some of you might be thinking that the scenario described above is not
what people think of as a distributed Machine Learning in the
traditional sense. For instance, in the work by Ng et al. cited above,
they distribute the learning algorithm itself between different
machines. While that approach might make sense in some cases, we have
found that to be not always the norm, especially when a dataset can be
stored on a single instance. To understand why, we first need to explain
the different levels at which a model training process can be
distributed.\
 \
 In a standard scenario, we will have a particular model with multiple
instances. Those instances might correspond to different partitions in
your problem space. A typical situation is to have different models
trained for different countries or regions since the feature
distribution and even the item space might be very different from one
region to the other. This represents the first initial level at which we
can decide to distribute our learning process. We could have, for
example, a separate machine train each of the 41 countries where Netflix
operates, since each region can be trained entirely independently.\
 \
 However, as explained above, training a single instance actually
implies training and testing several models, each corresponding to a
different combinations of hyperparameters. This represents the second
level at which the process can be distributed. This level is
particularly interesting if there are many parameters to optimize and
you have a good strategy to optimize them, like Bayesian optimization
with Gaussian Processes. The only communication between runs are
hyperparameter settings and test evaluation metrics.\
 \
 Finally, the algorithm training itself can be distributed. While this
is also interesting, it comes at a cost. For example, training ANN is a
comparatively communication-intensive process. Given that you are likely
to have thousands of cores available in a single GPU instance, it is
very convenient if you can squeeze the most out of that GPU and avoid
getting into costly across-machine communication scenarios. This is
because communication within a machine using memory is usually much
faster than communication over a network.\
 \
 The following pseudo code below illustrates the three levels at which
an algorithm training process like us can be distributed.\
 \
 for each region -\> level 1 distribution\
 for each hyperparameter combination -\> level 2 distribution\
 train model -\> level 3 distribution\
 end for\
 end for\
 \
 In this post we will explain how we addressed level 1 and 2
distribution in our use case. Note that one of the reasons we did not
need to address level 3 distribution is because our model has millions
of parameters (compared to the billions in the original paper by Ng).\
 \

### Optimizing the CUDA Kernel

\
 Before we addressed distribution problem though, we had to make sure
the GPU-based parallel training was efficient. We approached this by
first getting a proof-of-concept to work on our own development machines
and then addressing the issue of how to scale and use the cloud as a
second stage. We started by using a Lenovo S20 workstation with a
[Nvidia Quadro
600](http://www.nvidia.com/object/product-quadro-600-us.html) GPU. This
GPU has 98 cores and provides a useful baseline for our experiments;
especially considering that we planned on using a more powerful machine
and GPU in the AWS cloud. Our first attempt to train our Neural Network
model took 7 hours.\
 \
 We then ran the same code to train the model in on a EC2’s cg1.4xlarge
instance, which has a more powerful [Tesla
M2050](http://www.nvidia.com/object/tesla-servers.html) with 448 cores.
However, the training time jumped from 7 to over 20 hours. Profiling
showed that most of the time was spent on the function calls to Nvidia
Performance Primitive library, e.g. nppsMulC\_32f\_I, nppsExp\_32f\_I.
Calling the npps functions repeatedly took 10x more system time on the
cg1 instance than in the Lenovo S20.\
 \
 While we tried to uncover the root cause, we worked our way around the
issue by reimplementing the npps functions using the customized cuda
kernel, e.g. replace nppsMulC\_32f\_I function with:\
 \
 \_\_global\_\_\
 void KernelMulC(float c, float \*data, int n)\
 {\
 int i = blockIdx.x \* blockDim.x + threadIdx.x;\
 if (i < n) {\
 data[i] = c \* data[i];\
 }\
 }\
 \
 Replacing all npps functions in this way for the Neural Network code
reduced the total training time on the cg1 instance from over 20 hours
to just 47 minutes when training on 4 million samples. Training 1
million samples took 96 seconds of GPU time. Using the same approach on
the Lenovo S20 the total training time also reduced from 7 hours to 2
hours. This makes us believe that the implementation of these functions
is suboptimal regardless of the card specifics.\
 \

### PCI configuration space and virtualized environments

\
 While we were implementing this “hack”, we also worked with the AWS
team to find a principled solution that would not require a kernel
patch. In doing so, we found that the performance degradation was
related to the NVreg\_CheckPCIConfigSpace parameter of the kernel.
[According to
RedHat](http://www.linux-kvm.org/wiki/images/e/ed/Kvm-forum-2013-VFIO-VGA.pdf),
setting this parameter to 0 disables very slow accesses to the PCI
configuration space. In a virtualized environment such as the AWS cloud,
these accesses cause a trap in the hypervisor that results in even
slower access.\
 \
 NVreg\_CheckPCIConfigSpace is a parameter of kernel module
nvidia-current, that can be set using:\
 \
 sudo modprobe nvidia-current NVreg\_CheckPCIConfigSpace=0\
 \
 We tested the effect of changing this parameter using a benchmark that
calls MulC repeatedly (128x1000 times). Below are the results (runtime
in sec) on our cg1.4xlarge instances:\
 \

  ------------ ------------ ------------
               KernelMulC   npps\_MulC
  CheckPCI=1   3.37         103.04
  CheckPCI=0   2.56         6.17
  ------------ ------------ ------------

\
 As you can see, disabling accesses to PCI space had a spectacular
effect in the original npps functions, decreasing the runtime by 95%.
The effect was significant even in our optimized Kernel functions saving
almost 25% in runtime. However, it is important to note that even when
the PCI access is disabled, our customized functions performed almost
60% better than the default ones.\
 \
 We should also point out that there are other options, which we have
not explored so far but could be useful for others. First, we could look
at optimizing our code by applying a [kernel
fusion](http://arxiv.org/abs/1305.1183) trick that combines several
computation steps into one kernel to reduce the memory access. Finally,
we could think about using
[Theano](http://conference.scipy.org/proceedings/scipy2010/pdfs/bergstra.pdf),
the GPU Match compiler in Python, which is supposed to also improve
performance in these cases.\
 \

### G2 Instances

\
 While our initial work was done using cg1.4xlarge EC2 instances, we
were interested in moving to the new EC2 GPU g2.2xlarge instance type,
which has a [GRID
K520](http://www.nvidia.com/object/cloud-gaming-gpu-boards.html) GPU
(GK104 chip) with 1536 cores. Currently our application is also bounded
by GPU memory bandwidth and the GRID K520‘s memory bandwidth is 198
GB/sec, which is an improvement over the Tesla M2050’s at 148 GB/sec. Of
course, using a GPU with faster memory would also help (e.g. TITAN’s
memory bandwidth is 288 GB/sec).\
 \
 We repeated the same comparison between the default npps functions and
our customized ones (with and without PCI space access) on the
g2.2xlarge instances.\
 \

  ------------ ------------ ------------
               KernelMulC   npps\_MulC
  CheckPCI=1   2.01         299.23
  CheckPCI=0   0.97         3.48
  ------------ ------------ ------------

\
 One initial surprise was that we measured worse performance for npps on
the g2 instances than the cg1 when PCI space access was enabled.
However, disabling it improved performance between 45% and 65% compared
to the cg1 instances. Again, our KernelMulC customized functions are
over 70% better, with benchmark times under a second. Thus, switching to
G2 with the right configuration allowed us to run our experiments
faster, or alternatively larger experiments in the same amount of time.\
 \

### Distributed Bayesian Hyperparameter Optimization

\
 Once we had optimized the single-node training and testing operations,
we were ready to tackle the issue of hyperparameter optimization. If you
are not familiar with this concept, here is a simple explanation: Most
machine learning algorithms have parameters to tune, which are called
often called hyperparameters to distinguish them from model parameters
that are produced as a result of the learning algorithm. For example, in
the case of a Neural Network, we can think about optimizing the number
of hidden units, the learning rate, or the regularization weight. In
order to tune these, you need to train and test several different
combinations of hyperparameters and pick the best one for your final
model. A naive approach is to simply perform an exhaustive grid search
over the different possible combinations of reasonable hyperparameters.
However, when faced with a complex model where training each one is time
consuming and there are many hyperparameters to tune, it can be
prohibitively costly to perform such exhaustive grid searches. Luckily,
you can do better than this by thinking of parameter tuning as an
optimization problem in itself.\
 \
 One way to do this is to use a Bayesian Optimization approach where an
algorithm’s performance with respect to a set of hyperparameters [is
modeled as a sample from a Gaussian
Process](http://books.nips.cc/papers/files/nips25/NIPS2012_1338.pdf).
Gaussian Processes are a very effective way to perform regression and
while they can have trouble scaling to large problems, they work well
when there is a limited amount of data, like what we encounter when
performing hyperparameter optimization. We use package
[spearmint](https://github.com/JasperSnoek/spearmint) to perform
Bayesian Optimization and find the best hyperparameters for the Neural
Network training algorithm. We hook up spearmint with our training
algorithm by having it choose the set of hyperparameters and then
training a Neural Network with those parameters using our GPU-optimized
code. This model is then tested and the test metric results used to
update the next hyperparameter choices made by spearmint.\
 \
 We’ve squeezed high performance from our GPU but we only have 1-2 GPU
cards per machine, so we would like to make use of the distributed
computing power of the AWS cloud to perform the hyperparameter tuning
for all configurations, such as different models per international
region. To do this, we use the distributed task queue
[Celery](http://www.celeryproject.org/) to send work to each of the
GPUs. Each worker process listens to the task queue and runs the
training on one GPU. This allows us, for example, to tune, train, and
update several models daily for all international regions.\
 \
 Although the Spearmint + Celery system is working, we are currently
evaluating more complete and flexible solutions using HTCondor or
StarCluster. [HTCondor](http://research.cs.wisc.edu/htcondor/) can be
used to manage the workflow of any Directed Acyclic Graph (DAG). It
handles input/output file transfer and resource management. In order to
use Condor, we need each compute node register into the manager with a
given ClassAd (e.g. SLOT1\_HAS\_GPU=TRUE; STARD\_ATTRS=HAS\_GPU). Then
the user can submit a job with a configuration "Requirements=HAS\_GPU"
so that the job only runs on AWS instances that have an available GPU.
The main advantage of using Condor is that it also manages the
distribution of the data needed for the training of the different
models. Condor also allows us to run the Spearmint Bayesian optimization
on the Manager instead of having to run it on each of the workers.\
 \
 Another alternative is to use
[StarCluster](http://star.mit.edu/cluster/) , which is an open source
cluster computing framework for AWS EC2 developed at MIT. StarCluster
runs on the [Oracle Grid
Engine](http://en.wikipedia.org/wiki/Oracle_Grid_Engine) (formerly Sun
Grid Engine) in a fault-tolerant way and is fully supported by
Spearmint.\
 \
 Finally, we are also looking into integrating Spearmint with
[Jobman](http://deeplearning.net/software/jobman/mlp_jobman.html#mlp-jobman)
in order to better manage the hyperparameter search workflow. \
 Figure below illustrates the generalized setup using Spearmint plus
Celery, Condor, or StarCluster:\
 \
 \

![image](https://lh6.googleusercontent.com/Alk0JTiWGJYYc32QTeJN2vuCPMrfdDu6TZweJ_AozR-evzxTPr78_Mj6lKEx_noosibVvylyuUEEC9JU19jQQo08WEB7i9Xb43DN30nxqKEpBxtSey35yr0S1Q)\
 \

### Conclusions

\
 Implementing bleeding edge solutions such as using GPUs to train
large-scale Neural Networks can be a daunting endeavour. If you need to
do it in your own custom infrastructure, the cost and the complexity
might be overwhelming. Levering the public AWS cloud can have obvious
benefits, provided care is taken in the customization and use of the
instance resources. By sharing our experience we hope to make it much
easier and straightforward for others to develop similar applications.\
 \
 We are always looking for talented researchers and engineers to join
our team. So if you are interested in solving these types of problems,
please take a look at some of our open positions on the [Netflix jobs
page](http://jobs.netflix.com/).\

\
 \

Posted by [Xavier
Amatriain](http://www.blogger.com/profile/14166119485952054870 "author profile")
at [8:51
AM](http://techblog.netflix.com/2014/02/distributed-neural-networks-with-gpus.html "permanent link")
[![image](http://img2.blogblog.com/img/icon18_edit_allbkg.gif)](http://www.blogger.com/post-edit.g?blogID=725338818844296080&postID=1642273166323040485&from=pencil "Edit Post")

[Email
This](http://www.blogger.com/share-post.g?blogID=725338818844296080&postID=1642273166323040485&target=email "Email This")[BlogThis!](http://www.blogger.com/share-post.g?blogID=725338818844296080&postID=1642273166323040485&target=blog "BlogThis!")[Share
to
Twitter](http://www.blogger.com/share-post.g?blogID=725338818844296080&postID=1642273166323040485&target=twitter "Share to Twitter")[Share
to
Facebook](http://www.blogger.com/share-post.g?blogID=725338818844296080&postID=1642273166323040485&target=facebook "Share to Facebook")

Labels: [AWS](http://techblog.netflix.com/search/label/AWS),
[CUDA](http://techblog.netflix.com/search/label/CUDA), [deep
learning](http://techblog.netflix.com/search/label/deep%20learning),
[distributed](http://techblog.netflix.com/search/label/distributed),
[GPU](http://techblog.netflix.com/search/label/GPU), [machine
learning](http://techblog.netflix.com/search/label/machine%20learning),
[neural
networks](http://techblog.netflix.com/search/label/neural%20networks)

[Older
Post](http://techblog.netflix.com/2014/01/improving-netflixs-operational.html "Older Post")
[Home](http://techblog.netflix.com/)

Links
-----

-   [Netflix US & Canada Blog](http://blog.netflix.com/)
-   [Netflix America Latina Blog](http://americalatinablog.netflix.com/)
-   [Netflix Brasil Blog](http://brasilblog.netflix.com/)
-   [Netflix UK & Ireland Blog](http://ukirelandblog.netflix.com/)
-   [Open positions at Netflix](http://www.netflix.com/Jobs)
-   [Netflix Website](http://www.netflix.com/)
-   [Facebook Netflix Page](http://www.facebook.com/netflix)
-   [RSS Feed](http://techblog.netflix.com/rss.xml)

[![image](http://img1.blogblog.com/img/icon18_wrench_allbkg.png)](//www.blogger.com/rearrange?blogID=725338818844296080&widgetType=LinkList&widgetId=LinkList1&action=editWidget&sectionId=sidebar-right-1 "Edit")

About the Netflix Tech Blog
---------------------------

This is a Netflix blog focused on technology and technology issues.
We'll share our perspectives, decisions and challenges regarding the
software we build and use to create the Netflix service.\
\

[![image](http://img1.blogblog.com/img/icon18_wrench_allbkg.png)](//www.blogger.com/rearrange?blogID=725338818844296080&widgetType=Text&widgetId=Text1&action=editWidget&sectionId=sidebar-right-1 "Edit")

Blog Archive
------------

-   [▼](javascript:void(0))
    [2014](http://techblog.netflix.com/search?updated-min=2014-01-01T00:00:00-08:00&updated-max=2015-01-01T00:00:00-08:00&max-results=4)
    (4)
    -   [▼](javascript:void(0))
        [February](http://techblog.netflix.com/2014_02_01_archive.html)
        (1)
        -   [Distributed Neural Networks with GPUs in the AWS
            C...](http://techblog.netflix.com/2014/02/distributed-neural-networks-with-gpus.html)

    -   [►](javascript:void(0))
        [January](http://techblog.netflix.com/2014_01_01_archive.html)
        (3)

-   [►](javascript:void(0))
    [2013](http://techblog.netflix.com/search?updated-min=2013-01-01T00:00:00-08:00&updated-max=2014-01-01T00:00:00-08:00&max-results=50)
    (52)
    -   [►](javascript:void(0))
        [December](http://techblog.netflix.com/2013_12_01_archive.html)
        (9)

    -   [►](javascript:void(0))
        [November](http://techblog.netflix.com/2013_11_01_archive.html)
        (4)

    -   [►](javascript:void(0))
        [October](http://techblog.netflix.com/2013_10_01_archive.html)
        (2)

    -   [►](javascript:void(0))
        [September](http://techblog.netflix.com/2013_09_01_archive.html)
        (2)

    -   [►](javascript:void(0))
        [August](http://techblog.netflix.com/2013_08_01_archive.html)
        (1)

    -   [►](javascript:void(0))
        [July](http://techblog.netflix.com/2013_07_01_archive.html) (2)

    -   [►](javascript:void(0))
        [June](http://techblog.netflix.com/2013_06_01_archive.html) (7)

    -   [►](javascript:void(0))
        [May](http://techblog.netflix.com/2013_05_01_archive.html) (5)

    -   [►](javascript:void(0))
        [April](http://techblog.netflix.com/2013_04_01_archive.html) (1)

    -   [►](javascript:void(0))
        [March](http://techblog.netflix.com/2013_03_01_archive.html) (8)

    -   [►](javascript:void(0))
        [February](http://techblog.netflix.com/2013_02_01_archive.html)
        (4)

    -   [►](javascript:void(0))
        [January](http://techblog.netflix.com/2013_01_01_archive.html)
        (7)

-   [►](javascript:void(0))
    [2012](http://techblog.netflix.com/search?updated-min=2012-01-01T00:00:00-08:00&updated-max=2013-01-01T00:00:00-08:00&max-results=37)
    (37)
    -   [►](javascript:void(0))
        [December](http://techblog.netflix.com/2012_12_01_archive.html)
        (6)

    -   [►](javascript:void(0))
        [November](http://techblog.netflix.com/2012_11_01_archive.html)
        (3)

    -   [►](javascript:void(0))
        [October](http://techblog.netflix.com/2012_10_01_archive.html)
        (2)

    -   [►](javascript:void(0))
        [September](http://techblog.netflix.com/2012_09_01_archive.html)
        (2)

    -   [►](javascript:void(0))
        [July](http://techblog.netflix.com/2012_07_01_archive.html) (6)

    -   [►](javascript:void(0))
        [June](http://techblog.netflix.com/2012_06_01_archive.html) (5)

    -   [►](javascript:void(0))
        [May](http://techblog.netflix.com/2012_05_01_archive.html) (1)

    -   [►](javascript:void(0))
        [April](http://techblog.netflix.com/2012_04_01_archive.html) (2)

    -   [►](javascript:void(0))
        [March](http://techblog.netflix.com/2012_03_01_archive.html) (2)

    -   [►](javascript:void(0))
        [February](http://techblog.netflix.com/2012_02_01_archive.html)
        (4)

    -   [►](javascript:void(0))
        [January](http://techblog.netflix.com/2012_01_01_archive.html)
        (4)

-   [►](javascript:void(0))
    [2011](http://techblog.netflix.com/search?updated-min=2011-01-01T00:00:00-08:00&updated-max=2012-01-01T00:00:00-08:00&max-results=17)
    (17)
    -   [►](javascript:void(0))
        [December](http://techblog.netflix.com/2011_12_01_archive.html)
        (1)

    -   [►](javascript:void(0))
        [November](http://techblog.netflix.com/2011_11_01_archive.html)
        (2)

    -   [►](javascript:void(0))
        [October](http://techblog.netflix.com/2011_10_01_archive.html)
        (1)

    -   [►](javascript:void(0))
        [September](http://techblog.netflix.com/2011_09_01_archive.html)
        (1)

    -   [►](javascript:void(0))
        [August](http://techblog.netflix.com/2011_08_01_archive.html)
        (1)

    -   [►](javascript:void(0))
        [July](http://techblog.netflix.com/2011_07_01_archive.html) (1)

    -   [►](javascript:void(0))
        [June](http://techblog.netflix.com/2011_06_01_archive.html) (1)

    -   [►](javascript:void(0))
        [May](http://techblog.netflix.com/2011_05_01_archive.html) (1)

    -   [►](javascript:void(0))
        [April](http://techblog.netflix.com/2011_04_01_archive.html) (2)

    -   [►](javascript:void(0))
        [March](http://techblog.netflix.com/2011_03_01_archive.html) (2)

    -   [►](javascript:void(0))
        [February](http://techblog.netflix.com/2011_02_01_archive.html)
        (1)

    -   [►](javascript:void(0))
        [January](http://techblog.netflix.com/2011_01_01_archive.html)
        (3)

-   [►](javascript:void(0))
    [2010](http://techblog.netflix.com/search?updated-min=2010-01-01T00:00:00-08:00&updated-max=2011-01-01T00:00:00-08:00&max-results=8)
    (8)
    -   [►](javascript:void(0))
        [December](http://techblog.netflix.com/2010_12_01_archive.html)
        (8)

[![image](http://img1.blogblog.com/img/icon18_wrench_allbkg.png)](//www.blogger.com/rearrange?blogID=725338818844296080&widgetType=BlogArchive&widgetId=BlogArchive1&action=editWidget&sectionId=sidebar-right-1 "Edit")

Labels
------

-   [accelerated
    compositing](http://techblog.netflix.com/search/label/accelerated%20compositing)
    (2)
-   [Aegisthus](http://techblog.netflix.com/search/label/Aegisthus) (1)
-   [aminator](http://techblog.netflix.com/search/label/aminator) (1)
-   [analytics](http://techblog.netflix.com/search/label/analytics) (3)
-   [Android](http://techblog.netflix.com/search/label/Android) (1)
-   [api](http://techblog.netflix.com/search/label/api) (14)
-   [appender](http://techblog.netflix.com/search/label/appender) (1)
-   [Archaius](http://techblog.netflix.com/search/label/Archaius) (2)
-   [Asgard](http://techblog.netflix.com/search/label/Asgard) (1)
-   [Astyanax](http://techblog.netflix.com/search/label/Astyanax) (3)
-   [autoscaling](http://techblog.netflix.com/search/label/autoscaling)
    (3)
-   [availability](http://techblog.netflix.com/search/label/availability)
    (3)
-   [AWS](http://techblog.netflix.com/search/label/AWS) (23)
-   [benchmark](http://techblog.netflix.com/search/label/benchmark) (2)
-   [big data](http://techblog.netflix.com/search/label/big%20data) (5)
-   [Blitz4j](http://techblog.netflix.com/search/label/Blitz4j) (1)
-   [build](http://techblog.netflix.com/search/label/build) (3)
-   [Cable](http://techblog.netflix.com/search/label/Cable) (1)
-   [caching](http://techblog.netflix.com/search/label/caching) (1)
-   [Cassandra](http://techblog.netflix.com/search/label/Cassandra) (10)
-   [chaos
    monkey](http://techblog.netflix.com/search/label/chaos%20monkey) (4)
-   [ci](http://techblog.netflix.com/search/label/ci) (1)
-   [Clojure](http://techblog.netflix.com/search/label/Clojure) (1)
-   [cloud](http://techblog.netflix.com/search/label/cloud) (20)
-   [cloud
    architecture](http://techblog.netflix.com/search/label/cloud%20architecture)
    (12)
-   [cloud
    prize](http://techblog.netflix.com/search/label/cloud%20prize) (3)
-   [collection](http://techblog.netflix.com/search/label/collection)
    (1)
-   [concurrency](http://techblog.netflix.com/search/label/concurrency)
    (1)
-   [configuration](http://techblog.netflix.com/search/label/configuration)
    (2)
-   [configuration
    management](http://techblog.netflix.com/search/label/configuration%20management)
    (1)
-   [conformity
    monkey](http://techblog.netflix.com/search/label/conformity%20monkey)
    (1)
-   [continuous
    delivery](http://techblog.netflix.com/search/label/continuous%20delivery)
    (2)
-   [coordination](http://techblog.netflix.com/search/label/coordination)
    (2)
-   [cost
    management](http://techblog.netflix.com/search/label/cost%20management)
    (1)
-   [Cryptography](http://techblog.netflix.com/search/label/Cryptography)
    (1)
-   [CSS](http://techblog.netflix.com/search/label/CSS) (2)
-   [CUDA](http://techblog.netflix.com/search/label/CUDA) (1)
-   [data
    migration](http://techblog.netflix.com/search/label/data%20migration)
    (1)
-   [data
    pipeline](http://techblog.netflix.com/search/label/data%20pipeline)
    (2)
-   [data
    science](http://techblog.netflix.com/search/label/data%20science)
    (4)
-   [data
    visualization](http://techblog.netflix.com/search/label/data%20visualization)
    (1)
-   [database](http://techblog.netflix.com/search/label/database) (2)
-   [DataStax](http://techblog.netflix.com/search/label/DataStax) (2)
-   [deadlock](http://techblog.netflix.com/search/label/deadlock) (1)
-   [deep
    learning](http://techblog.netflix.com/search/label/deep%20learning)
    (1)
-   [Denominator](http://techblog.netflix.com/search/label/Denominator)
    (2)
-   [dependency
    injection](http://techblog.netflix.com/search/label/dependency%20injection)
    (1)
-   [device](http://techblog.netflix.com/search/label/device) (2)
-   [device
    proliferation](http://techblog.netflix.com/search/label/device%20proliferation)
    (1)
-   [distributed](http://techblog.netflix.com/search/label/distributed)
    (9)
-   [DNS](http://techblog.netflix.com/search/label/DNS) (1)
-   [DSL](http://techblog.netflix.com/search/label/DSL) (1)
-   [Dyn](http://techblog.netflix.com/search/label/Dyn) (1)
-   [DynECT](http://techblog.netflix.com/search/label/DynECT) (1)
-   [Elastic Load
    Balancer](http://techblog.netflix.com/search/label/Elastic%20Load%20Balancer)
    (1)
-   [ELB](http://techblog.netflix.com/search/label/ELB) (1)
-   [EMR](http://techblog.netflix.com/search/label/EMR) (1)
-   [encoding](http://techblog.netflix.com/search/label/encoding) (1)
-   [eucalyptus](http://techblog.netflix.com/search/label/eucalyptus)
    (1)
-   [eureka](http://techblog.netflix.com/search/label/eureka) (2)
-   [evcache](http://techblog.netflix.com/search/label/evcache) (1)
-   [failover](http://techblog.netflix.com/search/label/failover) (2)
-   [fault-tolerance](http://techblog.netflix.com/search/label/fault-tolerance)
    (11)
-   [Flow](http://techblog.netflix.com/search/label/Flow) (1)
-   [FRP](http://techblog.netflix.com/search/label/FRP) (1)
-   [functional
    reactive](http://techblog.netflix.com/search/label/functional%20reactive)
    (1)
-   [garbage](http://techblog.netflix.com/search/label/garbage) (1)
-   [garbage
    collection](http://techblog.netflix.com/search/label/garbage%20collection)
    (1)
-   [gc](http://techblog.netflix.com/search/label/gc) (1)
-   [Genie](http://techblog.netflix.com/search/label/Genie) (3)
-   [Governator](http://techblog.netflix.com/search/label/Governator)
    (1)
-   [GPU](http://techblog.netflix.com/search/label/GPU) (2)
-   [Groovy](http://techblog.netflix.com/search/label/Groovy) (1)
-   [Hadoop](http://techblog.netflix.com/search/label/Hadoop) (10)
-   [HBase](http://techblog.netflix.com/search/label/HBase) (1)
-   [high
    volume](http://techblog.netflix.com/search/label/high%20volume) (3)
-   [high volume distributed
    systems](http://techblog.netflix.com/search/label/high%20volume%20distributed%20systems)
    (7)
-   [HTML5](http://techblog.netflix.com/search/label/HTML5) (6)
-   [Hystrix](http://techblog.netflix.com/search/label/Hystrix) (5)
-   [IBM](http://techblog.netflix.com/search/label/IBM) (1)
-   [ice](http://techblog.netflix.com/search/label/ice) (1)
-   [initialization](http://techblog.netflix.com/search/label/initialization)
    (1)
-   [innovation](http://techblog.netflix.com/search/label/innovation)
    (2)
-   [insights](http://techblog.netflix.com/search/label/insights) (1)
-   [inter process
    communication](http://techblog.netflix.com/search/label/inter%20process%20communication)
    (1)
-   [Ipv6](http://techblog.netflix.com/search/label/Ipv6) (2)
-   [ISP](http://techblog.netflix.com/search/label/ISP) (1)
-   [java](http://techblog.netflix.com/search/label/java) (2)
-   [JavaScript](http://techblog.netflix.com/search/label/JavaScript)
    (6)
-   [jclouds](http://techblog.netflix.com/search/label/jclouds) (1)
-   [jenkins](http://techblog.netflix.com/search/label/jenkins) (1)
-   [Karyon](http://techblog.netflix.com/search/label/Karyon) (2)
-   [lifecycle](http://techblog.netflix.com/search/label/lifecycle) (1)
-   [lipstick](http://techblog.netflix.com/search/label/lipstick) (2)
-   [load
    balancing](http://techblog.netflix.com/search/label/load%20balancing)
    (3)
-   [locking](http://techblog.netflix.com/search/label/locking) (1)
-   [locks](http://techblog.netflix.com/search/label/locks) (1)
-   [log4j](http://techblog.netflix.com/search/label/log4j) (1)
-   [logging](http://techblog.netflix.com/search/label/logging) (2)
-   [machine
    learning](http://techblog.netflix.com/search/label/machine%20learning)
    (1)
-   [Map-Reduce](http://techblog.netflix.com/search/label/Map-Reduce)
    (1)
-   [meetup](http://techblog.netflix.com/search/label/meetup) (2)
-   [memcache](http://techblog.netflix.com/search/label/memcache) (1)
-   [Mobile](http://techblog.netflix.com/search/label/Mobile) (1)
-   [monitoring](http://techblog.netflix.com/search/label/monitoring)
    (1)
-   [Netflix](http://techblog.netflix.com/search/label/Netflix) (13)
-   [Netflix
    API](http://techblog.netflix.com/search/label/Netflix%20API) (6)
-   [netflix
    graph](http://techblog.netflix.com/search/label/netflix%20graph) (1)
-   [Netflix
    OSS](http://techblog.netflix.com/search/label/Netflix%20OSS) (7)
-   [NetflixOSS](http://techblog.netflix.com/search/label/NetflixOSS)
    (9)
-   [neural
    networks](http://techblog.netflix.com/search/label/neural%20networks)
    (1)
-   [NoSQL](http://techblog.netflix.com/search/label/NoSQL) (4)
-   [Open
    source](http://techblog.netflix.com/search/label/Open%20source) (6)
-   [operational
    insight](http://techblog.netflix.com/search/label/operational%20insight)
    (1)
-   [operational
    visibility](http://techblog.netflix.com/search/label/operational%20visibility)
    (1)
-   [OSS](http://techblog.netflix.com/search/label/OSS) (1)
-   [outage](http://techblog.netflix.com/search/label/outage) (1)
-   [Paypal](http://techblog.netflix.com/search/label/Paypal) (1)
-   [performance](http://techblog.netflix.com/search/label/performance)
    (10)
-   [personalization](http://techblog.netflix.com/search/label/personalization)
    (2)
-   [phone](http://techblog.netflix.com/search/label/phone) (1)
-   [Pig](http://techblog.netflix.com/search/label/Pig) (2)
-   [prediction](http://techblog.netflix.com/search/label/prediction)
    (2)
-   [prize](http://techblog.netflix.com/search/label/prize) (1)
-   [pytheas](http://techblog.netflix.com/search/label/pytheas) (1)
-   [python](http://techblog.netflix.com/search/label/python) (1)
-   [Quality](http://techblog.netflix.com/search/label/Quality) (1)
-   [rca](http://techblog.netflix.com/search/label/rca) (2)
-   [Reactive
    Programming](http://techblog.netflix.com/search/label/Reactive%20Programming)
    (1)
-   [real-time
    insights](http://techblog.netflix.com/search/label/real-time%20insights)
    (1)
-   [Recipe](http://techblog.netflix.com/search/label/Recipe) (1)
-   [recommendations](http://techblog.netflix.com/search/label/recommendations)
    (3)
-   [reinvent](http://techblog.netflix.com/search/label/reinvent) (2)
-   [reliability](http://techblog.netflix.com/search/label/reliability)
    (6)
-   [remote procedure
    calls](http://techblog.netflix.com/search/label/remote%20procedure%20calls)
    (1)
-   [research](http://techblog.netflix.com/search/label/research) (1)
-   [resiliency](http://techblog.netflix.com/search/label/resiliency)
    (6)
-   [REST](http://techblog.netflix.com/search/label/REST) (2)
-   [Ribbon](http://techblog.netflix.com/search/label/Ribbon) (2)
-   [Riot Games](http://techblog.netflix.com/search/label/Riot%20Games)
    (1)
-   [root-cause
    analysis](http://techblog.netflix.com/search/label/root-cause%20analysis)
    (2)
-   [Route53](http://techblog.netflix.com/search/label/Route53) (1)
-   [Rx](http://techblog.netflix.com/search/label/Rx) (1)
-   [scalability](http://techblog.netflix.com/search/label/scalability)
    (7)
-   [security](http://techblog.netflix.com/search/label/security) (3)
-   [Servo](http://techblog.netflix.com/search/label/Servo) (1)
-   [shared
    libraries](http://techblog.netflix.com/search/label/shared%20libraries)
    (1)
-   [simian
    army](http://techblog.netflix.com/search/label/simian%20army) (4)
-   [SimpleDB](http://techblog.netflix.com/search/label/SimpleDB) (3)
-   [ssd](http://techblog.netflix.com/search/label/ssd) (1)
-   [STAASH](http://techblog.netflix.com/search/label/STAASH) (1)
-   [suro](http://techblog.netflix.com/search/label/suro) (1)
-   [SWF](http://techblog.netflix.com/search/label/SWF) (1)
-   [synchronization](http://techblog.netflix.com/search/label/synchronization)
    (1)
-   [tablet](http://techblog.netflix.com/search/label/tablet) (1)
-   [TV](http://techblog.netflix.com/search/label/TV) (3)
-   [UI](http://techblog.netflix.com/search/label/UI) (4)
-   [UltraDNS](http://techblog.netflix.com/search/label/UltraDNS) (1)
-   [unit test](http://techblog.netflix.com/search/label/unit%20test)
    (1)
-   [uptime](http://techblog.netflix.com/search/label/uptime) (2)
-   [user
    interface](http://techblog.netflix.com/search/label/user%20interface)
    (2)
-   [visualization](http://techblog.netflix.com/search/label/visualization)
    (1)
-   [WebKit](http://techblog.netflix.com/search/label/WebKit) (3)
-   [Wii U](http://techblog.netflix.com/search/label/Wii%20U) (1)
-   [winner](http://techblog.netflix.com/search/label/winner) (1)
-   [winners](http://techblog.netflix.com/search/label/winners) (1)
-   [workflow](http://techblog.netflix.com/search/label/workflow) (1)
-   [ZooKeeper](http://techblog.netflix.com/search/label/ZooKeeper) (1)
-   [zuul](http://techblog.netflix.com/search/label/zuul) (1)
-   [“cloud
    architecture”](http://techblog.netflix.com/search/label/%E2%80%9Ccloud%20architecture%E2%80%9D)
    (3)

[![image](http://img1.blogblog.com/img/icon18_wrench_allbkg.png)](//www.blogger.com/rearrange?blogID=725338818844296080&widgetType=Label&widgetId=Label1&action=editWidget&sectionId=sidebar-right-1 "Edit")

Awesome Inc. template. Powered by [Blogger](http://www.blogger.com).

[![image](http://img1.blogblog.com/img/icon18_wrench_allbkg.png)](//www.blogger.com/rearrange?blogID=725338818844296080&widgetType=Attribution&widgetId=Attribution1&action=editWidget&sectionId=footer-3 "Edit")

This markdown document has been converted from the html document located at:
http://techblog.netflix.com/2014/02/distributed-neural-networks-with-gpus.html
