[*Ars*Technica](http://arstechnica.com)
=======================================

[Register](/civis/ucp.php?mode=register) [Log
in](/civis/ucp.php?mode=login&return_to=http%3A%2F%2Farstechnica.com%2Finformation-technology%2F2014%2F05%2Fask-ars-why-are-some-programming-languages-faster-than-others%2F)

-   [Home](/)
-   [Main Menu](#)

    -   [Information Technology Technology Lab](/information-technology)
    -   [Product News & Reviews Gear & Gadgets](/gadgets)
    -   [Business of Technology Ministry of Innovation](/business)
    -   [Security & Hacktivism Risk Assessment](/security)
    -   [Civilization & Discontents Law & Disorder](/tech-policy)
    -   [The Apple Ecosystem Infinite Loop](/apple)
    -   [Gaming & Entertainment Opposable Thumbs](/gaming)
    -   [Science & Exploration The Scientific Method](/science)

    [Feature Series Interop 2014 Develop your IT
    strategy.](/feature-series/interop-2014/)

    Layout:

    -   [Grid
        View](http://arstechnica.com/information-technology/2014/05/ask-ars-why-are-some-programming-languages-faster-than-others/?view=grid)
    -   [Article
        View](http://arstechnica.com/information-technology/2014/05/ask-ars-why-are-some-programming-languages-faster-than-others/?view=archive)

    Site Theme
    ----------

    -   [Dark on
        light](http://arstechnica.com/information-technology/2014/05/ask-ars-why-are-some-programming-languages-faster-than-others/?theme=light)
    -   [Light on
        dark](http://arstechnica.com/information-technology/2014/05/ask-ars-why-are-some-programming-languages-faster-than-others/?theme=dark)

    Explore Ars
    -----------

    -   [Reviews](/reviews/)
    -   [Video](/video/)
    -   [Staff Blogs](/staff/)
    -   [Feature Archive](/features/)
    -   [Staff Directory](/staff-directory/)
    -   [Contact Us](http://arstechnica.com/contact-us/)

    Featured Disciplines
    --------------------

    -   [Photography](/discipline/photography/)
    -   [Productivity](/discipline/productivity/)
    -   [Cloud](/discipline/cloud-2/)
    -   [Gadgets](/discipline/gadgets-3/)
    -   [Tablets](/discipline/tablets-2/)

-   [My Stories: 0](#)

    New Since Last Visit
    --------------------

    [See more news stories](#)

    We Recommend
    ------------

    My Discussions
    --------------

    Log in to track your discussions.

-   [Forums](/civis/)
-   [Subscribe](/subscriptions/)
-   [Jobs](/jobs/)
-   [](/search/)

[Technology Lab / Information Technology](http://arstechnica.com/information-technology/)
=========================================================================================

Ask Ars: Why are some programming languages faster than others?
===============================================================

Scientists use Fortran because it's fast, but what makes it so?
---------------------------------------------------------------

by [Peter Bright](http://arstechnica.com/author/peter-bright/) - May 11,
2014 7:00 pm UTC

-   [Development](/discipline/development-2)

[126](http://arstechnica.com/information-technology/2014/05/ask-ars-why-are-some-programming-languages-faster-than-others/?comments=1 "84 posters participating, including story author.")

![image](http://cdn.arstechnica.net/wp-content/uploads/2014/05/2482009942_6caea217e0_z.jpg)

[Bill Bradford](https://www.flickr.com/photos/mrbill/2482009942)

In our look at [scientific
computing](http://arstechnica.com/science/2014/05/scientific-computings-future-can-any-coding-language-top-a-1950s-behemoth/)
and the continued longevity of Fortran in science and engineering
circles, one of the recurring themes in the discussion that followed was
performance.

One of the big reasons that Fortran remains important is because it's
fast: number crunching routines written in Fortran tend to be quicker
than equivalent routines written in most other languages. The languages
that are competing with Fortran in this space—C and C++—are used because
they're competitive with this performance.

This raises the question: why? What is it about C++ and Fortran that
make them fast, and why do they outperform other popular languages, such
as Java or Python?

Interpreting versus compiling
-----------------------------

There are many ways to categorize and define programming languages,
according to the style of programming they encourage and features they
offer. When looking at performance, the biggest single distinction is
between interpreted languages and compiled ones.

The divide is not hard; rather, there's a spectrum. At one end, we have
traditional compiled languages, a group that includes Fortran, C, and
C++. In these languages, there is a discrete *compilation* stage that
translates the source code of a program into an executable form that the
processor can use.

This compilation process has several steps. The source code is analyzed
and parsed. Basic coding mistakes such as typos and spelling errors can
be detected at this point. The parsed code is used to generate an
in-memory representation, which too can be used to detect mistakes—this
time, *semantic* mistakes, such as calling functions that don't exist,
or trying to perform arithmetic operations on strings of text.

This in-memory representation is then used to drive a code generator,
the part that produces executable code. Code optimization, to improve
the performance of the generated code, is performed at various times
within this process: high-level optimizations can be performed on the
code representation, and lower-level optimizations are used on the
output of the code generator.

Actually *executing* the code happens later. The entire compilation
process is simply used to create something that *can* be executed.

At the opposite end, we have interpreters. The interpreters will include
a parsing stage similar to that of the compiler, but this is then used
to drive *direct* execution, with the program being run immediately.

The simplest interpreter has within it executable code corresponding to
the various features the language supports—so it will have functions for
adding numbers, joining strings, whatever else a given language has. As
it parses the code, it will look up the corresponding function and
execute it. Variables created in the program will be kept in some kind
of lookup table that maps their names to their data.

The most extreme example of the interpreter style is something like a
batch file or shell script. In these languages, the executable code is
often not even built into the interpreter itself, but rather separate,
standalone programs.

So why does this make a difference to performance? In general, each
layer of indirection reduces performance. For example, the fastest way
to add two numbers is to have both of those numbers in registers in the
processor, and to use the processor's add instruction. That's what
compiled programs can do; they can put variables into registers and take
advantage of processor instructions. But in interpreted programs, that
same addition might require two lookups in a table of variables to fetch
the values to add, then calling a function to perform the addition. That
function may very well use the same processor instruction as the
compiled program uses to perform the actual addition, but all the extra
work before the instruction can actually be *used* makes things slower.

Blurring the lines
------------------

Between the extremes are a range of options. For example, many high
performance interpreters will act a lot more like compilers: they'll
perform the same steps as a compiler, including the generation of
directly executable code, but they'll then execute that code immediately
(instead of storing it on disk for execution later). While these
interpreters might keep the executable code around for the duration of
the program so they don't have to generate code for a particular
function more than once, it'll generally be thrown away when the program
finishes. If you run the program again, the interpreter will have to
generate the code all over again.

This process of compiling when the program is run is called just-in-time
(JIT) compilation, and the JavaScript engines of Internet Explorer,
Firefox, and Chrome all use this technique to maximize their scripting
performance.

JIT compilation is typically faster than traditional interpreting.
However, it generally can't compete with conventional ahead-of-time
compilation. AOT compilation can be slow, with compilers spending
considerable time to optimize the code to the best of their ability.
They can afford to do this because nobody is actually *waiting* for the
compilation to take place. JIT compilation, however, happens at runtime,
with a user waiting at the keyboard for the program to actually run.
This limits the time that can be spent optimizing. Techniques such as
performing additional optimization on a background thread and making use
of modern multicore processors can go some way toward closing this gap.

In principle, JIT compilation can offer performance benefits over
conventional compilation. A conventionally compiled program generally
has to be quite conservative in some ways. Microsoft can't easily
compile Windows to, for example, take advantage of the latest AVX
instructions found in newer Intel and AMD processors, because Windows
has to run on processors that don't support AVX. A JIT compiler,
however, knows exactly the hardware it will be used on, and so can take
maximal advantage of it.

Historically, JIT compilers haven't been very good at using the kind of
complex instructions that modern processors offer. Indeed, making use of
instructions like SSE and AVX is a challenge even for AOT compilers, in
spite of their lack of time constraints. However, this is starting to
change, with for example Oracle's HotSpot compiler for Java including
some early support for these instructions.

Another common technique is the use of bytecode. Bytecode-based
platforms, including Java and .NET, have a traditional compilation
process, but instead of generating executable machine code, the compiler
generates bytecode, a kind of machine code designed not for real
hardware, but for an idealized virtual machine. This bytecode can then
be interpreted or JIT compiled when the program is actually run.

Generally, the performance of these bytecode systems is somewhere
between interpreted ones and compiled ones. The bytecode is easier to
JIT compile and optimize at runtime, giving an advantage over
interpreters, but it still doesn't enable the same optimization effort
as the compiler.

These various intermediate options in turn give rise to a range of
intermediate performance options between the compiled and interpreted
extremes.

Technically, the use of a compiler or an interpreter is not a property
of the language *itself*. There are various projects that, for example,
create interpreters for C, a language that's traditionally compiled.
JavaScript has gone from simple interpreters to complex JIT compilers to
get better performance.

However, mainstream languages don't tend to switch too often; C++ is
essentially *always* going to be compiled ahead of time. So too is
Fortran. C\# and Java are almost always going to be compiled to bytecode
and then JIT compiled at runtime. Python and Ruby are almost always
going to be interpreted. This tends to create a performance hierarchy:
C++ and Fortran are faster than Java and C\#, which in turn are faster
than Python and Ruby.

The languages themselves
------------------------

Within each category there's still plenty of performance variation. Much
of this is simply due to priorities. Consider Python and JavaScript, for
example; two popular scripting languages that are both traditionally
interpreted. In practice, JavaScript tends to be a lot faster than
Python, not because of any feature of the languages themselves—they're
broadly comparable in terms of expressiveness and capabilities—but
because companies like Microsoft, Google, and Mozilla have invested
heavily in making JavaScript faster.

Within similar kinds of language, this difference in investment (or
development priorities) is probably the biggest single determinant in
language performance. Making high-speed interpreters and compilers is a
lot of work, and not every language actually needs all that work.

However, language differences can account for some of the differences.
The longevity of Fortran happens to be a good example of this. For a
long time, equivalent programs in Fortran and C (or C++) would be faster
in Fortran, because Fortran enabled better optimizations. This was true
even when the C compiler used the same code generator and optimizer as
the Fortran compiler. This difference wasn't because of a feature
Fortran had. In fact, it was the reverse: it was because of a feature
that Fortran *didn't* have.

Number crunching apps typically do their work on large arrays of
numbers: numbers packed in memory representing points in 3D space or
forces or similar. The computations generally iterate over these arrays
doing the same operation on each element of the array in turn. A
function to add two arrays to each other element-by-element, for
example, will take three inputs: the two arrays to add, and a third
array to put the answers.

Element-wise addition like this can be done in any order. This allows
all kinds of optimizations. For instance, it can also vector
instructions like SSE and AVX. Instead of adding each element one by
one, four elements from one array can be added to four elements from the
other array, simultaneously, using SSE or AVX, for an easy four-fold
improvement in performance. This kind of function is also amenable to
being made multithreaded: if you have four cores, then one core can add
the first quarter of the array, the next the next quarter, and so on.
These array-based functions afford some of the most powerful compiler
optimizations, able to make the code run *many times* faster.

C doesn't really let functions have arrays as inputs (or, for that
matter, outputs). Instead, it has a thing called pointers. Pointers more
or less represent memory addresses, and C has built-in features for
reading and writing values stored at these memory addresses. In many
regards, C uses pointers and arrays interchangeably; for an array, the
pointer just represents the address of the first element of the array.
The other elements of the array just have the next, sequential, memory
addresses. C has built-in features for performing arithmetic on the
memory addresses to access the arrays.

Pointers are very flexible, and C developers use them to construct
complex data structures, as well as tightly packed sequential arrays.
But this flexibility can make them difficult for compilers to optimize.
Consider again that function to add two arrays. In C, it would take not
three arrays as its input, but three pointers; two for the inputs, one
for the output.

Here's the problem. Those pointers could represent any memory address.
More to the point, they could *overlap*. The memory address of the
output array could be the same as one of the input arrays. They might
even overlap *partially*. The output array could lie over half of one of
the input arrays.

This is a problem for the optimizer, because it means that the
assumptions that the array-based optimizer could make no longer hold
true. Specifically, the order in which elements are added now matters;
if the output overlaps with one of the input arrays then the result of
the calculation will be different depending on whether each element of
the input array is read *before* they get overwritten as output, or
*after*.

This means that those fancy optimizations—multithreading and vector
instructions—aren't available. The compiler doesn't know if it's safe to
do, so it has to conservatively do what the source code instructs, *in
the order it instructs*, and no more. The compiler no longer has the
freedom to rearrange the program to make it go faster.

This problem is called aliasing, and it's something that traditional
Fortran doesn't suffer from, because traditional Fortran doesn't have
pointers; it just has non-overlapping arrays. For many years, it meant
that Fortran compilers (and developers) could apply powerful
optimizations that weren't available in C. This cemented Fortran's
position as the fastest language for number crunching.

Obviously for this kind of function, this flexibility that pointers
offer isn't actually useful. If the arrays overlap then there's no right
way to process the data, so it's bad luck that the optimizations can't
be used. In the 1999 update to the C specification, known as C99, C was
given an answer to this problem. In C99, pointers can be specified as
not overlapping. When this is done, the compiler can make all the
optimizations it wants, and with this feature, C99 (and C++, as most
compiler vendors gave it a comparable capability) became as optimizable
as Fortran.

This aliasing problem shows how language features can be relevant to
optimization, especially when it comes to making big, transformative
optimizations that, for example, automatically convert single-threaded
code to multithreaded. However, it also shows how such differences
aren't necessarily permanent. Developers want to be able to use C and
C++ for their number crunching code, and if it takes small changes to
the language to make it as fast as Fortran, those changes will be made.

[Expand full
story](http://arstechnica.com/information-technology/2014/05/ask-ars-why-are-some-programming-languages-faster-than-others/)

[Reader comments
126](http://arstechnica.com/information-technology/2014/05/ask-ars-why-are-some-programming-languages-faster-than-others/?comments=1)

You must [login or create an account](/civis/ucp.php?mode=login) to
comment.

[![image](http://cdn.arstechnica.net/wp-content/uploads/authors/Peter-Bright-sq.jpg)](/author/peter-bright)

[Peter Bright](/author/peter-bright) / Peter is Technology Editor at
Ars. He covers Microsoft, programming and software development, Web
technology and browsers, and security. He is based in Houston, TX.

[@drpizza](https://twitter.com/drpizza)

[← Older
Story](http://arstechnica.com/tech-policy/2014/05/facebook-zynga-beat-wiretap-lawsuits/)

[Newer Story
→](http://arstechnica.com/business/2014/05/gallery-inside-hashfast-a-struggling-bitcoin-chipmaker-startup/)

You May Also Like
-----------------

-   [](http://arstechnica.com/sponsored/how-to-prevent-sports-injuries-use-predictive-analytics/)

    ![image](http://cdn.arstechnica.net/wp-content/themes/arstechnica/assets/images/frs/ibm-sponsor-ribbn.png)
    ![image](http://cdn.arstechnica.net/wp-content/uploads/2014/05/ibm-waratahs-300x150.jpg)

    Sponsored: How to prevent sports injuries? Use predictive analytics

Latest Feature Story
--------------------

[![image](http://cdn.arstechnica.net/wp-content/uploads/2014/05/Mario-Kart-8-6-300x100.png)](http://arstechnica.com/gaming/2014/05/mario-kart-8-review-one-step-forward-one-step-back/)

Feature Story (2 pages)
-----------------------

*Mario Kart 8* review: One step forward, one step back
======================================================

Great visuals and course design marred by some baffling changes for the
worse.

Watch Ars Video
---------------

![image](http://brightcove.vo.llnwd.net/d21/unsecured/media/636468927001/201405/636/636468927001_3527039783001_747.jpg?pubId=636468927001)

[](http://arstechnica.com/science/2014/04/gallery-nasas-shuttle-carrying-747-makes-final-journey-to-museum/)

NASA's 747 shuttle carrier on the move
======================================

Disassembled and spread out across a thousand foot convoy, one of NASA's
Shuttle Carrier Aircraft makes its final journey.

Stay in the know with
---------------------

-   [](https://www.facebook.com/arstechnica)

    Follow Ars on Facebook
    ----------------------

-   [](https://twitter.com/arstechnica)

    Follow Ars on Twitter
    ---------------------

-   [](https://plus.google.com/+ArsTechnica/posts)

    Follow Ars on Google Plus
    -------------------------

-   [](http://arstechnica.us1.list-manage.com/subscribe?u=af7f013bad7e785d15aab736f&id=0adf3ee3d9)

    Sign Up for the Ars Newsletter
    ------------------------------

    Sign up for the Ars Technica Dispatch, which delivers links to the
    most popular articles, journals, and multimedia features via e-mail
    to your inbox every week.

    Your email address:

    I understand and agree that registration on or use of this site
    constitutes agreement to its [User Agreement](#) and [Privacy
    Policy](#).

-   [](/rss-feeds/)

    Follow Ars with RSS
    -------------------

    -   [\> Main site RSS
        feed](http://feeds.arstechnica.com/arstechnica/index/) (all
        content)
    -   [\> View a list](http://arstechnica.com/rss-feeds/) of all our
        RSS feeds

-   [](/podcast/)

    The Ars Podcast
    ---------------

    It's the Ars Technicast! Listen to our staff discuss the issues of
    the day.

    -   [\> View podcast archive](http://arstechnica.com/podcast/)

Latest News
-----------

1.  [](http://arstechnica.com/information-technology/2014/05/mad-at-the-fcc-use-this-code-to-create-your-own-slow-lane-on-the-web/)

    Loading...
    ----------

    Mad at the FCC? Use this code to create your own “slow lane” on the Web
    =======================================================================

2.  [](http://arstechnica.com/security/2014/05/former-subway-sandwich-franchisee-cops-to-40000-gift-card-hack-scheme/)

    SKIMMING THE TILL
    -----------------

    Former Subway sandwich franchisee cops to $40,000 gift-card hack scheme
    =======================================================================

3.  [](http://arstechnica.com/information-technology/2014/05/microsoft-sticking-to-its-guns-so-far-leaving-xp-unpatched-and-exploited/)

    This time, we mean it!
    ----------------------

    Microsoft sticking to its guns so far, leaving XP unpatched and exploited
    =========================================================================

4.  [![image](http://cdn.arstechnica.net/wp-content/uploads/2014/05/padlock-150x150.jpg)](http://arstechnica.com/information-technology/2014/05/driven-by-necessity-mozilla-to-enable-html5-drm-in-firefox/)

    Driven by necessity, Mozilla to enable HTML5 DRM in Firefox
    ===========================================================

5.  [](http://arstechnica.com/tech-policy/2014/05/sony-pictures-scores-movie-rights-to-snowden-saga/)

    Surveillance and the silver screen
    ----------------------------------

    Sony Pictures scores movie rights to Snowden saga
    =================================================

6.  [![image](http://cdn.arstechnica.net/wp-content/uploads/2014/05/614px-Pamela_Geller_2011-150x150.jpg)](http://arstechnica.com/tech-policy/2014/05/deemed-disparaging-anti-muslim-website-denied-trademark/)

    Deemed “disparaging,” anti-Muslim website denied trademark
    ==========================================================

Site Links
----------

-   [About Us](/about-us/)
-   [Advertise with us](/advertise-with-us/)
-   [Contact Us](/contact-us/)
-   [Reprints](/reprints/)

Subscriptions
-------------

-   [Subscribe to Ars](/subscriptions/)

More Reading
------------

-   [RSS Feeds](/rss-feeds/)
-   [Newsletters](/newsletters/)

Conde Nast Sites
----------------

-   [Reddit](http://www.reddit.com/)
-   [Wired](http://www.wired.com/)
-   [Vanity Fair](http://www.vanityfair.com/)
-   [Style](http://www.style.com/)
-   [Details](http://www.details.com/)

Visit our sister sites - - - - - - - - - - - - - - GQ Concierge
Epicurious Men.Style.com Style.com Wired.com Lipstick.com NutritionData
Allure Architectural Digest Bon Appétit Brides Condé Nast Portfolio
Glamour Golf Digest Golf World Lucky Self Teen Vogue The New Yorker
Vanity Fair W

Subscribe to a magazine View All Titles - - - - - - - - - - - - - -
Allure Architectural Digest Bon Appétit Brides Condé Nast Portfolio
Condé Nast Traveler Details Elegant Bride Glamour Golf Digest Golf World
GQ Lucky Modern Bride Self Teen Vogue The New Yorker Vanity Fair Vogue W
Wired

[View Mobile Site](http://arstechnica.com/information-technology/2014/05/ask-ars-why-are-some-programming-languages-faster-than-others/?view=mobile)
----------------------------------------------------------------------------------------------------------------------------------------------------

© 2014 Condé Nast. All rights reserved\
 Use of this Site constitutes acceptance of our [User
Agreement](http://www.condenast.com/privacy-policy) (effective 1/2/14)
and [Privacy
Policy](http://www.condenast.com/privacy-policy#privacypolicy)
(effective 1/2/14), and [Ars Technica Addendum (effective
5/17/2012)](/amendment-to-conde-nast-user-agreement-privacy-policy/)\
 [Your California Privacy
Rights](http://www.condenast.com/privacy-policy#privacypolicy-california)\
 The material on this site may not be reproduced, distributed,
transmitted, cached or otherwise used, except with the prior written
permission of Condé Nast.\
\
 [Ad
Choices](http://www.condenast.com/privacy-policy#privacypolicy-optout)![image](http://cdn.arstechnica.net/wp-content/themes/arstechnica/assets/images/ad_choices_arrow.png)

Log In
------

Username or Email: Password:

Keep me logged in

[Forgot your
password?](https://arstechnica.com/civis/ucp.php?mode=sendpassword) |
[Resend activation
e-mail](https://arstechnica.com/civis/ucp.php?mode=resend_act)

Register an Account
-------------------

Don't have an Ars account? It’s fast and easy to register for one.

Need to register for a new account?
-----------------------------------

If you don't have an account yet it's free and easy.

[Register](https://arstechnica.com/civis/ucp.php?mode=register)

[![image](http://condenast.112.2o7.net/b/ss/condenet-dev/1/H.15.1--NS/0)](http://www.omniture.com "Web Analytics")

This markdown document has been converted from the html document located at:
http://arstechnica.com/information-technology/2014/05/ask-ars-why-are-some-programming-languages-faster-than-others/
