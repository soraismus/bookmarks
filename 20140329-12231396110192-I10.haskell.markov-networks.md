[My Experiments in Truth](http://izbicki.me/blog "Return to main page")

Markov Networks, Monoids, and Futurama
======================================

May 9, 2013 in [Computer
Science](http://izbicki.me/blog/category/computer-science "View all posts in Computer Science"),
[Haskell](http://izbicki.me/blog/category/computer-science/haskell "View all posts in Haskell"),
[HLearn](http://izbicki.me/blog/category/computer-science/haskell/hlearn "View all posts in HLearn"),
[Statistics](http://izbicki.me/blog/category/computer-science/statistics "View all posts in Statistics")
| [15
comments](http://izbicki.me/blog/markov-networks-monoids-and-futurama#comments)

![fry](http://izbicki.me/blog/wp-content/uploads/2013/05/fry-300x225.jpg)In
this post, we’re going to look at how to manipulate multivariate
distributions in Haskell’s [HLearn
library](https://github.com/mikeizbicki/HLearn). There are many ways to
represent multivariate distributions, but we’ll use a technique called
[Markov networks](https://en.wikipedia.org/wiki/Markov_random_field).
These networks have the algebraic structure called a
[monoid](https://en.wikipedia.org/wiki/Monoid) (and group and vector
space), and training them is a
[homomorphism](https://en.wikipedia.org/wiki/Monoid_homomorphism#Monoid_homomorphisms).
Despite the scary names, these mathematical structures make working with
our distributions really easy and convenient—they give us online and
parallel training algorithms “for free.” If you want to go into the
details of how, you can check out my [TFP13
submission](http://izbicki.me/public/papers/tfp2013-hlearn-a-machine-learning-library-for-haskell.pdf),
but in this post we’ll ignore those mathy details to focus on how to use
the library in practice. We’ll use a running example of creating a
distribution over characters in the show Futurama.

### Prelimiaries: Creating the data Types

As usual, this post is a literate haskell file. To run this code, you’ll
need to install the
[hlearn-distributions](http://hackage.haskell.org/package/HLearn-distributions)
package. This package requires GHC version at least 7.6.

    bash> cabal install hlearn-distributions-1.1

Now for some code. We start with our language extensions and imports:

    >{-# LANGUAGE DataKinds #-}
    >{-# LANGUAGE TypeFamilies #-}
    >{-# LANGUAGE TemplateHaskell #-}
    >
    >import HLearn.Algebra
    >import HLearn.Models.Distributions

Next, we’ll create data type to represent Futurama characters. There are
a lot of characters, so we’ll need to keep things pretty organized. The
data type will have a record for everything we might want to know about
a character. Each of these records will be one of the variables in our
multivariate distribution, and all of our data points will have this
type.

![FuturamaCast](http://izbicki.me/blog/wp-content/uploads/2013/05/FuturamaCast-1024x439.png)

    >data Character = Character
    >   { _name      :: String
    >   , _species   :: String
    >   , _job       :: Job
    >   , _isGood    :: Maybe Bool
    >   , _age       :: Double -- in years
    >   , _height    :: Double -- in feet
    >   , _weight    :: Double -- in pounds
    >   }
    >   deriving (Read,Show,Eq,Ord)
    >
    >data Job = Manager | Crew | Henchman | Other
    >   deriving (Read,Show,Eq,Ord)

Now, in order for our library to be able to interpret the Character
type, we call the template haskell function:

    >makeTypeLenses ''Character

This function creates a bunch of data types and type classes for us.
These “type lenses” give us a type-safe way to reference the different
variables in our multivariate distribution. We’ll see how to use these
type level lenses a bit later. There’s no need to understand what’s
going on under the hood, but if you’re curious then checkout the
[hackage
documentation](http://hackage.haskell.org/packages/archive/HLearn-distributions/1.0.0.1/doc/html/HLearn-Models-Distributions-Multivariate-Internal-TypeLens.html)
or [source
code](https://github.com/mikeizbicki/HLearn/blob/master/HLearn-distributions/src/HLearn/Models/Distributions/Multivariate/Internal/TypeLens.hs).

### Training a distribution

Now, we’re ready to create a data set and start training. Here’s a list
of the employees of Planet Express provided by the resident bureaucrat
Hermes Conrad. This list will be our first data set.

![hermes-zoom](http://izbicki.me/blog/wp-content/uploads/2013/05/hermes-zoom.png)

    >planetExpress = 
    >   [ Character "Philip J. Fry"         "human" Crew     (Just True) 1026   5.8 195
    >   , Character "Turanga Leela"         "alien" Crew     (Just True) 43     5.9 170
    >   , Character "Professor Farnsworth"  "human" Manager  (Just True) 85     5.5 160
    >   , Character "Hermes Conrad"         "human" Manager  (Just True) 36     5.3 210
    >   , Character "Amy Wong"              "human" Other    (Just True) 21     5.4 140
    >   , Character "Zoidberg"              "alien" Other    (Just True) 212    5.8 225
    >   , Character "Cubert Farnsworth"     "human" Other    (Just True) 8      4.3 135
    >   ]

Let’s train a distribution from this data. Here’s how we would train a
distribution where every variable is independent of every other
variable:

    >dist1 = train planetExpress :: Multivariate Character
    >  '[ Independent Categorical '[String,String,Job,Maybe Bool]
    >   , Independent Normal '[Double,Double,Double]
    >   ]
    >   Double

In the HLearn library, we always use the function **train** to train a
model from data points. We specify which model to train in the type
signature.

As you can see, the Multivariate distribution takes three type
parameters. The first parameter is the type of our data point, in this
case Character. The second parameter describes the dependency structure
of our distribution. We’ll go over the syntax for the dependency
structure in a bit. For now, just notice that it’s a type-level list of
distributions. Finally, the third parameter is the type we will use to
store our probabilities.

What can we do with this distribution? One simple task we can do is to
find [marginal
distributions](https://en.wikipedia.org/wiki/Marginal_distribution). The
marginal distribution is the distribution of a certain variable ignoring
all the other variables. For example, let’s say I want a distribution of
the species that work at planet express. I can get this by:

    >dist1a = getMargin TH_species dist1

Notice that we specified which variable we’re taking the marginal of by
using the type level lens TH\_species. This data constructor was
automatically created for us by out template haskell function
makeTypeLenses. Every one of our records in the data type has its own
unique type lens. It’s name is the name of the record, prefixed by TH.
These lenses let us infer the types of our marginal distributions at
compile time, rather than at run time. For example, the type of the
marginal distribution of species is:

    ghci> :t dist1a
    dist1a :: Categorical String Double

That is, a categorical distributions whose data points are Strings and
which stores probabilities as a Double. Now, if I wanted a distribution
of the weights of the employees, I can get that by:

    >dist1b = getMargin TH_weight dist1

And the type of this distribution is:

    ghci> :t dist1b
    dist1b :: Normal Double

Now, I can easily plot these marginal distributions with the
**plotDistribution** function:

    ghci> plotDistribution (plotFile "dist1a" $ PNG 250 250) dist1a
    ghci> plotDistribution (plotFile "dist1b" $ PNG 250 250) dist1b

\

![dist1a](http://izbicki.me/blog/wp-content/uploads/2013/05/dist1a.png)![dist1b](http://izbicki.me/blog/wp-content/uploads/2013/05/dist1b.png)

\

![futurama-bender-smoking-cigar-wallpaper](http://izbicki.me/blog/wp-content/uploads/2013/05/futurama-bender-smoking-cigar-wallpaper-225x300.jpg)But
wait! I accidentally forgot to include Bender in the planetExpress data
set! What can I do?

In a traditional statistics library, we would have to retrain our data
from scratch. If we had billions of elements in our data set, this would
be an expensive mistake. But in our HLearn library, we can take
advantage of the model’s monoid structure. In particular, the compiler
used this structure to automatically derive a function called **add1dp**
for us. Let’s look at its type:

    ghci> :t add1dp
    add1dp :: HomTrainer model => model -> Datapoint model -> model

It’s pretty simple. The function takes a model and adds the data point
associated with that model. It returns the model we would have gotten if
the data point had been in our original data set. This is called online
training.

Again, because our distributions form monoids, the compiler derived an
efficient and exact online training algorithm for us automatically.

So let’s create a new distribution that considers bender:

    >bender = Character "Bender Rodriguez" "robot" Crew (Just True) 44 6.1 612
    >dist1' = add1dp dist1 bender

And plot our new marginals:

    ghci> plotDistribution (plotFile "dist1-withbender-species" $ PNG 250 250) $ 
                    getMargin TH_species dist1'
    ghci> plotDistribution (plotFile "dist1-withbender-weight"  $ PNG 250 250) $ 
                    getMargin TH_weight dist1'

\

![dist1-withbender-species](http://izbicki.me/blog/wp-content/uploads/2013/05/dist1-withbender-species.png)![dist1-withbender-weight](http://izbicki.me/blog/wp-content/uploads/2013/05/dist1-withbender-weight.png)

\
 Notice that our categorical marginal has clearly changed, but that our
normal marginal doesn’t seemed to have changed at all. This is because
the plotting routines automatically scale the distribution, and the
normal distribution, when scaled, always looks the same. We can double
check that we actually did change the weight distribution by comparing
the mean:

    ghci> mean dist1b
    176.42857142857142
    ghci> mean $ getMargin TH_weight dist1'
    230.875

Bender’s weight really changed the distribution after all!

### Complicated DependencE structureS

That’s cool, but our original distribution isn’t very interesting. What
makes multivariate distributions interesting is when the variables
affect each other. This is true in our case, so we’d like to be able to
model it. For example, we’ve already seen that robots are much heavier
than organic lifeforms, and are throwing off our statistics. The HLearn
library supports a small subset of Markov Networks for expressing these
dependencies.

We represent Markov Networks as graphs with undirected edges. Every
attribute in our distribution is a node, and every dependence between
attributes is an edge. We can draw this graph with the **plotNetwork**
command:

    ghci> plotNetwork "dist1-network" dist1

![dist1-network](http://izbicki.me/blog/wp-content/uploads/2013/05/dist1-network-300x276.png)As
expected, there are no edges in our graph because everything is
independent. Let’s create a more interesting distribution and plot its
Markov network.

    >dist2 = train planetExpress :: Multivariate Character
    >  '[ Ignore                  '[String]
    >   , MultiCategorical        '[String]
    >   , Independent Categorical '[Job,Maybe Bool]
    >   , Independent Normal      '[Double,Double,Double]
    >   ]
    >   Double

    ghci> plotNetwork "dist2-network" dist2

![dist2-network](http://izbicki.me/blog/wp-content/uploads/2013/05/dist2-network-300x263.png)

Okay, so what just happened?

The syntax for representing the dependence structure is a little
confusing, so let’s go step by step. We represent the dependence
information in the graph as a list of types. Each element in the list
describes both the marginal distribution and the dependence structure
for one or more records in our data type. We must list these elements in
the same order as the original data type.

Notice that we’ve made two changes to the list. First, our list now
starts with the type Ignore ‘[String]. This means that the first string
in our data type—the name—will be ignored. Notice that TH\_name is no
longer in the Markov Network. This makes sense because we expect that a
character’s name should not tell us too much about any of their other
attributes.

Second, we’ve added a dependence. The MultiCategorical distribution
makes everything afterward in the list dependent on that item, but not
the things before it. This means that the exact types of dependencies it
can specify are dependent on the order of the records in our data type.
Let’s see what happens if we change the location of the
MultiCategorical:

    >dist3 = train planetExpress :: Multivariate Character
    >  '[ Ignore '[String]
    >   , Independent Categorical '[String]
    >   , MultiCategorical '[Job]
    >   , Independent Categorical '[Maybe Bool]
    >   , Independent Normal '[Double,Double,Double]
    >   ]
    >   Double

    ghci> plotNetwork "dist3-network" dist3

![dist3-network](http://izbicki.me/blog/wp-content/uploads/2013/05/dist3-network1-300x246.png)

As you can see, our species no longer have any relation to anything
else. Unfortunately, using this syntax, the order of list elements is
important, and so the order we specify our data records is important.

Finally, we can substitute any valid univariate distribution for our
Normal and Categorical distributions. The HLearn library currently
supports Binomial, Exponential, Geometric, LogNormal, and Poisson
distributions. These just don’t make much sense for modelling Futurama
characters, so we’re not using them.

Now, we might be tempted to specify that every variable is fully
dependent on every other variable. In order to do this, we have to
introduce the “Dependent” type. Any valid multivariate distribution can
follow Dependent, but only those records specified in the type-list will
actually be dependent on each other. For example:

    >dist4 = train planetExpress :: Multivariate Character
    >  '[ Ignore '[String]
    >   , MultiCategorical '[String,Job,Maybe Bool]
    >   , Dependent MultiNormal '[Double,Double,Double]
    >   ]
    >   Double

    ghci> plotNetwork "dist4-network" dist4

![distb-network](http://izbicki.me/blog/wp-content/uploads/2013/05/distb-network-300x226.png)

Undoubtably, this is in always going to be the case—everything always
has a slight influence on everything else. Unfortunately, it is not easy
in practice to model these fully dependent distributions. We need
roughly data points to accurately train a distribution, where n is the
number of nodes in our graph and e is the number of edges in our
network. Thus, by selecting that two attributes are independent of each
other, we can greatly reduce the amount of data we need to train an
accurate distribution.

I realize that this syntax is a little awkward. I chose it because it
was relatively easy to implement. Future versions of the library should
support a more intuitive syntax. I also plan to use
[copulas](https://en.wikipedia.org/wiki/Copula_(probability_theory)) to
greatly expand the expressiveness of these distributions. In the mean
time, the best way to figure out the dependencies in a Markov Network
are just to plot it and see visually.

Okay. So what distribution makes the most sense for Futurama characters?
We’ll say that everything depends on both the characters’ species and
job, and that their weight depends on their height.

    >planetExpressDist = train planetExpress :: Multivariate Character
    >  '[ Ignore '[String]
    >   , MultiCategorical '[String,Job]
    >   , Independent Categorical '[Maybe Bool]
    >   , Independent Normal '[Double]
    >   , Dependent MultiNormal '[Double,Double]
    >   ]
    >   Double

    ghci> plotNetwork "planetExpress-network" planetExpressDist

![dist4-network](http://izbicki.me/blog/wp-content/uploads/2013/05/dist4-network-300x225.png)

We still don’t have enough data to to train this network, so let’s
create some more. We start by creating a type for our Markov network
called FuturamaDist. This is just for convenience so we don’t have to
retype the dependence structure many times.

    >type FuturamaDist = Multivariate Character
    >  '[ Ignore '[String]
    >   , MultiCategorical '[String,Job]
    >   , Independent Categorical '[Maybe Bool]
    >   , Independent Normal '[Double]
    >   , Dependent MultiNormal '[Double,Double]
    >   ]
    >   Double

Next, we train some more distribubtions of this type on some of the
characters. We’ll start with Mom Corporation and the brave Space Forces.

![200-futurama\_mom\_and\_sons](http://izbicki.me/blog/wp-content/uploads/2013/05/200-futurama_mom_and_sons.jpg)
![200-kif and
zapp](http://izbicki.me/blog/wp-content/uploads/2013/05/200-kif-and-zapp.jpg)

    >momCorporation = 
    >   [ Character "Mom"                   "human" Manager  (Just False) 100 5.5 130
    >   , Character "Walt"                  "human" Henchman (Just False) 22  6.1 170
    >   , Character "Larry"                 "human" Henchman (Just False) 18  5.9 180
    >   , Character "Igner"                 "human" Henchman (Just False) 15  5.8 175
    >   ]
    >momDist = train momCorporation :: FuturamaDist

    >spaceForce = 
    >   [ Character "Zapp Brannigan"        "human" Manager  (Nothing)   45  6.0 230
    >   , Character "Kif Kroker"            "alien" Crew     (Just True) 113 4.5 120
    >   ]
    >spaceDist = train spaceForce :: FuturamaDist

And now some more robots:

![200-robotmafia](http://izbicki.me/blog/wp-content/uploads/2013/05/200-robotmafia.jpg)
![200-hedonismbot](http://izbicki.me/blog/wp-content/uploads/2013/05/200-hedonismbot.jpg)

    >robots = 
    >   [ bender
    >   , Character "Calculon"              "robot" Other    (Nothing)    123  6.8 650
    >   , Character "The Crushinator"       "robot" Other    (Nothing)    45   8.0 4500
    >   , Character "Clamps"                "robot" Henchman (Just False) 134  5.8 330
    >   , Character "DonBot"                "robot" Manager  (Just False) 178  5.8 520
    >   , Character "Hedonismbot"           "robot" Other    (Just False) 69   4.3 1200
    >   , Character "Preacherbot"           "robot" Manager  (Nothing)    45   5.8 350
    >   , Character "Roberto"               "robot" Other    (Just False) 77   5.9 250
    >   , Character "Robot Devil"           "robot" Other    (Just False) 895  6.0 280
    >   , Character "Robot Santa"           "robot" Other    (Just False) 488  6.3 950
    >   ]
    >robotDist = train robots :: FuturamaDist

Now we’re going to take advantage of the monoid structure of our
multivariate distributions to combine all of these distributions into
one.

    > futuramaDist = planetExpressDist <> momDist <> spaceDist <> robotDist

The resulting distribution is equivalent to having trained a
distribution from scratch on all of the data points:

    train (planetExpress++momCorporation++spaceForces++robots) :: FuturamaDist

We can take advantage of this property any time we use the train
function to automatically parallelize our code. The higher order
function **parallel** will split the training task evenly over each of
your available processors, then merge them together with the monoid
operation. This results in “theoretically perfect” parallel training of
these models.

    parallel train (planetExpress++momCorporation++spaceForces++robots) :: FuturamaDist

Again, this is only possible because the distributions have a monoid
structure.

Now, let’s ask some questions of our distribution. If I pick a character
at random, what’s the probability that they’re a good guy? Let’s plot
the marginal.

    ghci> plotDistribution (plotFile "goodguy" $ PNG 250 250) $ getMargin TH_isGood futuramaDist

![goodguy](http://izbicki.me/blog/wp-content/uploads/2013/05/goodguy.png)

But what if I only want to pick from those characters that are humans,
or those characters that are robots? Statisticians call this
conditioning. We can do that with the condition function:

    ghci> plotDistribution (plotFile "goodguy-human" $ PNG 250 250) $
                 getMargin TH_isGood $ condition TH_species "human" futuramaDist
    ghci> plotDistribution (plotFile "goodguy-robot" $ PNG 250 250) $
                 getMargin TH_isGood $ condition TH_species "robot" futuramaDist

![Preacherbot](http://izbicki.me/blog/wp-content/uploads/2013/05/Preacherbot-174x300.jpg)![goodguy-human](http://izbicki.me/blog/wp-content/uploads/2013/05/goodguy-human.png)
![goodguy-robot](http://izbicki.me/blog/wp-content/uploads/2013/05/goodguy-robot.png)

On the left is the plot for humans, and on the right the plot for
robots. Apparently, original robot sin is much worse than that in
humans! If only they would listen to Preacherbot and repent of their
wicked ways…

Now let’s ask: What’s the average age of an evil robot?

    ghci> mean $ getMargin TH_age $ 
             condition TH_isGood (Just False) $ condition TH_species "robot" futuramaDist 
    273.0769230769231

Notice that conditioning a distribution is a commutative operation. That
means we can condition in any order and still get the exact same
results. Let’s try it:

    ghci> mean $ getMargin TH_age $ 
             condition TH_species "robot" $ condition TH_isGood (Just False) futuramaDist 
    273.0769230769231

There’s one last thing for us to consider. What does our Markov network
look like after conditioning? Let’s find out!

    plotNetwork "condition-species-isGood" $ 
             condition TH_species "robot" $ condition TH_isGood (Just False) futuramaDist

![condition-species-isGood](http://izbicki.me/blog/wp-content/uploads/2013/05/condition-species-isGood-300x168.png)

Notice that conditioning against these variables caused them to go away
from our Markov Network.

Finally, there’s another similar process to conditioning called
“marginalizing out.” This lets us ignore the effects of a single
attribute without specifically saying what that attribute must be. When
we marginalize out on our Markov network, we get the same dependence
structure as if we conditioned.

    plotNetwork "marginalizeOut-species-isGood" $ 
             marginalizeOut TH_species $ marginalizeOut TH_isGood futuramaDist

![condition-species-isGood](http://izbicki.me/blog/wp-content/uploads/2013/05/condition-species-isGood-300x168.png)

Effectively, what the marginalizeOut function does is “forget” the extra
dependencies, whereas the condition function “applies” those
dependencies. In the end, the resulting Markov network has the same
structure, but different values.

Finally, at the start of the post, I mentioned that our multivariate
distributions have group and vector space structure. This gives us two
more operations we can use: the inverse and scalar multiplication. You
can find more posts on how to take advantage of these structures
[here](http://izbicki.me/blog/the-categorical-distributions-algebraic-structure)
and
[here](http://izbicki.me/blog/nuclear-weapon-statistics-using-monoids-groups-and-modules-in-haskell).

### Next time…

![futurama-spacesuits](http://izbicki.me/blog/wp-content/uploads/2013/05/futurama-spacesuits-300x208.jpg)

The best part of all of this is still coming. Next, we’ll take a look at
full on Bayesian classification and why it forms a monoid. Besides
online and parallel trainers, this also gives us a fast cross-validation
method.

There’ll also be a posts about the monoid structure of Markov *chains*,
the Free HomTrainer, and how this whole algebraic framework applies to
NP-approximation algorithms as well.

Subscribe to the [RSS feed](http://izbicki.me/blog/feed) to stay tuned.

15 comments
-----------

[Comments feed for this
article](http://izbicki.me/blog/markov-networks-monoids-and-futurama/feed)

Trackback link:
[http://izbicki.me/blog/markov-networks-monoids-and-futurama/trackback](http://izbicki.me/blog/markov-networks-monoids-and-futurama/trackback)

1.  ![Kate’s
    avatar](http://0.gravatar.com/avatar/a0489e211cb9cbba300825f22429b2ce?s=50&d=http%3A%2F%2F0.gravatar.com%2Favatar%2Fad516503a11cd5ca435acc9bb6523536%3Fs%3D50&r=G)

    Kate on [June 19, 2013 at 10:46
    AM](http://izbicki.me/blog/markov-networks-monoids-and-futurama#comment-1727 "Permalink to this comment")

    Very interesting! I look forward to hearing your explanation of why
    Markov networks and Bayesian classification have a monoid structure

    [Reply](/blog/markov-networks-monoids-and-futurama?replytocom=1727#respond)

2.  [![Philippe Desjardins-Proulx’s
    avatar](http://1.gravatar.com/avatar/b3d06ebf77476699d95bee629c8b3c0c?s=50&d=http%3A%2F%2F1.gravatar.com%2Favatar%2Fad516503a11cd5ca435acc9bb6523536%3Fs%3D50&r=G)](http://phdp.github.io/)

    [Philippe Desjardins-Proulx](http://phdp.github.io/) on [June 19,
    2013 at 8:45
    PM](http://izbicki.me/blog/markov-networks-monoids-and-futurama#comment-1734 "Permalink to this comment")

    Thank you for the great post, your work on the algebraic structure
    of machine learning algorithms is fascinating.

    [Reply](/blog/markov-networks-monoids-and-futurama?replytocom=1734#respond)

3.  ![nh2’s
    avatar](http://0.gravatar.com/avatar/ebedd2f3dfbc9d3ec006979967ee8a75?s=50&d=http%3A%2F%2F0.gravatar.com%2Favatar%2Fad516503a11cd5ca435acc9bb6523536%3Fs%3D50&r=G)

    nh2 on [July 11, 2013 at 2:06
    AM](http://izbicki.me/blog/markov-networks-monoids-and-futurama#comment-1840 "Permalink to this comment")

    For the current version (HLearn-distritubutions-1.0.0.1) I had to
    also pass in the PicType for plotting:

    plotDistribution (plotFile “dist1a” EPS) dist1a

    [Reply](/blog/markov-networks-monoids-and-futurama?replytocom=1840#respond)

4.  ![nh2’s
    avatar](http://0.gravatar.com/avatar/ebedd2f3dfbc9d3ec006979967ee8a75?s=50&d=http%3A%2F%2F0.gravatar.com%2Favatar%2Fad516503a11cd5ca435acc9bb6523536%3Fs%3D50&r=G)

    nh2 on [July 12, 2013 at 5:06
    AM](http://izbicki.me/blog/markov-networks-monoids-and-futurama#comment-1844 "Permalink to this comment")

    \> planetExpress = train planetExpress :: Multivariate Character

    doesn’t compile, it should be:

    \> planetExpressDist = train planetExpress :: Multivariate Character

    and in

    \> futuramaDist = dist1 momDist spaceDist robotDist

    dist1 has the wrong type, it shoud be be:

    \> futuramaDist = planetExpressDist momDist spaceDist robotDist

    [Reply](/blog/markov-networks-monoids-and-futurama?replytocom=1844#respond)

    1.  [![Mike’s
        avatar](http://1.gravatar.com/avatar/7019dabd783b2ecfaf04f5440ebb56df?s=50&d=http%3A%2F%2F1.gravatar.com%2Favatar%2Fad516503a11cd5ca435acc9bb6523536%3Fs%3D50&r=G)](http://izbicki.me)

        [Mike](http://izbicki.me) on [July 12, 2013 at 9:28
        AM](http://izbicki.me/blog/markov-networks-monoids-and-futurama#comment-1845 "Permalink to this comment")

        Fixed. Thanks for the corrections.

        [Reply](/blog/markov-networks-monoids-and-futurama?replytocom=1845#respond)

        1.  ![nh2’s
            avatar](http://0.gravatar.com/avatar/ebedd2f3dfbc9d3ec006979967ee8a75?s=50&d=http%3A%2F%2F0.gravatar.com%2Favatar%2Fad516503a11cd5ca435acc9bb6523536%3Fs%3D50&r=G)

            nh2 on [July 12, 2013 at 7:13
            PM](http://izbicki.me/blog/markov-networks-monoids-and-futurama#comment-1847 "Permalink to this comment")

            Awesome. I’d like to see more blog posts about HLearn!

            [Reply](/blog/markov-networks-monoids-and-futurama?replytocom=1847#respond)

            1.  [![Mike’s
                avatar](http://1.gravatar.com/avatar/7019dabd783b2ecfaf04f5440ebb56df?s=50&d=http%3A%2F%2F1.gravatar.com%2Favatar%2Fad516503a11cd5ca435acc9bb6523536%3Fs%3D50&r=G)](http://izbicki.me)

                [Mike](http://izbicki.me) on [July 13, 2013 at 12:34
                AM](http://izbicki.me/blog/markov-networks-monoids-and-futurama#comment-1848 "Permalink to this comment")

                They’re on the way
                ![:)](http://izbicki.me/blog/wp-includes/images/smilies/icon_smile.gif)

                [Reply](/blog/markov-networks-monoids-and-futurama?replytocom=1848#respond)

5.  ![Michael’s
    avatar](http://1.gravatar.com/avatar/91eb9333a30be7f4124cf1ea044c9388?s=50&d=http%3A%2F%2F1.gravatar.com%2Favatar%2Fad516503a11cd5ca435acc9bb6523536%3Fs%3D50&r=G)

    Michael on [March 21, 2014 at 3:22
    AM](http://izbicki.me/blog/markov-networks-monoids-and-futurama#comment-2567 "Permalink to this comment")

    In your hackage documentation[1] the links under Trainable and
    TypeLens are broken (works fine for the same links under getHList or
    the links under TemplateHaskell) In addition the link to your source
    code is broken[2] (though a quick github search shows where it now
    is[3]. Thanks for the post and the library.\
     [1]
    [http://hackage.haskell.org/package/HLearn-distributions-1.0.0.1/docs/HLearn-Models-Distributions-Multivariate-Internal-TypeLens.html\#t:GetHList](http://hackage.haskell.org/package/HLearn-distributions-1.0.0.1/docs/HLearn-Models-Distributions-Multivariate-Internal-TypeLens.html#t:GetHList)\
     [2]
    [https://github.com/mikeizbicki/HLearn/blob/master/HLearn-distributions/src/HLearn/Models/Distributions/Multivariate/Internal/TypeLens.hs](https://github.com/mikeizbicki/HLearn/blob/master/HLearn-distributions/src/HLearn/Models/Distributions/Multivariate/Internal/TypeLens.hs)\
     [3]
    [https://github.com/mikeizbicki/HLearn/blob/master/src/HLearn/Models/Distributions/Multivariate/Internal/TypeLens.hs](https://github.com/mikeizbicki/HLearn/blob/master/src/HLearn/Models/Distributions/Multivariate/Internal/TypeLens.hs)

    [Reply](/blog/markov-networks-monoids-and-futurama?replytocom=2567#respond)

6.  ![Brandon’s
    avatar](http://0.gravatar.com/avatar/e8d817bc2cce0c2be410647d3ce67316?s=50&d=http%3A%2F%2F0.gravatar.com%2Favatar%2Fad516503a11cd5ca435acc9bb6523536%3Fs%3D50&r=G)

    Brandon on [March 21, 2014 at 11:02
    AM](http://izbicki.me/blog/markov-networks-monoids-and-futurama#comment-2572 "Permalink to this comment")

    I’m running OS X 10.9 Mavericks and ran the . I receive the
    following error when using cabal install:

    src/HLearn/Models/Distributions/Univariate/Internal/Moments.hs:32:1:\
     \`basicUnsafeFreeze’ is not a (visible) method of class
    \`Data.Vector.Generic.Base.Vector’\
     \#\#\
     Failed to install HLearn-distributions-1.1.0.1

    Any suggestions?

    [Reply](/blog/markov-networks-monoids-and-futurama?replytocom=2572#respond)

    1.  [![Mike’s
        avatar](http://1.gravatar.com/avatar/7019dabd783b2ecfaf04f5440ebb56df?s=50&d=http%3A%2F%2F1.gravatar.com%2Favatar%2Fad516503a11cd5ca435acc9bb6523536%3Fs%3D50&r=G)](http://izbicki.me)

        [Mike](http://izbicki.me) on [March 21, 2014 at 11:39
        AM](http://izbicki.me/blog/markov-networks-monoids-and-futurama#comment-2573 "Permalink to this comment")

        Which ghc version are you running? It requires 7.6

        [Reply](/blog/markov-networks-monoids-and-futurama?replytocom=2573#respond)

        1.  ![Brandon’s
            avatar](http://0.gravatar.com/avatar/e8d817bc2cce0c2be410647d3ce67316?s=50&d=http%3A%2F%2F0.gravatar.com%2Favatar%2Fad516503a11cd5ca435acc9bb6523536%3Fs%3D50&r=G)

            Brandon on [March 21, 2014 at 12:13
            PM](http://izbicki.me/blog/markov-networks-monoids-and-futurama#comment-2579 "Permalink to this comment")

            \>ghc –version\
             The Glorious Glasgow Haskell Compilation System, version
            7.6.3

            [Reply](/blog/markov-networks-monoids-and-futurama?replytocom=2579#respond)

            1.  [![Mike’s
                avatar](http://1.gravatar.com/avatar/7019dabd783b2ecfaf04f5440ebb56df?s=50&d=http%3A%2F%2F1.gravatar.com%2Favatar%2Fad516503a11cd5ca435acc9bb6523536%3Fs%3D50&r=G)](http://izbicki.me)

                [Mike](http://izbicki.me) on [March 21, 2014 at 1:23
                PM](http://izbicki.me/blog/markov-networks-monoids-and-futurama#comment-2582 "Permalink to this comment")

                The file your error is in doesn’t actually use a vector
                at all. I it does provide an Unbox instance though for
                the Moments type. It does this via the vector-unbox-th
                package, which provides a template haskell function to
                generate the instance. Maybe the error is caused by an
                incompatable version? Which version of that package do
                you have installed?

                [Reply](/blog/markov-networks-monoids-and-futurama?replytocom=2582#respond)

                1.  ![Brandon’s
                    avatar](http://0.gravatar.com/avatar/e8d817bc2cce0c2be410647d3ce67316?s=50&d=http%3A%2F%2F0.gravatar.com%2Favatar%2Fad516503a11cd5ca435acc9bb6523536%3Fs%3D50&r=G)

                    Brandon on [March 21, 2014 at 1:38
                    PM](http://izbicki.me/blog/markov-networks-monoids-and-futurama#comment-2583 "Permalink to this comment")

                    \>cabal install vector-th-unbox\
                     Resolving dependencies…\
                     All the requested packages are already installed:\
                     vector-th-unbox-0.2.0.2\
                     Use –reinstall if you want to reinstall anyway.

                    —————————————————–\
                     Here’s the full install error section:

                    Loading package HLearn-datastructures-1.1.0 …
                    linking … done.

                    src/HLearn/Models/Distributions/Univariate/Internal/Moments.hs:32:1:\
                     \`basicUnsafeFreeze’ is not a (visible) method of
                    class \`Data.Vector.Generic.Base.Vector’

                    src/HLearn/Models/Distributions/Univariate/Internal/Moments.hs:32:1:\
                     \`basicUnsafeThaw’ is not a (visible) method of
                    class \`Data.Vector.Generic.Base.Vector’

                    src/HLearn/Models/Distributions/Univariate/Internal/Moments.hs:32:1:\
                     \`basicLength’ is not a (visible) method of class
                    \`Data.Vector.Generic.Base.Vector’

                    src/HLearn/Models/Distributions/Univariate/Internal/Moments.hs:32:1:\
                     \`basicUnsafeSlice’ is not a (visible) method of
                    class \`Data.Vector.Generic.Base.Vector’

                    src/HLearn/Models/Distributions/Univariate/Internal/Moments.hs:32:1:\
                     \`basicUnsafeIndexM’ is not a (visible) method of
                    class \`Data.Vector.Generic.Base.Vector’

                    src/HLearn/Models/Distributions/Univariate/Internal/Moments.hs:32:1:\
                     \`basicUnsafeCopy’ is not a (visible) method of
                    class \`Data.Vector.Generic.Base.Vector’

                    src/HLearn/Models/Distributions/Univariate/Internal/Moments.hs:32:1:\
                     \`elemseq’ is not a (visible) method of class
                    \`Data.Vector.Generic.Base.Vector’

                    src/HLearn/Models/Distributions/Univariate/Internal/Moments.hs:32:1:\
                     \`basicUnsafeFreeze’ is not a (visible) method of
                    class \`Data.Vector.Generic.Base.Vector’

                    src/HLearn/Models/Distributions/Univariate/Internal/Moments.hs:32:1:\
                     \`basicUnsafeThaw’ is not a (visible) method of
                    class \`Data.Vector.Generic.Base.Vector’

                    src/HLearn/Models/Distributions/Univariate/Internal/Moments.hs:32:1:\
                     \`basicLength’ is not a (visible) method of class
                    \`Data.Vector.Generic.Base.Vector’

                    src/HLearn/Models/Distributions/Univariate/Internal/Moments.hs:32:1:\
                     \`basicUnsafeSlice’ is not a (visible) method of
                    class \`Data.Vector.Generic.Base.Vector’

                    src/HLearn/Models/Distributions/Univariate/Internal/Moments.hs:32:1:\
                     \`basicUnsafeIndexM’ is not a (visible) method of
                    class \`Data.Vector.Generic.Base.Vector’

                    src/HLearn/Models/Distributions/Univariate/Internal/Moments.hs:32:1:\
                     \`basicUnsafeCopy’ is not a (visible) method of
                    class \`Data.Vector.Generic.Base.Vector’

                    src/HLearn/Models/Distributions/Univariate/Internal/Moments.hs:32:1:\
                     \`elemseq’ is not a (visible) method of class
                    \`Data.Vector.Generic.Base.Vector’

                    src/HLearn/Models/Distributions/Univariate/Internal/Moments.hs:32:1:\
                     \`basicLength’ is not a (visible) method of class
                    \`Data.Vector.Generic.Mutable.MVector’

                    src/HLearn/Models/Distributions/Univariate/Internal/Moments.hs:32:1:\
                     \`basicUnsafeSlice’ is not a (visible) method of
                    class \`Data.Vector.Generic.Mutable.MVector’

                    src/HLearn/Models/Distributions/Univariate/Internal/Moments.hs:32:1:\
                     \`basicOverlaps’ is not a (visible) method of class
                    \`Data.Vector.Generic.Mutable.MVector’

                    src/HLearn/Models/Distributions/Univariate/Internal/Moments.hs:32:1:\
                     \`basicUnsafeNew’ is not a (visible) method of
                    class \`Data.Vector.Generic.Mutable.MVector’

                    src/HLearn/Models/Distributions/Univariate/Internal/Moments.hs:32:1:\
                     \`basicUnsafeReplicate’ is not a (visible) method
                    of class \`Data.Vector.Generic.Mutable.MVector’

                    src/HLearn/Models/Distributions/Univariate/Internal/Moments.hs:32:1:\
                     \`basicUnsafeRead’ is not a (visible) method of
                    class \`Data.Vector.Generic.Mutable.MVector’

                    src/HLearn/Models/Distributions/Univariate/Internal/Moments.hs:32:1:\
                     \`basicUnsafeWrite’ is not a (visible) method of
                    class \`Data.Vector.Generic.Mutable.MVector’

                    src/HLearn/Models/Distributions/Univariate/Internal/Moments.hs:32:1:\
                     \`basicClear’ is not a (visible) method of class
                    \`Data.Vector.Generic.Mutable.MVector’

                    src/HLearn/Models/Distributions/Univariate/Internal/Moments.hs:32:1:\
                     \`basicSet’ is not a (visible) method of class
                    \`Data.Vector.Generic.Mutable.MVector’

                    src/HLearn/Models/Distributions/Univariate/Internal/Moments.hs:32:1:\
                     \`basicUnsafeCopy’ is not a (visible) method of
                    class \`Data.Vector.Generic.Mutable.MVector’

                    src/HLearn/Models/Distributions/Univariate/Internal/Moments.hs:32:1:\
                     \`basicUnsafeMove’ is not a (visible) method of
                    class \`Data.Vector.Generic.Mutable.MVector’

                    src/HLearn/Models/Distributions/Univariate/Internal/Moments.hs:32:1:\
                     \`basicUnsafeGrow’ is not a (visible) method of
                    class \`Data.Vector.Generic.Mutable.MVector’

                    src/HLearn/Models/Distributions/Univariate/Internal/Moments.hs:32:1:\
                     \`basicLength’ is not a (visible) method of class
                    \`Data.Vector.Generic.Mutable.MVector’

                    src/HLearn/Models/Distributions/Univariate/Internal/Moments.hs:32:1:\
                     \`basicUnsafeSlice’ is not a (visible) method of
                    class \`Data.Vector.Generic.Mutable.MVector’

                    src/HLearn/Models/Distributions/Univariate/Internal/Moments.hs:32:1:\
                     \`basicOverlaps’ is not a (visible) method of class
                    \`Data.Vector.Generic.Mutable.MVector’

                    src/HLearn/Models/Distributions/Univariate/Internal/Moments.hs:32:1:\
                     \`basicUnsafeNew’ is not a (visible) method of
                    class \`Data.Vector.Generic.Mutable.MVector’

                    src/HLearn/Models/Distributions/Univariate/Internal/Moments.hs:32:1:\
                     \`basicUnsafeReplicate’ is not a (visible) method
                    of class \`Data.Vector.Generic.Mutable.MVector’

                    src/HLearn/Models/Distributions/Univariate/Internal/Moments.hs:32:1:\
                     \`basicUnsafeRead’ is not a (visible) method of
                    class \`Data.Vector.Generic.Mutable.MVector’

                    src/HLearn/Models/Distributions/Univariate/Internal/Moments.hs:32:1:\
                     \`basicUnsafeWrite’ is not a (visible) method of
                    class \`Data.Vector.Generic.Mutable.MVector’

                    src/HLearn/Models/Distributions/Univariate/Internal/Moments.hs:32:1:\
                     \`basicClear’ is not a (visible) method of class
                    \`Data.Vector.Generic.Mutable.MVector’

                    src/HLearn/Models/Distributions/Univariate/Internal/Moments.hs:32:1:\
                     \`basicSet’ is not a (visible) method of class
                    \`Data.Vector.Generic.Mutable.MVector’

                    src/HLearn/Models/Distributions/Univariate/Internal/Moments.hs:32:1:\
                     \`basicUnsafeCopy’ is not a (visible) method of
                    class \`Data.Vector.Generic.Mutable.MVector’

                    src/HLearn/Models/Distributions/Univariate/Internal/Moments.hs:32:1:\
                     \`basicUnsafeMove’ is not a (visible) method of
                    class \`Data.Vector.Generic.Mutable.MVector’

                    src/HLearn/Models/Distributions/Univariate/Internal/Moments.hs:32:1:\
                     \`basicUnsafeGrow’ is not a (visible) method of
                    class \`Data.Vector.Generic.Mutable.MVector’\
                     Failed to install HLearn-distributions-1.1.0.1

                    ——————————————————————————-

                    Thanks! I’m incredibly excited to start using this.

                2.  [![Mike’s
                    avatar](http://1.gravatar.com/avatar/7019dabd783b2ecfaf04f5440ebb56df?s=50&d=http%3A%2F%2F1.gravatar.com%2Favatar%2Fad516503a11cd5ca435acc9bb6523536%3Fs%3D50&r=G)](http://izbicki.me)

                    [Mike](http://izbicki.me) on [March 21, 2014 at 2:13
                    PM](http://izbicki.me/blog/markov-networks-monoids-and-futurama#comment-2587 "Permalink to this comment")

                    The problem is definitely with the template haskell
                    code, but I don’t see what’s causing it. Somewhere,
                    there must be a dependency that’s not set right.

                    If you download the tarball code from hackage (or
                    use the master branch on github, but this has some
                    extra changes and lacks documentation), you should
                    be able to uncomment these lines to get it to work:

                    ` derivingUnbox "Moments3" [t| (U.Unbox a) => (Moments3 a) -> (a, a, a) |] [| \ (Moments3 m0 m1 m2) -> (m0,m1,m2) |] [| \ (m0,m1,m2) -> (Moments3 m0 m1 m2) |]`

                    the unbox instance isn’t actually necessary here,
                    it’s just provided for convenience in case you want
                    to put any of the distributions in an unboxed tuple.

    2.  [![Mike’s
        avatar](http://1.gravatar.com/avatar/7019dabd783b2ecfaf04f5440ebb56df?s=50&d=http%3A%2F%2F1.gravatar.com%2Favatar%2Fad516503a11cd5ca435acc9bb6523536%3Fs%3D50&r=G)](http://izbicki.me)

        [Mike](http://izbicki.me) on [March 26, 2014 at 9:27
        PM](http://izbicki.me/blog/markov-networks-monoids-and-futurama#comment-2588 "Permalink to this comment")

        A similar error was reported on github:
        [https://github.com/mikeizbicki/HLearn/issues/34](https://github.com/mikeizbicki/HLearn/issues/34)
        along with a fix. I’ve uploaded a new version to hackage, so you
        should be able to just cabal install it now.

        [Reply](/blog/markov-networks-monoids-and-futurama?replytocom=2588#respond)

### Reply [Cancel reply](/blog/markov-networks-monoids-and-futurama#respond)

Your email address will not be published. Required fields are marked \*

Name \*

Email \*

Website

Your comment

You may use these HTML tags and attributes:
`<a href="" title=""> <abbr title=""> <acronym title=""> <b> <blockquote cite=""> <cite> <code> <del datetime=""> <em> <i> <q cite=""> <strike> <strong> `

### Categories

-   [Beer](http://izbicki.me/blog/category/beer "View all posts filed under Beer")
    (3)
-   [Computer
    Science](http://izbicki.me/blog/category/computer-science "View all posts filed under Computer Science")
    (12)
    -   [Data
        Mining](http://izbicki.me/blog/category/computer-science/data-mining "View all posts filed under Data Mining")
        (2)
    -   [Haskell](http://izbicki.me/blog/category/computer-science/haskell "View all posts filed under Haskell")
        (8)
        -   [HLearn](http://izbicki.me/blog/category/computer-science/haskell/hlearn "View all posts filed under HLearn")
            (7)
        -   [Other](http://izbicki.me/blog/category/computer-science/haskell/haskell-other "View all posts filed under Other")
            (1)

    -   [Paper
        Analysis](http://izbicki.me/blog/category/computer-science/paper-analysis-computer-science "View all posts filed under Paper Analysis")
        (1)
    -   [Statistics](http://izbicki.me/blog/category/computer-science/statistics "View all posts filed under Statistics")
        (5)

-   [Projects](http://izbicki.me/blog/category/projects "View all posts filed under Projects")
    (8)
    -   [Litig-o-meter](http://izbicki.me/blog/category/projects/litig-o-meter "View all posts filed under Litig-o-meter")
        (1)
    -   [Plowshares](http://izbicki.me/blog/category/projects/plowshares "View all posts filed under Plowshares")
        (1)
    -   [PokerPirate](http://izbicki.me/blog/category/projects/pokerpirate-projects "View all posts filed under PokerPirate")
        (6)

-   [Religion](http://izbicki.me/blog/category/religion "View all posts filed under Religion")
    (21)
    -   [Anarchism](http://izbicki.me/blog/category/religion/anarchism "View all posts filed under Anarchism")
        (3)
    -   [Conscientious
        Objection](http://izbicki.me/blog/category/religion/conscientious-objection "View all posts filed under Conscientious Objection")
        (4)
    -   [Experiments](http://izbicki.me/blog/category/religion/experiments "View all posts filed under Experiments")
        (3)
    -   [My CO
        Discharge](http://izbicki.me/blog/category/religion/my-co-discharge "View all posts filed under My CO Discharge")
        (8)
    -   [Thoughts](http://izbicki.me/blog/category/religion/thoughts "View all posts filed under Thoughts")
        (4)

### Archives

-   [August 2013](http://izbicki.me/blog/2013/08 "August 2013") (2)
-   [July 2013](http://izbicki.me/blog/2013/07 "July 2013") (2)
-   [June 2013](http://izbicki.me/blog/2013/06 "June 2013") (2)
-   [May 2013](http://izbicki.me/blog/2013/05 "May 2013") (2)
-   [April 2013](http://izbicki.me/blog/2013/04 "April 2013") (1)
-   [January 2013](http://izbicki.me/blog/2013/01 "January 2013") (3)
-   [November 2012](http://izbicki.me/blog/2012/11 "November 2012") (3)
-   [October 2012](http://izbicki.me/blog/2012/10 "October 2012") (2)
-   [July 2012](http://izbicki.me/blog/2012/07 "July 2012") (4)
-   [June 2012](http://izbicki.me/blog/2012/06 "June 2012") (3)
-   [May 2012](http://izbicki.me/blog/2012/05 "May 2012") (2)
-   [April 2012](http://izbicki.me/blog/2012/04 "April 2012") (1)
-   [March 2012](http://izbicki.me/blog/2012/03 "March 2012") (3)
-   [December 2011](http://izbicki.me/blog/2011/12 "December 2011") (2)
-   [November 2011](http://izbicki.me/blog/2011/11 "November 2011") (6)
-   [October 2011](http://izbicki.me/blog/2011/10 "October 2011") (4)

[Subscribe to feed](http://izbicki.me/blog/feed)

Powered by [WordPress](http://wordpress.org/) and
[Tarski](http://tarskitheme.com/)

-   [](http://twitter.com/share)
-   -   -   -   [Pin
    It](http://pinterest.com/pin/create/button/?url=http%3A%2F%2Fizbicki.me%2Fblog%2Fmarkov-networks-monoids-and-futurama&media=&description=In+this+post%2C+we%27re+going+to+look+at+how+to+manipulate+multivariate+distributions+in+Haskell%27s%C2%A0HLearn+library.+There+are+many+ways+to+repr)
-   

This markdown document has been converted from the html document located at:
http://izbicki.me/blog/markov-networks-monoids-and-futurama
