[Some Thoughts on a Mysterious Universe](http://moalquraishi.wordpress.com/ "Some Thoughts on a Mysterious Universe")
=====================================================================================================================

by Mohammed AlQuraishi
----------------------

Menu
====

[Skip to content](#content "Skip to content")

-   [Home](http://moalquraishi.wordpress.com/)
-   [About Blog](http://moalquraishi.wordpress.com/about-blog/)
-   [About Me](http://moalquraishi.wordpress.com/about/)

[What Does a Neural Network Actually Do?](http://moalquraishi.wordpress.com/2014/05/25/what-does-a-neural-network-actually-do/)
===============================================================================================================================

[**Mohammed
AlQuraishi](http://moalquraishi.wordpress.com/author/moalquraishi/ "View all posts by Mohammed AlQuraishi")[**May
25,
2014](http://moalquraishi.wordpress.com/2014/05/25/what-does-a-neural-network-actually-do/ "8:34 am")[**12](http://moalquraishi.wordpress.com/2014/05/25/what-does-a-neural-network-actually-do/#comments "Comment on What Does a Neural Network Actually Do?")

There has been a lot of renewed interest lately in neural networks (NNs)
due to their popularity as a model for deep learning architectures
(there are non-NN based deep learning approaches based on [sum-products
networks](http://turing.cs.washington.edu/papers/uai11-poon.pdf) and
[support vector machines with deep
kernels](http://cseweb.ucsd.edu/~saul/papers/nips09_kernel.pdf), among
others). Perhaps due to their loose analogy with biological brains, the
behavior of neural networks has acquired an almost mystical status. This
is compounded by the fact that theoretical analysis of multilayer
perceptrons (one of the most common architectures) remains very limited,
although the situation is [gradually
improving](http://www.stanford.edu/~asaxe/papers/Saxe,%20McClelland,%20Ganguli%20-%202013%20-%20Dynamics%20of%20learning%20in%20deep%20linear%20neural%20networks.pdf).
To gain an intuitive understanding of what a learning algorithm does, I
usually like to think about its representational power, as this provides
insight into what *can*, if not necessarily what *does*, happen inside
the algorithm to solve a given problem. I will do this here for the case
of multilayer perceptrons. By the end of this informal discussion I hope
to provide an intuitive picture of the surprisingly simple
representations that NNs encode.

I should note at the outset that what I will describe applies only to a
very limited subset of neural networks, namely the feedforward
architecture known as a multilayer perceptron. There are many other
architectures that are capable of very different representations.
Furthermore, I will be making certain simplifying assumptions that do
not generally hold even for multilayer perceptrons. I find that these
assumptions help to substantially simplify the discussion while still
capturing the underlying essence of what this type of neural network
does. I will try to be explicit about everything.

Let’s begin with the simplest configuration possible: two inputs node
wired to a single output node. Our NN looks like this:

![Figure
1](http://moalquraishi.files.wordpress.com/2014/05/figure-1.png?w=330&h=165)

The label associated with a node denotes its output value, and the label
associated with an edge denotes its weight. The topmost node
![h](http://s0.wp.com/latex.php?latex=h&bg=ffffff&fg=666666&s=-1 "h")
represents the output of this NN, which is:

![h = f\\left(w\_1 x\_1+w\_2
x\_2+b\\right)](http://s0.wp.com/latex.php?latex=h+%3D+f%5Cleft%28w_1+x_1%2Bw_2+x_2%2Bb%5Cright%29&bg=ffffff&fg=666666&s=1 "h = f\left(w_1 x_1+w_2 x_2+b\right)")

In other words, the NN computes a linear combination of the two inputs
![x\_1](http://s0.wp.com/latex.php?latex=x_1&bg=ffffff&fg=666666&s=-1 "x_1")
and
![x\_2](http://s0.wp.com/latex.php?latex=x_2&bg=ffffff&fg=666666&s=-1 "x_2"),
weighted by
![w\_1](http://s0.wp.com/latex.php?latex=w_1&bg=ffffff&fg=666666&s=-1 "w_1")
and
![w\_2](http://s0.wp.com/latex.php?latex=w_2&bg=ffffff&fg=666666&s=-1 "w_2")
respectively, adds an arbitrary bias term
![b](http://s0.wp.com/latex.php?latex=b&bg=ffffff&fg=666666&s=-1 "b")
and then passes the result through a function
![f](http://s0.wp.com/latex.php?latex=f&bg=ffffff&fg=666666&s=-1 "f"),
known as the activation function. There are a number of different
activation functions in common use and they all typically exhibit a
nonlinearity. The sigmoid activation
![f(a)=\\frac{1}{1+e\^{-a}}](http://s0.wp.com/latex.php?latex=f%28a%29%3D%5Cfrac%7B1%7D%7B1%2Be%5E%7B-a%7D%7D&bg=ffffff&fg=666666&s=-1 "f(a)=\frac{1}{1+e^{-a}}"),
plotted below, is a common example.

![Figure
2](http://moalquraishi.files.wordpress.com/2014/05/figure-2.png?w=373&h=244)

As we shall see momentarily, the nonlinearity of an activation function
is what enables neural networks to represent complicated input-output
mappings. The linear regime of an activation function can also be
exploited by a neural network, but for the sake of simplifying our
discussion even further, we will choose an activation function without a
linear regime. In other words,
![f](http://s0.wp.com/latex.php?latex=f&bg=ffffff&fg=666666&s=-1 "f")
will be a simple step function:

![Figure
3](http://moalquraishi.files.wordpress.com/2014/05/figure-3.png?w=373&h=244)

This will allow us to reason about the salient features of a neural
network without getting bogged down in the details. In particular, let’s
consider what our current neural network is capable of. The output node
can generate one of two values, and this is determined by a linear
weighting of the values of the input nodes. Such a function is a binary
linear classifier. As shown below, depending on the values of
![w\_1](http://s0.wp.com/latex.php?latex=w_1&bg=ffffff&fg=666666&s=-1 "w_1")
and
![w\_2](http://s0.wp.com/latex.php?latex=w_2&bg=ffffff&fg=666666&s=-1 "w_2"),
one regime in this two-dimensional input space yields a response of
![0](http://s0.wp.com/latex.php?latex=0&bg=ffffff&fg=666666&s=-1 "0")
(white) and the other a response of
![1](http://s0.wp.com/latex.php?latex=1&bg=ffffff&fg=666666&s=-1 "1")
(shaded):

![Figure
4](http://moalquraishi.files.wordpress.com/2014/05/figure-4.png?w=362&h=363)

Let’s now add two more output nodes (a neural network can have more than
a single output). I will need to introduce a bit of notation to keep
track of everything. The weight associated with an edge from the
![jth](http://s0.wp.com/latex.php?latex=jth&bg=ffffff&fg=666666&s=-1 "jth")
node in the first layer to the
![ith](http://s0.wp.com/latex.php?latex=ith&bg=ffffff&fg=666666&s=-1 "ith")
node in the second layer will be denoted by
![w\_{ij}\^{(1)}](http://s0.wp.com/latex.php?latex=w_%7Bij%7D%5E%7B%281%29%7D&bg=ffffff&fg=666666&s=-1 "w_{ij}^{(1)}").
The output of the
![ith](http://s0.wp.com/latex.php?latex=ith&bg=ffffff&fg=666666&s=-1 "ith")
node in the
![nth](http://s0.wp.com/latex.php?latex=nth&bg=ffffff&fg=666666&s=-1 "nth")
layer will be denoted by
![a\_i\^{(n)}](http://s0.wp.com/latex.php?latex=a_i%5E%7B%28n%29%7D&bg=ffffff&fg=666666&s=-1 "a_i^{(n)}").
Thus ![x\_1 =
a\_1\^{(1)}](http://s0.wp.com/latex.php?latex=x_1+%3D+a_1%5E%7B%281%29%7D&bg=ffffff&fg=666666&s=-1 "x_1 = a_1^{(1)}")
and ![x\_2 =
a\_2\^{(1)}](http://s0.wp.com/latex.php?latex=x_2+%3D+a_2%5E%7B%281%29%7D&bg=ffffff&fg=666666&s=-1 "x_2 = a_2^{(1)}").

![Figure
5](http://moalquraishi.files.wordpress.com/2014/05/figure-5.png?w=348&h=192)

Every output node in this NN is wired to the same set of input nodes,
but the weights are allowed to vary. Below is one possible
configuration, where the regions triggering a value of
![1](http://s0.wp.com/latex.php?latex=1&bg=ffffff&fg=666666&s=-1 "1")
are overlaid and colored in correspondence with the colors of the output
nodes:

![Figure
6](http://moalquraishi.files.wordpress.com/2014/05/figure-6.png?w=362&h=358)

So far we haven’t really done anything, because we just overlaid the
decision boundaries of three linear classifiers without combining them
in any meaningful way. Let’s do that now, by feeding the outputs of the
top three nodes as inputs into a new node. I will hollow out the nodes
in the middle layer to indicate that they are no longer the final output
of the NN.

![Figure
7](http://moalquraishi.files.wordpress.com/2014/05/figure-7.png?w=477&h=321)

The value of the single output node at the third layer is:

![a\_1\^{(3)} = f \\left(w\_{11}\^{(2)} a\_1\^{(2)}+w\_{12}\^{(2)}
a\_2\^{(2)}+w\_{13}\^{(2)}
a\_3\^{(2)}+b\_1\^{(2)}\\right)](http://s0.wp.com/latex.php?latex=a_1%5E%7B%283%29%7D+%3D+f+%5Cleft%28w_%7B11%7D%5E%7B%282%29%7D+a_1%5E%7B%282%29%7D%2Bw_%7B12%7D%5E%7B%282%29%7D+a_2%5E%7B%282%29%7D%2Bw_%7B13%7D%5E%7B%282%29%7D+a_3%5E%7B%282%29%7D%2Bb_1%5E%7B%282%29%7D%5Cright%29&bg=ffffff&fg=666666&s=1 "a_1^{(3)} = f \left(w_{11}^{(2)} a_1^{(2)}+w_{12}^{(2)} a_2^{(2)}+w_{13}^{(2)} a_3^{(2)}+b_1^{(2)}\right)")

Let’s consider what this means for a moment. Every node in the middle
layer is acting as an indicator function, returning
![0](http://s0.wp.com/latex.php?latex=0&bg=ffffff&fg=666666&s=-1 "0") or
![1](http://s0.wp.com/latex.php?latex=1&bg=ffffff&fg=666666&s=-1 "1")
depending on where the input lies in
![\\mathbb{R}\^2](http://s0.wp.com/latex.php?latex=%5Cmathbb%7BR%7D%5E2&bg=ffffff&fg=666666&s=-1 "\mathbb{R}^2").
We are then taking a weighted sum of these indicator functions and
feeding it into yet another nonlinearity. The possibilities may seem
endless, since we are not placing any restrictions on the weight
assignments. In reality characterizing the set of NNs (with the above
architecture) that exhibit distinct behaviors does require a little bit
of work–see
[Aside](http://moalquraishi.wordpress.com/2014/03/02/characterizing-the-space-of-distinct-neural-networks/ "Characterizing The Space of Distinct Neural Networks")–but
the point, as we shall see momentarily, is that we do not need to worry
about all such possibilities. One specific choice of assignments already
gives the key insight into the representational power of this type of
neural network. By setting all weights in the middle layer to
![1/3](http://s0.wp.com/latex.php?latex=1%2F3&bg=ffffff&fg=666666&s=-1 "1/3"),
and setting the bias of the middle layer
![(b\_1\^{(2)})](http://s0.wp.com/latex.php?latex=%28b_1%5E%7B%282%29%7D%29&bg=ffffff&fg=666666&s=-1 "(b_1^{(2)})")
to
![-1](http://s0.wp.com/latex.php?latex=-1&bg=ffffff&fg=666666&s=-1 "-1"),
the activation function of the output neuron
![(a\_1\^{(3)})](http://s0.wp.com/latex.php?latex=%28a_1%5E%7B%283%29%7D%29&bg=ffffff&fg=666666&s=-1 "(a_1^{(3)})")
will output
![1](http://s0.wp.com/latex.php?latex=1&bg=ffffff&fg=666666&s=-1 "1")
whenever the input lies in the intersection of all three
[half-spaces](http://en.wikipedia.org/wiki/Half-space_(geometry))
defined by the decision boundaries, and
![0](http://s0.wp.com/latex.php?latex=0&bg=ffffff&fg=666666&s=-1 "0")
otherwise. Since there was nothing special about our choice of decision
boundaries, we are able to carve out any arbitrary polygon and have the
NN fire precisely when the input is inside the polygon (in the general
case we set the weights to
![1/k](http://s0.wp.com/latex.php?latex=1%2Fk&bg=ffffff&fg=666666&s=-1 "1/k"),
where
![k](http://s0.wp.com/latex.php?latex=k&bg=ffffff&fg=666666&s=-1 "k") is
the number of hyperplanes defining the polygon).

![Figure
8](http://moalquraishi.files.wordpress.com/2014/05/figure-8.png?w=356&h=357)

This fact demonstrates both the power and limitation of this type of NN
architecture. On the one hand, it is capable of carving out decision
boundaries comprised of arbitrary polygons (or more generally
[polytopes](http://en.wikipedia.org/wiki/Polytope)). Creating regions
comprised of multiple polygons, even disjoint ones, can be achieved by
adding a set of neurons for each polygon and setting the weights of
their respective edges to
![1/k\_i](http://s0.wp.com/latex.php?latex=1%2Fk_i&bg=ffffff&fg=666666&s=-1 "1/k_i"),
where
![k\_i](http://s0.wp.com/latex.php?latex=k_i&bg=ffffff&fg=666666&s=-1 "k_i")
is the number of hyperplanes defining the
![ith](http://s0.wp.com/latex.php?latex=ith&bg=ffffff&fg=666666&s=-1 "ith")
polygon. This explains why, from an expressiveness standpoint, we don’t
need to worry about all possible weight combinations, because defining a
binary classifier over unions of polygons is *all we can do*. Any
combination of weights that we assign to the middle layer in the above
NN will result in a discrete set of values, up to one unique value per
region formed by the union or intersection of the half-spaces defined by
the decision boundaries, that are inputted to the
![a\_1\^{(3)}](http://s0.wp.com/latex.php?latex=a_1%5E%7B%283%29%7D&bg=ffffff&fg=666666&s=-1 "a_1^{(3)}")
node. Since the bias
![b\_1\^{(2)}](http://s0.wp.com/latex.php?latex=b_1%5E%7B%282%29%7D&bg=ffffff&fg=666666&s=-1 "b_1^{(2)}")
can only adjust the threshold at which
![a\_1\^{(3)}](http://s0.wp.com/latex.php?latex=a_1%5E%7B%283%29%7D&bg=ffffff&fg=666666&s=-1 "a_1^{(3)}")
will fire, then the resulting behavior of any weight assignment is
activation over some union of polygons defined by the shaded regions.
Thus our restricted treatment, where we only consider weights equal to
![1/k](http://s0.wp.com/latex.php?latex=1%2Fk&bg=ffffff&fg=666666&s=-1 "1/k"),
already captures the representational power of this NN architecture.

Several caveats merit mention. First, the above says nothing about
representational efficiency, only power. A more thoughtful choice of
weights, presumably identified by training the NN using backpropagation,
can provide a more compact representation comprised of a smaller set of
nodes and edges. Second, I oversimplified the discussion by focusing
only on polygons. In reality, any intersection of half-spaces is
possible, even ones that do not result in bounded regions. Third, and
most seriously, feedforward NNs are not restricted to step functions for
their activation functions. In particular modern NNs that utilize
Rectified Linear Units (ReLUs) most likely exploit their linear regions.

Nonetheless, the above simplified discussion illustrates a limitation of
this type of NNs. While they are able to represent any boundary with
arbitrary accuracy, this would come at a significant cost, much like the
cost of polygonally rendering smoothly curved objects in computer
graphics. In principle NNs with sigmoidal activation functions are
universal approximators, meaning they can approximate any continuous
function with arbitrary accuracy. In practice I suspect that real NNs
with a limited number of neurons behave more like my simplified toy
models, carving out sharp regions in high-dimensional space, but on a
much larger scale. Regardless NNs still provide far more expressive
power than most other machine learning techniques and my focus on
![\\mathbb{R}\^2](http://s0.wp.com/latex.php?latex=%5Cmathbb%7BR%7D%5E2&bg=ffffff&fg=666666&s=-1 "\mathbb{R}^2")
disguises the fact that even simple decision boundaries, operating in
high-dimensional spaces, can be surprisingly powerful.

Before I wrap up, let me highlight one other aspect of NNs that this
“union of polygons” perspective helps make clear. It has [long been
known](http://neuron.eng.wayne.edu/tarek/MITbook/chap2/2_3.html) that an
NN with a single hidden layer, i.e. the three-layer architecture
discussed here, is equal in representational power to a neural network
with arbitrary depth, as long as the hidden layer is made sufficiently
wide. Why this is so is obvious in the simplified setting described
here, because unions of sets of unions of polygons can be flattened out
in terms of unions of the underlying polygons. For example, consider the
set of polygons formed by the following 10 boundaries:

![Figure
9](http://moalquraishi.files.wordpress.com/2014/05/figure-9.png?w=367&h=368)

We would like to create 8 neurons that correspond to the 8 possible
activation patterns formed by the polygons (i.e. fire when input is in
none of them (1 case), one of them (3 cases), two of them (3 cases), or
any of them (1 case)). In the “deep” case, we can set up a four-layer NN
such that the second layer defines the edges, the third layer defines
the polygons, and the fourth layer contains the 8 possible activation
patterns:

![Figure
10](http://moalquraishi.files.wordpress.com/2014/05/figure-10.png?w=800&h=258)

The third layer composes the second layer, by creating neurons that are
specific to each closed region. However, we can just as well collapse
this into the following three-layer architecture, where each neuron in
the third layer “rediscovers” the polygons and how they must be combined
to yield a specific activation pattern:

![Figure
11](http://moalquraishi.files.wordpress.com/2014/05/figure-11.png?w=800&h=186)

Deeper architectures allow deeper compositions, where more complex
polygons are made up of simpler ones, but in principle all this
complexity can be collapsed onto one (hidden) layer. There is a
difference in representational efficiency however, and the two
architectures above illustrate this important point. While the
three-layer approach is just as expressive as the four-layer one, it is
not as efficient: the three-layer NN has a 2-10-8 configuration,
resulting in 100 parameters (20 edges connecting first to second layer
plus 80 edges connecting second to third layer), while the four-layer
NN, with a 2-10-3-8 configuration, only has 74 parameters. Herein lies
the promise of deeper architectures, by enabling the inference of
complex models using a relatively small number of parameters. In
particular, lower-level features such as the polygons above can be
learned once and then reused by higher layers of the network.

That’s it for now. I hope this discussion provided some insight into the
workings of neural networks. If you’d like to read more, see the
[Aside](http://moalquraishi.wordpress.com/2014/03/02/characterizing-the-space-of-distinct-neural-networks/ "Characterizing The Space of Distinct Neural Networks"),
and I also recommend this [blog
entry](http://colah.github.io/posts/2014-03-NN-Manifolds-Topology/) by
Christopher Olah which takes a topological view of neural networks.

**Update:** HN discussion
[here](https://news.ycombinator.com/item?id=7797155).

### Share this:

-   [Google](http://moalquraishi.wordpress.com/2014/05/25/what-does-a-neural-network-actually-do/?share=google-plus-1 "Click to share on Google+")
-   [Facebook](http://moalquraishi.wordpress.com/2014/05/25/what-does-a-neural-network-actually-do/?share=facebook "Share on Facebook")
-   [Twitter](http://moalquraishi.wordpress.com/2014/05/25/what-does-a-neural-network-actually-do/?share=twitter "Click to share on Twitter")
-   [Email](http://moalquraishi.wordpress.com/2014/05/25/what-does-a-neural-network-actually-do/?share=email "Click to email this to a friend")
-   

### Google+

[![Mohammed
AlQuraishi](https://lh3.googleusercontent.com/-jzKaOV-ZmLk/AAAAAAAAAAI/AAAAAAAAAkE/sFmhC6gYLcg/photo.jpg?sz=40)](https://plus.google.com/114812870956863006734)[Mohammed
AlQuraishi](https://plus.google.com/114812870956863006734)

### Like this:

Like Loading...

### *Related*

This entry was posted in [AI, ML, and
Data](http://moalquraishi.wordpress.com/category/ai-ml-and-data/),
[Mathematics](http://moalquraishi.wordpress.com/category/mathematics/),
[Science](http://moalquraishi.wordpress.com/category/science/) and
tagged [deep
learning](http://moalquraishi.wordpress.com/tag/deep-learning/),
[machine
learning](http://moalquraishi.wordpress.com/tag/machine-learning/),
[neural
networks](http://moalquraishi.wordpress.com/tag/neural-networks/).
Bookmark the
[permalink](http://moalquraishi.wordpress.com/2014/05/25/what-does-a-neural-network-actually-do/ "Permalink to What Does a Neural Network Actually Do?").

Post navigation
===============

[← Characterizing The Space of Distinct Neural
Networks](http://moalquraishi.wordpress.com/2014/05/25/characterizing-the-space-of-distinct-neural-networks/)

* * * * *

12 comments
-----------

1.  Pingback: [What Does a Neural Network Actually Do? | Some Thoughts
    on a Mysterious Universe | Kyu's
    Blog](http://komputervision.wordpress.com/2014/05/25/what-does-a-neural-network-actually-do-some-thoughts-on-a-mysterious-universe/)

2.  ![image](http://0.gravatar.com/avatar/96e32880410a25f5c322e8d7071d3d7f?s=50&d=identicon&r=G)
    [eksith](http://eksith.wordpress.com) says:|

    [May 25, 2014 at 8:28
    pm](http://moalquraishi.wordpress.com/2014/05/25/what-does-a-neural-network-actually-do/comment-page-1/#comment-1104)

    Reblogged this on [This page intentionally left
    ugly](http://eksith.wordpress.com/2014/05/25/what-does-a-neural-network-actually-do/).

    [Reply](/2014/05/25/what-does-a-neural-network-actually-do/?replytocom=1104#respond)

3.  ![image](http://0.gravatar.com/avatar/6af3dcb626252af737249d014d581b24?s=50&d=identicon&r=G)
    [kyuhyoung](http://komputervision.wordpress.com) says:|

    [May 25, 2014 at 9:20
    pm](http://moalquraishi.wordpress.com/2014/05/25/what-does-a-neural-network-actually-do/comment-page-1/#comment-1105)

    Reblogged this on [Kyu's
    Blog](http://komputervision.wordpress.com/2014/05/26/what-does-a-neural-network-actually-do/)
    and commented:\
     Great article of new insight

    [Reply](/2014/05/25/what-does-a-neural-network-actually-do/?replytocom=1105#respond)

4.  Pingback: [Bookmarks for May 25th | Chris's Digital
    Detritus](http://chris.cothrun.com/2014/05/25/bookmarks-for-may-25th-3/)

5.  ![image](http://0.gravatar.com/avatar/faa7a15b0c213f2e26c8f17df4f67821?s=50&d=identicon&r=G)
    [Anand Jeyahar](http://anandjeyahar.wordpress.com/) says:|

    [May 26, 2014 at 5:04
    am](http://moalquraishi.wordpress.com/2014/05/25/what-does-a-neural-network-actually-do/comment-page-1/#comment-1110)

    Reblogged this on [Just another complex
    system](http://anandjeyahar.wordpress.com/2014/05/26/what-does-a-neural-network-actually-do/).

    [Reply](/2014/05/25/what-does-a-neural-network-actually-do/?replytocom=1110#respond)

6.  Pingback: [What Does a Neural Network Actually Do? « Some Thoughts
    on a Mysterious Universe | Liudas
    Notes](http://note.liudas.eu/v1/2014/05/what-does-a-neural-network-actually-do-some-thoughts-on-a-mysterious-universe/)

7.  ![image](http://1.gravatar.com/avatar/7daf6c79d4802916d83f6266e24850af?s=50&d=identicon&r=G)
    Senseman says:|

    [May 26, 2014 at 3:32
    pm](http://moalquraishi.wordpress.com/2014/05/25/what-does-a-neural-network-actually-do/comment-page-1/#comment-1121)

    [http://www.dkriesel.com/en/science/neural\_networks](http://www.dkriesel.com/en/science/neural_networks)
    Here is about 200 pages of free ebook about neural networks
    containing lots of illustrations, in case some guys of you want to
    read further. There’s also a nice java framework to try out for the
    coders (try to learn to get some numbers out of images :-) ).

    [Reply](/2014/05/25/what-does-a-neural-network-actually-do/?replytocom=1121#respond)

8.  ![image](http://2.gravatar.com/avatar/23fc5c91fb0ea8ae8b9f93b838eb08e0?s=50&d=identicon&r=G)
    [eddy85br](http://www.hexabio.com.br) says:|

    [May 27, 2014 at 12:13
    am](http://moalquraishi.wordpress.com/2014/05/25/what-does-a-neural-network-actually-do/comment-page-1/#comment-1123)

    Reblogged this on [Eduardo Lemons
    Francisco](http://eduardolf.wordpress.com/2014/05/27/what-does-a-neural-network-actually-do/)
    and commented:\
     Excelente e didática explicação sobre o que são as Redes Neurais
    Artificiais (ANNs), como funcionam e qual seu real poder de
    aplicação!

    [Reply](/2014/05/25/what-does-a-neural-network-actually-do/?replytocom=1123#respond)

9.  ![image](http://0.gravatar.com/avatar/?s=50&d=identicon&r=G)
    Anonymous says:|

    [May 27, 2014 at 12:14
    am](http://moalquraishi.wordpress.com/2014/05/25/what-does-a-neural-network-actually-do/comment-page-1/#comment-1124)

    Thanks for the great overview. Your illustrations really decompose
    what neural networks are actually doing.

    [Reply](/2014/05/25/what-does-a-neural-network-actually-do/?replytocom=1124#respond)

10. ![image](http://1.gravatar.com/avatar/1f82d4db50d683b401811403704e0719?s=50&d=identicon&r=G)
    changhaz says:|

    [May 27, 2014 at 2:10
    am](http://moalquraishi.wordpress.com/2014/05/25/what-does-a-neural-network-actually-do/comment-page-1/#comment-1125)

    Reblogged this on [Changhaz's
    Codeplay](http://changhaz.wordpress.com/2014/05/26/what-does-a-neural-network-actually-do/).

    [Reply](/2014/05/25/what-does-a-neural-network-actually-do/?replytocom=1125#respond)

11. ![image](http://0.gravatar.com/avatar/cced447c5c184168da91abc5e1f27ed6?s=50&d=identicon&r=G)
    [Craig Henderson](http://cctvcv.wordpress.com) says:|

    [May 27, 2014 at 7:42
    am](http://moalquraishi.wordpress.com/2014/05/25/what-does-a-neural-network-actually-do/comment-page-1/#comment-1131)

    Reblogged this on [CCTV Computer Vision
    Blog](http://cctvcv.wordpress.com/2014/05/27/what-does-a-neural-network-actually-do/).

    [Reply](/2014/05/25/what-does-a-neural-network-actually-do/?replytocom=1131#respond)

12. ![image](http://0.gravatar.com/avatar/?s=50&d=identicon&r=G)
    Anonymous says:|

    [May 27, 2014 at 8:19
    am](http://moalquraishi.wordpress.com/2014/05/25/what-does-a-neural-network-actually-do/comment-page-1/#comment-1132)

    Could you write similar post about recurrent nets?

    [Reply](/2014/05/25/what-does-a-neural-network-actually-do/?replytocom=1132#respond)

* * * * *

### Leave a Reply [Cancel reply](/2014/05/25/what-does-a-neural-network-actually-do/#respond)

Enter your comment here...

Fill in your details below or click an icon to log in:

-   [](#comment-form-guest "Guest")
-   [](#comment-form-load-service:WordPress.com "WordPress.com")
-   [](#comment-form-load-service:Twitter "Twitter")
-   [](#comment-form-load-service:Facebook "Facebook")
-   

[![Gravatar](http://1.gravatar.com/avatar/ad516503a11cd5ca435acc9bb6523536?s=25&d=identicon&forcedefault=y&r=G)](https://gravatar.com/site/signup/)

Email (Address never made public)

Name

Website

![WordPress.com
Logo](http://s2.wp.com/wp-content/mu-plugins/highlander-comments/images/wplogo.png?m=1391188133g)

**** You are commenting using your WordPress.com account. ( [Log
Out](javascript:HighlanderComments.doExternalLogout(%20'wordpress'%20);)
/ [Change](#) )

![Twitter
picture](http://1.gravatar.com/avatar/ad516503a11cd5ca435acc9bb6523536?s=25&d=identicon&forcedefault=y&r=G)

**** You are commenting using your Twitter account. ( [Log
Out](javascript:HighlanderComments.doExternalLogout(%20'twitter'%20);) /
[Change](#) )

![Facebook
photo](http://1.gravatar.com/avatar/ad516503a11cd5ca435acc9bb6523536?s=25&d=identicon&forcedefault=y&r=G)

**** You are commenting using your Facebook account. ( [Log
Out](javascript:HighlanderComments.doExternalLogout(%20'facebook'%20);)
/ [Change](#) )

![Google+
photo](http://1.gravatar.com/avatar/ad516503a11cd5ca435acc9bb6523536?s=25&d=identicon&forcedefault=y&r=G)

**** You are commenting using your Google+ account. ( [Log
Out](javascript:HighlanderComments.doExternalLogout(%20'googleplus'%20);)
/ [Change](#) )

[Cancel](javascript:HighlanderComments.cancelExternalWindow();)

Connecting to %s

Notify me of follow-up comments via email.

Notify me of new posts via email.

Recent Posts
============

-   [What Does a Neural Network Actually
    Do?](http://moalquraishi.wordpress.com/2014/05/25/what-does-a-neural-network-actually-do/)
-   [Characterizing The Space of Distinct Neural
    Networks](http://moalquraishi.wordpress.com/2014/05/25/characterizing-the-space-of-distinct-neural-networks/)
-   [Question: Would You
    Exit?](http://moalquraishi.wordpress.com/2014/02/09/question-would-you-exit/)
-   [Utopia](http://moalquraishi.wordpress.com/2013/12/29/utopia-2/)
-   [A Year in New England: Boston vs. The
    Valley](http://moalquraishi.wordpress.com/2013/10/27/a-year-in-new-england-boston-vs-the-valley/)

Search

[![RSS
Feed](http://moalquraishi.wordpress.com/i/rss/red-small.png?m=1391188133g)](http://moalquraishi.wordpress.com/feed/ "Subscribe to Posts")
[RSS -
Posts](http://moalquraishi.wordpress.com/feed/ "Subscribe to Posts")

Archives
========

-   [May 2014](http://moalquraishi.wordpress.com/2014/05/)
-   [February 2014](http://moalquraishi.wordpress.com/2014/02/)
-   [December 2013](http://moalquraishi.wordpress.com/2013/12/)
-   [October 2013](http://moalquraishi.wordpress.com/2013/10/)
-   [September 2013](http://moalquraishi.wordpress.com/2013/09/)
-   [July 2013](http://moalquraishi.wordpress.com/2013/07/)
-   [May 2013](http://moalquraishi.wordpress.com/2013/05/)
-   [April 2013](http://moalquraishi.wordpress.com/2013/04/)
-   [March 2013](http://moalquraishi.wordpress.com/2013/03/)
-   [February 2013](http://moalquraishi.wordpress.com/2013/02/)
-   [January 2013](http://moalquraishi.wordpress.com/2013/01/)
-   [December 2012](http://moalquraishi.wordpress.com/2012/12/)

Categories
==========

-   [AI, ML, and
    Data](http://moalquraishi.wordpress.com/category/ai-ml-and-data/)
-   [Biology](http://moalquraishi.wordpress.com/category/biology/)
-   [Books](http://moalquraishi.wordpress.com/category/books/)
-   [Conferences](http://moalquraishi.wordpress.com/category/conferences/)
-   [History](http://moalquraishi.wordpress.com/category/history/)
-   [Life, Society, and
    Culture](http://moalquraishi.wordpress.com/category/life-society-and-culture/)
-   [Mathematics](http://moalquraishi.wordpress.com/category/mathematics/)
-   [Politics](http://moalquraishi.wordpress.com/category/politics/)
-   [Science](http://moalquraishi.wordpress.com/category/science/)

[Adam
Lanza](http://moalquraishi.wordpress.com/tag/adam-lanza/ "1 topic")
[American
Revolution](http://moalquraishi.wordpress.com/tag/american-revolution/ "1 topic")
[Andrew Ng](http://moalquraishi.wordpress.com/tag/andrew-ng/ "1 topic")
[Auren
Hoffman](http://moalquraishi.wordpress.com/tag/auren-hoffman/ "1 topic")
[Bay Area](http://moalquraishi.wordpress.com/tag/bay-area/ "1 topic")
[Berlin](http://moalquraishi.wordpress.com/tag/berlin/ "1 topic")
[Boston](http://moalquraishi.wordpress.com/tag/boston/ "1 topic")
[CASP](http://moalquraishi.wordpress.com/tag/casp/ "1 topic")
[CASP10](http://moalquraishi.wordpress.com/tag/casp10/ "1 topic")
[China](http://moalquraishi.wordpress.com/tag/china/ "2 topics")
[computational
biology](http://moalquraishi.wordpress.com/tag/computational-biology/ "4 topics")
[consciousness](http://moalquraishi.wordpress.com/tag/consciousness/ "3 topics")
[deep
learning](http://moalquraishi.wordpress.com/tag/deep-learning/ "2 topics")
[efficiency](http://moalquraishi.wordpress.com/tag/efficiency/ "1 topic")
[evolution](http://moalquraishi.wordpress.com/tag/evolution/ "2 topics")
[future](http://moalquraishi.wordpress.com/tag/future/ "2 topics")
[genomics](http://moalquraishi.wordpress.com/tag/genomics/ "1 topic")
[Geoff
Hinton](http://moalquraishi.wordpress.com/tag/geoff-hinton/ "1 topic")
[Germany](http://moalquraishi.wordpress.com/tag/germany/ "2 topics")
[Giulio
Tononi](http://moalquraishi.wordpress.com/tag/giulio-tononi/ "1 topic")
[Google](http://moalquraishi.wordpress.com/tag/google/ "1 topic")
[Hermann
Hesse](http://moalquraishi.wordpress.com/tag/hermann-hesse/ "1 topic")
[ICSB](http://moalquraishi.wordpress.com/tag/icsb/ "1 topic")
[ICSB2013](http://moalquraishi.wordpress.com/tag/icsb2013/ "1 topic")
[machine
learning](http://moalquraishi.wordpress.com/tag/machine-learning/ "2 topics")
[Malcolm
Gladwell](http://moalquraishi.wordpress.com/tag/malcolm-gladwell/ "1 topic")
[Matt
Welsh](http://moalquraishi.wordpress.com/tag/matt-welsh/ "1 topic")
[Michael
Deem](http://moalquraishi.wordpress.com/tag/michael-deem/ "1 topic")
[modularity](http://moalquraishi.wordpress.com/tag/modularity/ "1 topic")
[multiculturalism](http://moalquraishi.wordpress.com/tag/multiculturalism/ "2 topics")
[neural
networks](http://moalquraishi.wordpress.com/tag/neural-networks/ "2 topics")
[neuroscience](http://moalquraishi.wordpress.com/tag/neuroscience-2/ "1 topic")
[NIPS](http://moalquraishi.wordpress.com/tag/nips/ "1 topic")
[NIPS2012](http://moalquraishi.wordpress.com/tag/nips2012/ "1 topic")
[origin of
life](http://moalquraishi.wordpress.com/tag/origin-of-life/ "1 topic")
[Outliers](http://moalquraishi.wordpress.com/tag/outliers/ "1 topic")
[philosophy](http://moalquraishi.wordpress.com/tag/philosophy/ "3 topics")
[quantified
self](http://moalquraishi.wordpress.com/tag/quantified-self/ "1 topic")
[religion](http://moalquraishi.wordpress.com/tag/religion/ "3 topics")
[Sandy
Hook](http://moalquraishi.wordpress.com/tag/sandy-hook/ "1 topic") [San
Francisco](http://moalquraishi.wordpress.com/tag/san-francisco/ "1 topic")
[Sebastian
Thrun](http://moalquraishi.wordpress.com/tag/sebastian-thrun/ "1 topic")
[Silicon
Valley](http://moalquraishi.wordpress.com/tag/silicon-valley/ "1 topic")
[structural
biology](http://moalquraishi.wordpress.com/tag/structural-biology/ "1 topic")
[systems
biology](http://moalquraishi.wordpress.com/tag/systems-biology/ "2 topics")
[theoretical
biology](http://moalquraishi.wordpress.com/tag/theoretical-biology/ "1 topic")
[Thomas
Jefferson](http://moalquraishi.wordpress.com/tag/thomas-jefferson/ "1 topic")
[tracking](http://moalquraishi.wordpress.com/tag/tracking/ "1 topic")
[United
States](http://moalquraishi.wordpress.com/tag/united-states/ "8 topics")
[urbanism](http://moalquraishi.wordpress.com/tag/urbanism/ "3 topics")
[Wolfram
Alpha](http://moalquraishi.wordpress.com/tag/wolfram-alpha/ "1 topic")

[Blog at WordPress.com](http://wordpress.com/?ref=footer_blog). |
[Customized Widely
Theme](http://theme.wordpress.com/credits/moalquraishi.wordpress.com/ "Learn about customizing this theme with the Custom Design upgrade").

[Follow](javascript:void(0))

### Follow “Some Thoughts on a Mysterious Universe”

Get every new post delivered to your Inbox.

Join 234 other followers

[Powered by WordPress.com](http://wordpress.com/signup/?ref=lof)

Send to Email Address Your Name Your Email Address

![loading](http://s2.wp.com/wp-content/mu-plugins/post-flair/sharing/images/loading.gif?m=1315610318g)
[Cancel](#cancel)

Post was not sent - check your email addresses!

Email check failed, please try again

Sorry, your blog cannot share posts by email.

%d bloggers like this:

![image](http://stats.wordpress.com/b.gif?v=noscript)

This markdown document has been converted from the html document located at:
http://moalquraishi.wordpress.com/2014/05/25/what-does-a-neural-network-actually-do/
