[Normal Deviate](http://normaldeviate.wordpress.com/ "Normal Deviate")
======================================================================

Thoughts on Statistics and Machine Learning

[Skip to content](#content "Skip to content")

-   [About](http://normaldeviate.wordpress.com/about/)

[« LOST CAUSES IN STATISTICS I: Finite
Additivity](http://normaldeviate.wordpress.com/2013/06/30/lost-causes-in-statistics-i-finite-additivity/)

[THE FIVE: Jeff Leek’s Challenge
»](http://normaldeviate.wordpress.com/2013/07/23/the-five-jeff-leeks-challenge/)

LOST CAUSES IN STATISTICS II: Noninformative Priors
---------------------------------------------------

LOST CAUSES IN STATISTICS II: Noninformative Priors

I thought I would post at a higher frequency in the summer. But I have
been working hard to finish some papers which has kept me quite busy.
So, apologies for the paucity of posts.

Today I’ll discuss another lost cause: noninformative priors.

I like to say that noninformative priors are the [perpetual
motion](http://en.wikipedia.org/wiki/Perpetual_motion) machines of
statistics. Everyone wants one but they don’t exist.

By definition, a prior represents information. So it should come as no
surprise that a prior cannot represent lack of information.

The first “noninformative prior” was of course the flat prior. The major
flaw with this prior is lack of invariance: if it is flat in one
parameterization it will not be flat in most other parameterizations.
Flat prior have lots of other problems too. See my earlier post
[here](http://normaldeviate.wordpress.com/2012/12/08/flat-priors-in-flatland-stones-paradox/).

The most famous noninformative prior (I’ll stop putting quotes around
the phrase from now on) is [Jeffreys
prior](http://en.wikipedia.org/wiki/Jeffreys_prior) which is
proportional to the square root of the determinant of the Fisher
information matrix. While this prior is invariant, it can still have
undesirable properties. In particular, while it may seem noninformative
for a parameter
![{\\theta}](http://s0.wp.com/latex.php?latex=%7B%5Ctheta%7D&bg=ffffff&fg=000000&s=0 "{\theta}")
it can end up being highly informative for functions of
![{\\theta}](http://s0.wp.com/latex.php?latex=%7B%5Ctheta%7D&bg=ffffff&fg=000000&s=0 "{\theta}").
For example, suppose that
![{Y}](http://s0.wp.com/latex.php?latex=%7BY%7D&bg=ffffff&fg=000000&s=0 "{Y}")
is multivariate Normal with mean vector
![{\\theta}](http://s0.wp.com/latex.php?latex=%7B%5Ctheta%7D&bg=ffffff&fg=000000&s=0 "{\theta}")
and identity covariance. The Jeffreys prior is the flat prior
![{\\pi(\\theta) \\propto
1}](http://s0.wp.com/latex.php?latex=%7B%5Cpi%28%5Ctheta%29+%5Cpropto+1%7D&bg=ffffff&fg=000000&s=0 "{\pi(\theta) \propto 1}").
Now suppose that we want to infer ![{\\psi = \\sum\_j
\\theta\_j\^2}](http://s0.wp.com/latex.php?latex=%7B%5Cpsi+%3D+%5Csum_j+%5Ctheta_j%5E2%7D&bg=ffffff&fg=000000&s=0 "{\psi = \sum_j \theta_j^2}").
The resulting posterior for
![{\\psi}](http://s0.wp.com/latex.php?latex=%7B%5Cpsi%7D&bg=ffffff&fg=000000&s=0 "{\psi}")
is a disaster. The coverage of the Bayesian
![{1-\\alpha}](http://s0.wp.com/latex.php?latex=%7B1-%5Calpha%7D&bg=ffffff&fg=000000&s=0 "{1-\alpha}")
posterior interval can be close to 0.

This is a general problem with noninformative priors. If
![{\\pi(\\theta)}](http://s0.wp.com/latex.php?latex=%7B%5Cpi%28%5Ctheta%29%7D&bg=ffffff&fg=000000&s=0 "{\pi(\theta)}")
is somehow noninformative for
![{\\theta}](http://s0.wp.com/latex.php?latex=%7B%5Ctheta%7D&bg=ffffff&fg=000000&s=0 "{\theta}"),
it may still be highly informative for sub-parameters, that is for
functions ![{\\psi =
g(\\theta)}](http://s0.wp.com/latex.php?latex=%7B%5Cpsi+%3D+g%28%5Ctheta%29%7D&bg=ffffff&fg=000000&s=0 "{\psi = g(\theta)}")
where ![{\\theta\\in
\\mathbb{R}\^d}](http://s0.wp.com/latex.php?latex=%7B%5Ctheta%5Cin+%5Cmathbb%7BR%7D%5Ed%7D&bg=ffffff&fg=000000&s=0 "{\theta\in \mathbb{R}^d}")
and ![{\\psi: \\mathbb{R}\^d \\rightarrow
\\mathbb{R}}](http://s0.wp.com/latex.php?latex=%7B%5Cpsi%3A+%5Cmathbb%7BR%7D%5Ed+%5Crightarrow+%5Cmathbb%7BR%7D%7D&bg=ffffff&fg=000000&s=0 "{\psi: \mathbb{R}^d \rightarrow \mathbb{R}}").

Jim Berger and Jose Bernardo wrote a series of interesting papers about
priors that were targeted to be noninformative for particular functions
of
![{\\theta}](http://s0.wp.com/latex.php?latex=%7B%5Ctheta%7D&bg=ffffff&fg=000000&s=0 "{\theta}").
These are often called reference priors. But what if you are interested
in many functions of
![{\\theta}](http://s0.wp.com/latex.php?latex=%7B%5Ctheta%7D&bg=ffffff&fg=000000&s=0 "{\theta}").
Should you use a different prior for each function of interest?

A more fundamental question is: what does it mean for a prior to be
noninformative? Of course, people have argued about this for many, many
years. One definition, which has the virtue of being somewhat precise,
is that a prior is noninformative if the
![{1-\\alpha}](http://s0.wp.com/latex.php?latex=%7B1-%5Calpha%7D&bg=ffffff&fg=000000&s=0 "{1-\alpha}")
posterior regions have frequentist coverage equal (approximately) to
![{1-\\alpha}](http://s0.wp.com/latex.php?latex=%7B1-%5Calpha%7D&bg=ffffff&fg=000000&s=0 "{1-\alpha}").
These are sometimes called “matching priors.”

In general, it is hard to construct matching priors especially in
high-dimensional complex models. But matching priors raise a fundamental
question: if your goal is to match frequentist coverage, why bother with
Bayes at all? Just use a frequentist confidence interval.

These days I think that most people agree that the virtue of Bayesian
methods is that it gives you a way to include prior information in a
systematic way. There is no reason to formulate a “noninformative
prior.”

On the other hand, in practice, we often deal with very complex,
high-dimensional models. Can we really formulate a meaningful
informative prior in such problems? And if we do, will anyone care about
our inferences?

In 1996, I wrote a review paper with Rob Kass on noninformative priors
(Kass and Wasserman 1996). We emphasized that a better term might be
“default prior” since that seems more honest and promises less. One of
our conclusions was:

*“We conclude that the problems raised by the research on priors chosen
by formal rules are serious and may not be dismissed lightly: When
sample sizes are small (relative the number of parameters being
estimated), it is dangerous to put faith in any default solution; but
when asymptotics take over, Jeffreys’s rules and their variants remain
reasonable choices.”*

Looking at this almost twenty years later, the one thing that has
changed is the “the number of parameters being estimated” which these
days is often very, very large.

My conclusion: noninformative priors are a lost cause.

**Reference**

Kass, Robert E and Wasserman, Larry. (1996). The selection of prior
distributions by formal rules. *Journal of the American Statistical
Association*, 91, 1343-1370.

[About these ads](http://en.wordpress.com/about-these-ads/)

### Share this:

-   [Twitter](http://normaldeviate.wordpress.com/2013/07/13/lost-causes-in-statistics-ii-noninformative-priors/?share=twitter "Click to share on Twitter")
-   [Facebook](http://normaldeviate.wordpress.com/2013/07/13/lost-causes-in-statistics-ii-noninformative-priors/?share=facebook "Share on Facebook")
-   [More](#)
-   

-   [Print](http://normaldeviate.wordpress.com/2013/07/13/lost-causes-in-statistics-ii-noninformative-priors/#print "Click to print")
-   [Email](http://normaldeviate.wordpress.com/2013/07/13/lost-causes-in-statistics-ii-noninformative-priors/?share=email "Click to email this to a friend")
-   -   

### Like this:

Like Loading...

### *Related*

This entry was written by
[normaldeviate](http://normaldeviate.wordpress.com/author/normaldeviate/ "View all posts by normaldeviate"),
posted on July 13, 2013 at 10:17 am, filed under
[Uncategorized](http://normaldeviate.wordpress.com/category/uncategorized/).
Bookmark the
[permalink](http://normaldeviate.wordpress.com/2013/07/13/lost-causes-in-statistics-ii-noninformative-priors/ "Permalink to LOST CAUSES IN STATISTICS II: Noninformative Priors").
Follow any comments here with the [RSS feed for this
post](http://normaldeviate.wordpress.com/2013/07/13/lost-causes-in-statistics-ii-noninformative-priors/feed/ "Comments RSS to LOST CAUSES IN STATISTICS II: Noninformative Priors").
Both comments and trackbacks are currently closed.

[« LOST CAUSES IN STATISTICS I: Finite
Additivity](http://normaldeviate.wordpress.com/2013/06/30/lost-causes-in-statistics-i-finite-additivity/)

[THE FIVE: Jeff Leek’s Challenge
»](http://normaldeviate.wordpress.com/2013/07/23/the-five-jeff-leeks-challenge/)

### 21 Comments

1.  ![image](http://2.gravatar.com/avatar/b95625e8e5f293760bbd49630f8a6c17?s=32&d=identicon&r=G)
    rj444

    Posted July 13, 2013 at 10:58 am |
    [Permalink](#comment-9440 "Permalink to this comment")

    Most people I’ve talked to think the virtue of Bayesian approaches
    is not the ability to systematically include prior information, but
    rather its ability to combine information efficiently.

    In most data analysis scenarios outside of a controlled experiment
    the distinction between “prior data” and “data” is a semi-arbitrary
    anyway. More and more data analysis is happening outside of
    controlled experiments (e.g. nate silver’s prediction models, high
    dimensional case/control designs in biology), which is why I suspect
    approaches such as empirical bayes and multilevel models are getting
    popular. The (hyper)priors mainly serve to “glue” the latent
    structure of a model.

2.  ![image](http://1.gravatar.com/avatar/1820a8334a71ac564c7f3a830fb58087?s=32&d=identicon&r=G)
    kenmccue

    Posted July 14, 2013 at 9:04 am |
    [Permalink](#comment-9449 "Permalink to this comment")

    Part of the motivation of the Shafer-Dempster theory was to avoid
    the problems with specifying (or being unable to acceptably specify)
    ignorance (or non-information). While this theory has been pretty
    much rejected by the statistical community, it has found numerous
    applications in the computer science/machine learning community. Any
    thoughts on whether this theory can resolve some of the issues you
    discuss (note that this theory can combine information efficiently,
    which, as rj444 notes, is an attraction of Bayesian analysis).

    -   ![image](http://0.gravatar.com/avatar/37312c618a28c7d016d4bbe4060f23b1?s=32&d=identicon&r=G)
        [normaldeviate](http://normaldeviate.wordpress.com)

        Posted July 14, 2013 at 9:15 am |
        [Permalink](#comment-9450 "Permalink to this comment")

        Interesting question.\
         I did my thesis on Dempster-Shafer theory 25 years ago.\
         I’ll post about this in the future.

3.  ![image](http://1.gravatar.com/avatar/d582b760aff3a7d7ce1197505fb57837?s=32&d=identicon&r=G)
    [Andrew Gelman](http://andrewgelman.com)

    Posted July 14, 2013 at 10:03 am |
    [Permalink](#comment-9451 "Permalink to this comment")

    Larry:

    I agree with you. In some specific cases, noninformative priors can
    improve our estimates (see here, for example), but in general I’ve
    found that it’s a good idea to include prior information. Even weak
    prior information can make a big difference (see
    [here](http://www.stat.columbia.edu/~gelman/research/published/priors11.pdf),
    for example).

    And, yes, we can formulate informative priors in high dimensions,
    for example by assigning priors to lower-dimensional projections
    that we understand. The key, I think, is to have the goal of the
    prior being informative without hoping that it will include *all*
    our prior information. Which is the way we typically think about
    statistical models in general.

    -   ![image](http://1.gravatar.com/avatar/d582b760aff3a7d7ce1197505fb57837?s=32&d=identicon&r=G)
        [Andrew Gelman](http://andrewgelman.com)

        Posted July 14, 2013 at 10:04 am |
        [Permalink](#comment-9452 "Permalink to this comment")

        P.S. I intended the first “see here” above to link to [this
        paper](http://www.stat.columbia.edu/~gelman/research/published/draft4_12.pdf).

    -   ![image](http://1.gravatar.com/avatar/a6f2d776a4e533d2d86682ba5b24131e?s=32&d=identicon&r=G)
        awblocker

        Posted July 14, 2013 at 2:41 pm |
        [Permalink](#comment-9455 "Permalink to this comment")

        One question on the second part of your comment, Andrew. In
        formulating priors in high-dimensional settings, you mention
        “assigning priors to lower-dimensional projections that we
        understand”, which seems like an eminently reasonable strategy.
        However, if we are assigning priors to such projections, it
        would seem that we are left with the problem of expressing
        “ignorance” with respect to the remaining dimensions. Do you see
        a resolution to this issue, or does the projection approach
        circle around to the classical problem of creating
        non-informative priors?

        -   ![image](http://1.gravatar.com/avatar/d582b760aff3a7d7ce1197505fb57837?s=32&d=identicon&r=G)
            [Andrew Gelman](http://andrewgelman.com)

            Posted July 14, 2013 at 3:28 pm |
            [Permalink](#comment-9456 "Permalink to this comment")

            Alex:

            I’d still like to be weakly informative on these other
            dimensions. We still have a ways to go, though, in
            developing intuition and experience with high-dimensional
            models such as splines and Gaussian processes.

4.  ![image](http://2.gravatar.com/avatar/8e7cfe4e5814e11357d1ab8d04ca71d3?s=32&d=identicon&r=G)
    [Sumedha Sengupta](http://www.facebook.com/sumedha.sengupta)

    Posted July 14, 2013 at 1:56 pm |
    [Permalink](#comment-9454 "Permalink to this comment")

    In my humble opinion, selection of a prior is very much tuned to the
    area and objective of the Bayesian Methods applications.\
     While the Non-Informative (NI) prior may be rejected as an useful
    prior in one application, it may be an indispensable tool in another
    one. In the area of ‘ Wavelet Modeling’ , where more than one
    parameters exist, in the wave decomposition, dependencies may exist
    between at least two of those parameters. In such cases, assigning a
    NI prior to act in such a way that it lets one to arrive at an
    analytically tractable and meaningful signal. Without setting the NI
    in that way, it may not be possible to arrive at that inference.\
     Not in a direct way, but in an indirect way, it proves to be a
    valuable alternative with more information than a classical\
     approach in such a situation.\
     Certain areas of applications have proven to be more user friendly
    to Bayesian methods showing good useful results.\
     In Wavelet Modeling, Bayesian methods have certainly been very
    useful.\
     Another area is the Bayesian Network.

    I found the following reference an excellent source of information
    on Wavelet modeling.

    1999, Peter Muller & Brani Vidakovic , Editors

    “Bayesian Inference in Wavelet-Models”\
     Lecture Notes in Statistics, 141\
     Springer.

    Thank you,

    Sumedha\
     An Outlier of the Stat Domain.

5.  ![image](http://0.gravatar.com/avatar/06a94e9ed03c0a7eb49807d552d4d28b?s=32&d=identicon&r=G)
    [pierpax](http://gravatar.com/pierpax)

    Posted July 15, 2013 at 9:19 am |
    [Permalink](#comment-9461 "Permalink to this comment")

    …talking about matching priors: in simple parametric cases, you must
    be Jeffreys (i.e. noninformative in a sense) to be matching (Welch &
    Peers, 1963…I guess).\
     In slightly more complicated situations (e.g. mixtures), to be
    matching you have to be data-dependent = informative, in another
    sense (…that’s you Larry, right?). Sooo, moving to even more
    complicated settings (nonparametrics), is there any room to be
    informative & matching? Probably Richard Nickl has something to say
    on this point. Then, assuming all this makes sense, in this process
    that moves from noninfo to info is there a phase transition w.r.t.
    the model complexity?

    -   ![image](http://0.gravatar.com/avatar/37312c618a28c7d016d4bbe4060f23b1?s=32&d=identicon&r=G)
        [normaldeviate](http://normaldeviate.wordpress.com)

        Posted July 15, 2013 at 9:33 am |
        [Permalink](#comment-9462 "Permalink to this comment")

        Indeed. I am skeptical about finding matching priors in complex
        models.

6.  ![image](http://0.gravatar.com/avatar/f3306bf0c4fc69a967151aa474beccbd?s=32&d=identicon&r=G)
    Keal G

    Posted July 15, 2013 at 11:20 am |
    [Permalink](#comment-9464 "Permalink to this comment")

    In my understanding, priors are part of the model specification.
    Therefore, the interesting question really is not uniformative
    priors (that do not exist) but \*implied\* priors. I believe that
    every sensible frequentist procedure has a Bayesian interpretation,
    or at least an interpretation as approximation to a Bayesian
    procedure. This is the reason why matching priors are so important –
    not to mimick frequentist procedures but to understand them
    properly!

    -   ![image](http://0.gravatar.com/avatar/37312c618a28c7d016d4bbe4060f23b1?s=32&d=identicon&r=G)
        [normaldeviate](http://normaldeviate.wordpress.com)

        Posted July 15, 2013 at 12:28 pm |
        [Permalink](#comment-9468 "Permalink to this comment")

        I don’t agree that every sensible frequentist procedure\
         has a Bayesian interpretation even approximately.\
         Examples are the permutation test and this previous post:

        [http://normaldeviate.wordpress.com/2012/08/28/robins-and-wasserman-respond-to-a-nobel-prize-winner/](http://normaldeviate.wordpress.com/2012/08/28/robins-and-wasserman-respond-to-a-nobel-prize-winner/)

7.  ![image](http://2.gravatar.com/avatar/5ef25b7d750cde285ba47b3ef579fa01?s=32&d=identicon&r=G)
    Konrad

    Posted July 16, 2013 at 5:43 pm |
    [Permalink](#comment-9472 "Permalink to this comment")

    Larry: to say that uninformative priors don’t exist or are a lost
    cause is to say that the uniform discrete distribution does not
    exist or is a lost cause. I doubt this is a claim you want to make?

    Let me elaborate: to counter your claim about uninformative priors,
    we only need a single counterexample so let’s talk about the
    simplest, and most well known one (not the Jeffreys prior, and not
    the continuous uniform distribution!), which arises in discrete
    problems. Whenever we formulate a Bayesian model for a coin- or
    dice-tossing problem (or one of their many real-world equivalents),
    part of the model is a prior on the set of discrete outcomes. In
    many, perhaps most, of these problems, we do not have any reason to
    bias the prior in a specific direction (no a priori reason to assign
    higher probability to some of the possible discrete observations
    than to others) – in these cases, the principle of indifference
    forces us to adopt the uninformative prior, which is just the
    uniform discrete distribution. It leads to correct and useful
    solutions to a great many real world problems.

    -   ![image](http://0.gravatar.com/avatar/37312c618a28c7d016d4bbe4060f23b1?s=32&d=identicon&r=G)
        [normaldeviate](http://normaldeviate.wordpress.com)

        Posted July 16, 2013 at 5:52 pm |
        [Permalink](#comment-9473 "Permalink to this comment")

        Useful, yes.\
         But in what sense it is noninformative?

        -   ![image](http://2.gravatar.com/avatar/5ef25b7d750cde285ba47b3ef579fa01?s=32&d=identicon&r=G)
            Konrad

            Posted July 22, 2013 at 1:48 pm |
            [Permalink](#comment-9510 "Permalink to this comment")

            In the sense that it is the prior forced on us by the
            principle of indifference.

    -   ![image](http://2.gravatar.com/avatar/2daed44593de3cbac1894e997b5503b4?s=32&d=identicon&r=G)
        Keith O'Rourke

        Posted July 17, 2013 at 9:37 am |
        [Permalink](#comment-9480 "Permalink to this comment")

        Konrad: It is not so simple here as being clever enough to
        extend probability to non-finite sets without creating
        distracting anomalies.

        Think of two proportions, say in the treated versus untreated
        condition, each discrete with uniform priors and look at the
        implied prior on say the odds ratio.

        Larry covered this generally and its sort of an elephant in room
        in any statistical approach and the second thing in statistics I
        try to explain to people.

8.  ![image](http://2.gravatar.com/avatar/8e7cfe4e5814e11357d1ab8d04ca71d3?s=32&d=identicon&r=G)
    [Sumedha Sengupta](http://www.facebook.com/sumedha.sengupta)

    Posted July 17, 2013 at 1:52 am |
    [Permalink](#comment-9477 "Permalink to this comment")

    I guess in the case of wavelet modeling example, the contribution of
    ” information” extracted from the weighted NI prior used for the
    interdependent parameters, was null. However, the total information
    was not.

    Although i agree, perhaps the name “noninformative” prior is not the
    appropriate one to be considered for all different applications
    under all different conditions.

9.  ![image](http://2.gravatar.com/avatar/5bc1fd1ae41aea4fb25c220a539f51e1?s=32&d=identicon&r=G)
    Secondtry

    Posted July 18, 2013 at 11:40 am |
    [Permalink](#comment-9492 "Permalink to this comment")

    Could you please clarify your point about “The resulting posterior
    for {\\psi} is a disaster.”?

    The posterior distribution of each {\\theta\_i} is normal with
    variance {1/n}. Then, if you obtain a sample from each {\\theta\_i}
    and then transform these samples into a sample of {\\psi}, the
    corresponding posterior should not be that terrible. Am I missing
    something? Which combination of the true parameters produce such
    disastrous posteriors?

    This paper might also be of interest:

    Hidden Dangers of Specifying Noninformative Priors.

    [http://www.tandfonline.com/doi/abs/10.1080/00031305.2012.695938\#.UegMB23BzeQ](http://www.tandfonline.com/doi/abs/10.1080/00031305.2012.695938#.UegMB23BzeQ)

    -   ![image](http://0.gravatar.com/avatar/37312c618a28c7d016d4bbe4060f23b1?s=32&d=identicon&r=G)
        [normaldeviate](http://normaldeviate.wordpress.com)

        Posted July 18, 2013 at 11:58 am |
        [Permalink](#comment-9493 "Permalink to this comment")

        The disaster is that the frequentist coverage\
         of the 95 percent bayesian interval is nearly 0\
         (in high dimensions)\
         See section 4.2.2 of my paper with Kass or\
         Stein (1959, Annals of Mathematical Statistics, 877)

        -   ![image](http://1.gravatar.com/avatar/1fa67a6bead3e2868a9d177e10657f3f?s=32&d=identicon&r=G)
            Secondtry

            Posted July 22, 2013 at 6:46 pm |
            [Permalink](#comment-9514 "Permalink to this comment")

            Do you any idea on the coverage of the profile likelihood
            intervals of this parameter of interest or the bootstrap
            confidence intervals?

10. ![image](http://2.gravatar.com/avatar/8e7cfe4e5814e11357d1ab8d04ca71d3?s=32&d=identicon&r=G)
    [Sumedha Sengupta](http://www.facebook.com/sumedha.sengupta)

    Posted July 22, 2013 at 2:57 pm |
    [Permalink](#comment-9511 "Permalink to this comment")

    Applied Statisticians are not that popular among the theoreticians.
    With that in mind, I am still making an analogy with a question and
    taking another senseless risk.\
     How do you evaluate the effect of a Placebo in a Clinical Trial ?\
     It gives one the effect of the Treatment and not the Placebo that
    has no effect but still is needed there.

### 9 Trackbacks

1.  By [Links for 07-14-2013 | Symposium
    Magazine](http://symposium-magazine.com/links-for-07-14-2013/) on
    July 14, 2013 at 3:07 am

    […] Lost Causes in Statistics:II: Noninformative Priors – Normal
    Deviate […]

2.  By [Bayesianism and noninformative priors – lost causes (wonkish) |
    LARS P
    SYLL](http://larspsyll.wordpress.com/2013/07/14/bayesianism-and-noninformative-priors-lost-causes-wonkish/)
    on July 14, 2013 at 6:00 am

    […] Larry Wasserman […]

3.  By [Links 7/17/13 | Mike the Mad
    Biologist](http://mikethemadbiologist.com/2013/07/17/links-71713/)
    on July 17, 2013 at 4:44 pm

    […] sponges invade Antarctica All About HD 189733b, That Other Blue
    Planet Less Research Is Needed LOST CAUSES IN STATISTICS II:
    Noninformative Priors Were Paleolithic Cave Painters High on
    Psychedelic Drugs? Scientists Propose Ingenious Theory for […]

4.  By [Friday links: Sears CEO vs. multi-level selection,
    noninformative priors = perpetual motion machines, scientific wills,
    a poetic paper, and more | Dynamic
    Ecology](http://dynamicecology.wordpress.com/2013/07/19/friday-links-11/)
    on July 19, 2013 at 7:55 am

    […] How is a noninformative prior like a perpetual motion machine?
    It would be nice if it existed, but it doesn’t. […]

5.  By [Entsophy](http://www.entsophy.net/blog/?p=95) on July 22, 2013
    at 12:02 am

    […] esteemed Dr. Wasserman claimed “This is a general problem with
    noninformative priors. If is somehow noninformative […]

6.  By [Lost causes in science | Dynamic
    Ecology](http://dynamicecology.wordpress.com/2013/10/03/lost-causes-in-science/)
    on October 3, 2013 at 7:21 am

    […] small number of people. For instance, in the second post in his
    series Larry Wasserman identifies noninformative priors as a lost
    cause. He compares them to perpetual motion machines: “[E]veryone
    wants one, but they don’t […]

7.  By [Entsophy](http://www.entsophy.net/blog/?p=195) on November 6,
    2013 at 9:01 am

    […] finish by ignoring the only data available and base everything
    on non-informative priors which prominent authorities assure us
    don’t even exist. Let the debauchery […]

8.  By [Friday links: Being Black, why science news doesn’t go viral,
    MOOC fail, and more | Dynamic
    Ecology](http://dynamicecology.wordpress.com/2013/11/22/friday-links-why-science-news-doesnt-go-viral-and-more/)
    on November 22, 2013 at 6:21 am

    […] of noninformative priors in Bayesian statistics. Reminds me of a
    remark of Larry Wasserman’s, comparing noninformative priors to
    perpetual motion machines: everyone wants one, but they don’t […]

9.  By [LOST CAUSES IN STATISTICS II: Noninformative Priors | Everything
    Is
    Bayesian](http://everythingisbayesian.wordpress.com/2014/03/15/lost-causes-in-statistics-ii-noninformative-priors/)
    on March 14, 2014 at 9:37 pm

    […] LOST CAUSES IN STATISTICS II: Noninformative Priors. […]

-   Search for:
-   ### Recent Posts

    -   [THE
        END](http://normaldeviate.wordpress.com/2013/12/16/the-end/)
    -   [Stein’s
        Method](http://normaldeviate.wordpress.com/2013/11/16/steins-method/)
    -   [Nonparametric Regression, ABC and
        CNN](http://normaldeviate.wordpress.com/2013/10/31/nonparametric-regression-abc-and-cnn/)
    -   [PubMed
        Commons](http://normaldeviate.wordpress.com/2013/10/23/pubmed-commons/)
    -   [The Terminator, Statistics, Fair Use and Hedge
        Funds](http://normaldeviate.wordpress.com/2013/10/15/the-terminator-statistics-fair-use-and-hedge-funds/)

-   ### Archives

    -   [December 2013](http://normaldeviate.wordpress.com/2013/12/)
    -   [November 2013](http://normaldeviate.wordpress.com/2013/11/)
    -   [October 2013](http://normaldeviate.wordpress.com/2013/10/)
    -   [September 2013](http://normaldeviate.wordpress.com/2013/09/)
    -   [August 2013](http://normaldeviate.wordpress.com/2013/08/)
    -   [July 2013](http://normaldeviate.wordpress.com/2013/07/)
    -   [June 2013](http://normaldeviate.wordpress.com/2013/06/)
    -   [May 2013](http://normaldeviate.wordpress.com/2013/05/)
    -   [April 2013](http://normaldeviate.wordpress.com/2013/04/)
    -   [March 2013](http://normaldeviate.wordpress.com/2013/03/)
    -   [February 2013](http://normaldeviate.wordpress.com/2013/02/)
    -   [January 2013](http://normaldeviate.wordpress.com/2013/01/)
    -   [December 2012](http://normaldeviate.wordpress.com/2012/12/)
    -   [November 2012](http://normaldeviate.wordpress.com/2012/11/)
    -   [October 2012](http://normaldeviate.wordpress.com/2012/10/)
    -   [September 2012](http://normaldeviate.wordpress.com/2012/09/)
    -   [August 2012](http://normaldeviate.wordpress.com/2012/08/)
    -   [July 2012](http://normaldeviate.wordpress.com/2012/07/)
    -   [June 2012](http://normaldeviate.wordpress.com/2012/06/)

-   ### Categories

    -   [Uncategorized](http://normaldeviate.wordpress.com/category/uncategorized/)

-   ### Meta

    -   [Register](https://en.wordpress.com/signup/?ref=wplogin)
    -   [Log in](https://normaldeviate.wordpress.com/wp-login.php)
    -   [Entries RSS](http://normaldeviate.wordpress.com/feed/)
    -   [Comments
        RSS](http://normaldeviate.wordpress.com/comments/feed/)
    -   [WordPress.com](http://wordpress.com/ "Powered by WordPress, state-of-the-art semantic personal publishing platform.")

-   ### Follow Blog via Email

    Enter your email address to follow this blog and receive
    notifications of new posts by email.

    Join 945 other followers

-   ### RSS

    -   [All
        posts](http://normaldeviate.wordpress.com/feed/ "Normal Deviate Posts RSS feed")
    -   [All
        comments](http://normaldeviate.wordpress.com/comments/feed/ "Normal Deviate Comments RSS feed")

-   ### Blog Stats

    -   462,914 hits

-   ### Search

-   ### Blogroll

    -   [Andrew Gelman's Blog](http://andrewgelman.com/)
    -   [Christian Robert's Blog](http://xianblog.wordpress.com/)
    -   [Cosma Shalizi's
        Blog](http://www.cscs.umich.edu/~crshalizi/weblog/)
    -   [Deborah Mayo's
        Blog](http://errorstatistics.com "Foundations of statistics")
    -   [John Langford's Blog](http://hunch.net/)
    -   [Judea Pearl](http://www.mii.ucla.edu/causality/)
    -   [Nuit Blanche](http://nuit-blanche.blogspot.it/)
    -   [Sebastien Bubeck](https://blogs.princeton.edu/imabandit/)

-   ### RSS Feeds

    -   [All
        posts](http://normaldeviate.wordpress.com/feed/ "Normal Deviate latest posts")
    -   [All
        comments](http://normaldeviate.wordpress.com/comments/feed/ "Normal Deviate latest comments")

-   ### Meta

    -   [Register](https://en.wordpress.com/signup/?ref=wplogin)
    -   [Log in](https://normaldeviate.wordpress.com/wp-login.php)

[Blog at WordPress.com](https://wordpress.com/?ref=footer_blog). | [The
Sandbox 1.6.2
Theme](https://wordpress.com/themes/sandbox-162/ "Learn more about this theme").

[Follow](javascript:void(0))

### Follow “Normal Deviate”

Get every new post delivered to your Inbox.

Join 945 other followers

[Powered by WordPress.com](https://wordpress.com/?ref=lof)

Send to Email Address Your Name Your Email Address

![loading](http://s2.wp.com/wp-content/mu-plugins/post-flair/sharing/images/loading.gif?m=1315610318g)
[Cancel](#cancel)

Post was not sent - check your email addresses!

Email check failed, please try again

Sorry, your blog cannot share posts by email.

%d bloggers like this:

![image](http://pixel.wp.com/b.gif?v=noscript)

This markdown document has been converted from the html document located at:
http://normaldeviate.wordpress.com/2013/07/13/lost-causes-in-statistics-ii-noninformative-priors/
