Vapnik–Chervonenkis theory
==========================

From Wikipedia, the free encyclopedia

Jump to: [navigation](#mw-navigation), [search](#p-search)

[Machine learning](/wiki/Machine_learning "Machine learning") and\
 [data mining](/wiki/Data_mining "Data mining")

[![Scatterplot featuring a linear support vector machine's decision
boundary (dashed
line)](//upload.wikimedia.org/wikipedia/commons/thumb/4/46/Linear-svm-scatterplot.svg/220px-Linear-svm-scatterplot.svg.png)](/wiki/File:Linear-svm-scatterplot.svg "Scatterplot featuring a linear support vector machine's decision boundary (dashed line)")

Problems

-   [Classification](/wiki/Statistical_classification "Statistical classification")
-   [Clustering](/wiki/Cluster_analysis "Cluster analysis")
-   [Regression](/wiki/Regression_analysis "Regression analysis")
-   [Anomaly detection](/wiki/Anomaly_detection "Anomaly detection")
-   [Association
    rules](/wiki/Association_rule_learning "Association rule learning")
-   [Reinforcement
    learning](/wiki/Reinforcement_learning "Reinforcement learning")
-   [Structured
    prediction](/wiki/Structured_prediction "Structured prediction")
-   [Feature learning](/wiki/Feature_learning "Feature learning")
-   [Online
    learning](/wiki/Online_machine_learning "Online machine learning")
-   [Semi-supervised
    learning](/wiki/Semi-supervised_learning "Semi-supervised learning")
-   [Grammar induction](/wiki/Grammar_induction "Grammar induction")

[Supervised learning](/wiki/Supervised_learning "Supervised learning")\

(**[classification](/wiki/Statistical_classification "Statistical classification")**
• **[regression](/wiki/Regression_analysis "Regression analysis")**)

-   [Decision
    trees](/wiki/Decision_tree_learning "Decision tree learning")
-   [Ensembles](/wiki/Ensemble_learning "Ensemble learning")
    ([Bagging](/wiki/Bootstrap_aggregating "Bootstrap aggregating"),
    [Boosting](/wiki/Boosting_(machine_learning) "Boosting (machine learning)"),
    [Random forest](/wiki/Random_forest "Random forest"))
-   [*k*-NN](/wiki/K-nearest_neighbors_classification "K-nearest neighbors classification")
-   [Linear regression](/wiki/Linear_regression "Linear regression")
-   [Naive Bayes](/wiki/Naive_Bayes_classifier "Naive Bayes classifier")
-   [Neural
    networks](/wiki/Artificial_neural_network "Artificial neural network")
-   [Logistic
    regression](/wiki/Logistic_regression "Logistic regression")
-   [Perceptron](/wiki/Perceptron "Perceptron")
-   [Support vector machine
    (SVM)](/wiki/Support_vector_machine "Support vector machine")
-   [Relevance vector machine
    (RVM)](/wiki/Relevance_vector_machine "Relevance vector machine")

[Clustering](/wiki/Cluster_analysis "Cluster analysis")

-   [BIRCH](/wiki/BIRCH_(data_clustering) "BIRCH (data clustering)")
-   [Hierarchical](/wiki/Hierarchical_clustering "Hierarchical clustering")
-   [*k*-means](/wiki/K-means_clustering "K-means clustering")
-   [Expectation-maximization
    (EM)](/wiki/Expectation-maximization_algorithm "Expectation-maximization algorithm")
-   \
     [DBSCAN](/wiki/DBSCAN "DBSCAN")
-   [OPTICS](/wiki/OPTICS_algorithm "OPTICS algorithm")
-   [Mean-shift](/wiki/Mean-shift "Mean-shift")

[Dimensionality
reduction](/wiki/Dimensionality_reduction "Dimensionality reduction")

-   [Factor analysis](/wiki/Factor_analysis "Factor analysis")
-   [CCA](/wiki/Canonical_correlation_analysis "Canonical correlation analysis")
-   [ICA](/wiki/Independent_component_analysis "Independent component analysis")
-   [LDA](/wiki/Linear_discriminant_analysis "Linear discriminant analysis")
-   [NMF](/wiki/Non-negative_matrix_factorization "Non-negative matrix factorization")
-   [PCA](/wiki/Principal_component_analysis "Principal component analysis")
-   [t-SNE](/wiki/T-distributed_stochastic_neighbor_embedding "T-distributed stochastic neighbor embedding")

[Structured
prediction](/wiki/Structured_prediction "Structured prediction")

-   [Graphical models](/wiki/Graphical_model "Graphical model") ([Bayes
    net](/wiki/Bayesian_network "Bayesian network"),
    [CRF](/wiki/Conditional_random_field "Conditional random field"),
    [HMM](/wiki/Hidden_Markov_model "Hidden Markov model"))

[Anomaly detection](/wiki/Anomaly_detection "Anomaly detection")

-   [*k*-NN](/wiki/K-nearest_neighbors_classification "K-nearest neighbors classification")
-   [Local outlier
    factor](/wiki/Local_outlier_factor "Local outlier factor")

[Neural
nets](/wiki/Artificial_neural_network "Artificial neural network")

-   [Autoencoder](/wiki/Autoencoder "Autoencoder")
-   [Deep learning](/wiki/Deep_learning "Deep learning")
-   [Multilayer
    perceptron](/wiki/Multilayer_perceptron "Multilayer perceptron")
-   [RNN](/wiki/Recurrent_neural_network "Recurrent neural network")
-   [Restricted Boltzmann
    machine](/wiki/Restricted_Boltzmann_machine "Restricted Boltzmann machine")
-   [SOM](/wiki/Self-organizing_map "Self-organizing map")
-   [Convolutional neural
    network](/wiki/Convolutional_neural_network "Convolutional neural network")

Theory

-   [Bias-variance
    dilemma](/wiki/Bias-variance_dilemma "Bias-variance dilemma")
-   [Computational learning
    theory](/wiki/Computational_learning_theory "Computational learning theory")
-   [Empirical risk
    minimization](/wiki/Empirical_risk_minimization "Empirical risk minimization")
-   [PAC
    learning](/wiki/Probably_approximately_correct_learning "Probably approximately correct learning")
-   [Statistical
    learning](/wiki/Statistical_learning_theory "Statistical learning theory")
-   **VC theory**

-   [![Portal
    icon](//upload.wikimedia.org/wikipedia/commons/thumb/d/d2/Internet_map_1024.jpg/16px-Internet_map_1024.jpg)](/wiki/File:Internet_map_1024.jpg)
    [Computer science
    portal](/wiki/Portal:Computer_science "Portal:Computer science")
-   [![Portal
    icon](//upload.wikimedia.org/wikipedia/commons/thumb/4/40/Fisher_iris_versicolor_sepalwidth.svg/16px-Fisher_iris_versicolor_sepalwidth.svg.png)](/wiki/File:Fisher_iris_versicolor_sepalwidth.svg)
    [Statistics portal](/wiki/Portal:Statistics "Portal:Statistics")

-   [v](/wiki/Template:Machine_learning_bar "Template:Machine learning bar")
-   [t](/wiki/Template_talk:Machine_learning_bar "Template talk:Machine learning bar")
-   [e](//en.wikipedia.org/w/index.php?title=Template:Machine_learning_bar&action=edit)

Contents
--------

-   [1 Introduction](#Introduction)
-   [2 Overview of VC theory in Empirical
    Processes](#Overview_of_VC_theory_in_Empirical_Processes)
    -   [2.1 Background on Empirical
        Processes](#Background_on_Empirical_Processes)
    -   [2.2 Symmetrization](#Symmetrization)
    -   [2.3 VC Connection](#VC_Connection)

-   [3 VC Inequality](#VC_Inequality)
-   [4 References](#References)

Introduction[[edit](/w/index.php?title=Vapnik%E2%80%93Chervonenkis_theory&action=edit&section=1 "Edit section: Introduction")]
------------------------------------------------------------------------------------------------------------------------------

**Vapnik–Chervonenkis theory** (also known as **VC theory**) was
developed during 1960–1990 by [Vladimir
Vapnik](/wiki/Vladimir_Vapnik "Vladimir Vapnik") and [Alexey
Chervonenkis](/wiki/Alexey_Chervonenkis "Alexey Chervonenkis"). The
theory is a form of [computational learning
theory](/wiki/Computational_learning_theory "Computational learning theory"),
which attempts to explain the learning process from a statistical point
of view.

VC theory is related to **statistical learning theory** and to
[empirical processes](/wiki/Empirical_processes "Empirical processes").
[Richard M. Dudley](/wiki/Richard_M._Dudley "Richard M. Dudley") and
[Vladimir Vapnik](/wiki/Vladimir_Vapnik "Vladimir Vapnik") himself,
among others, apply VC-theory to [empirical
processes](/wiki/Empirical_processes "Empirical processes").

VC theory covers at least four parts (as explained in *The Nature of
Statistical Learning
Theory*^[[1]](//en.wikipedia.org/wiki/Vapnik%E2%80%93Chervonenkis_theory#endnote_nslt)^):

-   Theory of consistency of learning processes
    -   What are (necessary and sufficient) conditions for consistency
        of a learning process based on the [empirical risk
        minimization](/wiki/Empirical_risk_minimization "Empirical risk minimization")
        principle?

-   Nonasymptotic theory of the rate of convergence of learning
    processes
    -   How fast is the rate of convergence of the learning process?

-   Theory of controlling the generalization ability of learning
    processes
    -   How can one control the rate of convergence (the
        [generalization](/wiki/Machine_learning#Generalization "Machine learning")
        ability) of the learning process?

-   Theory of constructing learning machines
    -   How can one construct algorithms that can control the
        generalization ability?

VC Theory is a major subbranch of [statistical learning
theory](/wiki/Statistical_learning_theory "Statistical learning theory").
One of its main applications in statistical learning theory is to
provide
[generalization](/wiki/Machine_learning#Generalization "Machine learning")
conditions for learning algorithms. From this point of view, VC theory
is related to
**[stability](/wiki/Stability_(learning_theory) "Stability (learning theory)")**,
which is an alternative approach for characterizing generalization.

In addition, VC theory and [VC
dimension](/wiki/VC_dimension "VC dimension") are instrumental in the
theory of [empirical
processes](/wiki/Empirical_processes "Empirical processes"), in the case
of processes indexed by VC classes. Arguably these are the most
important applications of the VC theory, and are employed in proving
generalization. Several techniques will be introduced that are widely
used in the empirical process and VC theory. The discussion is mainly
based on the book "Weak Convergence and Empirical Processes: With
Applications to
Statistics"^[[2]](//en.wikipedia.org/wiki/Vapnik%E2%80%93Chervonenkis_theory#endnote_wcep)^.

Overview of VC theory in Empirical Processes[[edit](/w/index.php?title=Vapnik%E2%80%93Chervonenkis_theory&action=edit&section=2 "Edit section: Overview of VC theory in Empirical Processes")]
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

### Background on Empirical Processes[[edit](/w/index.php?title=Vapnik%E2%80%93Chervonenkis_theory&action=edit&section=3 "Edit section: Background on Empirical Processes")]

Let
![X\_1,X\_2,\\ldots,X\_n](//upload.wikimedia.org/math/3/a/a/3aaa82d91860d83b245cb541ab06ba83.png)
are random elements defined on a measurable space ![(\\mathcal{X},
\\mathcal{A})](//upload.wikimedia.org/math/c/0/e/c0e326e90843d5dec791c18cef85c58d.png).
Define the empirical measure ![\\mathbb{P}\_n = n\^{-1} \\sum\_{i =
1}\^n
\\delta\_{X\_i}](//upload.wikimedia.org/math/a/3/c/a3c62405bcd44e4133eeea5ca7529efc.png),
where
![\\delta](//upload.wikimedia.org/math/f/1/0/f10f03c9836c36537d2539196058bfa2.png)
here stands for the [dirac
measure](/wiki/Dirac_measure "Dirac measure"). Denote with ![Qf = \\int
f
dQ](//upload.wikimedia.org/math/1/a/2/1a24d743938e698748fd4637e245c6d4.png)
for a measure
![Q](//upload.wikimedia.org/math/f/0/9/f09564c9ca56850d4cd6b3319e541aee.png).
Measurability issues, will be ignored here. For more technical detail
consult^[[3]](//en.wikipedia.org/wiki/Vapnik%E2%80%93Chervonenkis_theory#endnote_nslt)^.

Let
![\\mathcal{F}](//upload.wikimedia.org/math/2/6/a/26afd73f8c17f310707120691ccc4a35.png)
be a class of measurable functions ![f:\\mathcal{X} \\rightarrow
\\mathbb{R}](//upload.wikimedia.org/math/3/6/6/366e4a1a45c8b4e304fd40210290be63.png).
The empirical measure induces a map from
![\\mathcal{F}](//upload.wikimedia.org/math/2/6/a/26afd73f8c17f310707120691ccc4a35.png)to
![\\mathbb{R}](//upload.wikimedia.org/math/6/9/a/69a45f1e602cd2b2c2e67e41811fd226.png)
given by:

![f \\mapsto \\mathbb{P}\_n
f](//upload.wikimedia.org/math/a/0/8/a08810c85bf76cc4c58d00c4a0b57288.png)

Let ![\\vert\\vert Q\\vert\\vert\_{\\mathcal{F}} = \\sup \\{\\vert Qf
\\vert: f \\in \\mathcal{F}
\\}](//upload.wikimedia.org/math/e/2/7/e272f7154d48112f869f54b41faf7bbc.png).
Empirical Processes theory aims at identifying classes
![\\mathcal{F}](//upload.wikimedia.org/math/2/6/a/26afd73f8c17f310707120691ccc4a35.png)
for which statements like:

-   ![ \\vert \\vert \\mathbb{P}\_n - P\\vert \\vert\_{\\mathcal{F}}
    \\rightarrow 0
    ](//upload.wikimedia.org/math/3/8/4/384a83313bc1de5b4e5919b76b03a546.png),
    aka uniform law of large numbers
-   ![ \\mathbb{G}\_n = \\sqrt{n} (\\mathbb{P}\_n - P) \\rightsquigarrow
    \\mathbb{G}, \\quad
    ](//upload.wikimedia.org/math/6/8/3/6835613e00019d187047f3acab3b17e6.png)
    in
    ![\\ell\^{\\infty}(\\mathcal{F})](//upload.wikimedia.org/math/6/1/8/6188bcd2f45b2da0de0433a3bf170da8.png),
    aka uniform central limit theorem

hold. Here
![P](//upload.wikimedia.org/math/4/4/c/44c29edb103a2872f519ad0c9a0fdaaa.png)
is the underlying true distribution of the data, which is unknown in
practice. In the former case the class
![\\mathcal{F}](//upload.wikimedia.org/math/2/6/a/26afd73f8c17f310707120691ccc4a35.png)
is called *Glivenko-Cantelli*, and in the latter case (under the
assumption ![\\sup\_{f \\in \\mathcal{F}}\\vert f(x) - Pf \\vert <
\\infty. \\forall
x](//upload.wikimedia.org/math/9/2/9/9299e3b973f460708e2bef16e6c8b8d5.png))
the class
![\\mathcal{F}](//upload.wikimedia.org/math/2/6/a/26afd73f8c17f310707120691ccc4a35.png)
is called *Donsker* or
![P](//upload.wikimedia.org/math/4/4/c/44c29edb103a2872f519ad0c9a0fdaaa.png)-Donsker.
Obviously, a Donsker class is Glivenko-Cantelli in probability by an
application of [Slutsky's
theorem](/wiki/Slutsky%27s_theorem "Slutsky's theorem") .

These statements are true for a single
![f](//upload.wikimedia.org/math/8/f/a/8fa14cdd754f91cc6554c9e71929cce7.png),
by standard [LLN](/wiki/Law_of_large_numbers "Law of large numbers"),
[CLT](/wiki/Central_limit_theorem "Central limit theorem") arguments
under regularity conditions, and the difficulty in the Empirical
Processes comes in because joint statements are being made for all ![f
\\in
\\mathcal{F}](//upload.wikimedia.org/math/7/e/8/7e81a87cc6f71b97b7e816955a034fee.png).
Intuitively then, the set
![\\mathcal{F}](//upload.wikimedia.org/math/2/6/a/26afd73f8c17f310707120691ccc4a35.png)
cannot be too large, and as it turns out that the geometry of
![\\mathcal{F}](//upload.wikimedia.org/math/2/6/a/26afd73f8c17f310707120691ccc4a35.png)
plays a very important role.

One way of measuring how big the function set
![\\mathcal{F}](//upload.wikimedia.org/math/2/6/a/26afd73f8c17f310707120691ccc4a35.png)
is to use the so-called [covering
numbers](/wiki/Covering_number "Covering number"). The covering number
![N(\\varepsilon, \\mathcal{F},
||\\cdot||)](//upload.wikimedia.org/math/7/b/5/7b50d61618e958f49880a47a109cb2da.png)
is the minimal number of balls ![\\{g: ||g - f|| < \\varepsilon
\\}](//upload.wikimedia.org/math/2/0/1/2019607e891308bb019ee9290439103d.png)
needed to cover the set
![\\mathcal{F}](//upload.wikimedia.org/math/2/6/a/26afd73f8c17f310707120691ccc4a35.png)
(here it is obviously assumed that there is an underlying norm on
![\\mathcal{F}](//upload.wikimedia.org/math/2/6/a/26afd73f8c17f310707120691ccc4a35.png)).
The entropy is the logarithm of the covering number.

Two sufficient conditions are provided below, under which it can be
proved that the set
![\\mathcal{F}](//upload.wikimedia.org/math/2/6/a/26afd73f8c17f310707120691ccc4a35.png)
is Glivenko-Cantelli or Donsker.

A class
![\\mathcal{F}](//upload.wikimedia.org/math/2/6/a/26afd73f8c17f310707120691ccc4a35.png)
is
![P](//upload.wikimedia.org/math/4/4/c/44c29edb103a2872f519ad0c9a0fdaaa.png)-Glivenko-Cantelli
if it is
![P](//upload.wikimedia.org/math/4/4/c/44c29edb103a2872f519ad0c9a0fdaaa.png)-measurable
with envelope
![F](//upload.wikimedia.org/math/8/0/0/800618943025315f869e4e1f09471012.png)
such that ![P\^{\\ast} F <
\\infty](//upload.wikimedia.org/math/0/3/b/03b1b5c14c8ceaf1fab545c677618e2e.png)
and satisfies:

![\\sup\_{Q} N(\\varepsilon ||F||\_Q, \\mathcal{F}, L\_1(Q)) <
\\infty](//upload.wikimedia.org/math/e/1/4/e14fe03d8d5329f0f33db738b8bf348b.png)
for every ![\\varepsilon \>
0](//upload.wikimedia.org/math/b/0/f/b0f19c5714fe9f9891ed26ff783cf639.png).

The next condition is a version of the celebrated [Dudley's
theorem](/wiki/Dudley%27s_theorem "Dudley's theorem"). If
![\\mathcal{F}](//upload.wikimedia.org/math/2/6/a/26afd73f8c17f310707120691ccc4a35.png)
is a class of functions such that

![\\int\_0\^{\\infty} \\sup\_{Q} \\sqrt{\\log N(\\varepsilon
||F||\_{Q,2}, \\mathcal{F}, L\_2(Q))}d \\varepsilon <
\\infty](//upload.wikimedia.org/math/f/1/4/f143fffd003a803838367992be96a1a9.png)

then
![\\mathcal{F}](//upload.wikimedia.org/math/2/6/a/26afd73f8c17f310707120691ccc4a35.png)
is
![P](//upload.wikimedia.org/math/4/4/c/44c29edb103a2872f519ad0c9a0fdaaa.png)-Donsker
for every probability measure ![P
](//upload.wikimedia.org/math/4/4/c/44c29edb103a2872f519ad0c9a0fdaaa.png)
such that ![P\^{\\ast} F\^2 <
\\infty](//upload.wikimedia.org/math/d/d/5/dd5a533efefc09684236e674727cf43b.png).
In the last integral, the notation means ![||f||\_{Q,2} = \\left(\\int
|f|\^2 d
Q\\right)\^{1/2}](//upload.wikimedia.org/math/e/7/4/e747b7064972c27eb7b9530284b893b5.png).

### Symmetrization[[edit](/w/index.php?title=Vapnik%E2%80%93Chervonenkis_theory&action=edit&section=4 "Edit section: Symmetrization")]

The majority of the arguments of how to bound the empirical process,
rely on symmetrization, maximal and concentration inequalities and
chaining. Symmetrization is usually the first step of the proofs, and
since it is used in many machine learning proofs on bounding empirical
loss functions (including the proof of the VC inequality which is
discussed in the next section) it is presented here.

Consider the empirical process:

![f \\mapsto (\\mathbb{P}\_n - P)f = \\dfrac{1}{n} \\sum\_{i = 1}\^n
(f(X\_i) - Pf)
](//upload.wikimedia.org/math/e/9/d/e9ded659e1a06b701afa30220710f214.png)

Turns out that there is a connection between the empirical and the
following symmetrized process:

![f \\mapsto \\mathbb{P}\^0\_n = \\dfrac{1}{n} \\sum\_{i = 1}\^n
\\varepsilon\_i f(X\_i)
](//upload.wikimedia.org/math/0/c/9/0c9c31961721e6f335a6f6a3dbc5d5af.png)

The symmetrized process is a Rademacher process, conditionally on the
data ![X\_i
](//upload.wikimedia.org/math/4/f/c/4fc3e3c98d13ed389817f11dc66c10a6.png).
Therefore it is a sub-Gaussian process by [Hoeffding's
inequality](/wiki/Hoeffding%27s_inequality "Hoeffding's inequality").

**Lemma (Symmetrization).** For every nondecreasing, convex ![\\Phi :
\\mathbb{R} \\rightarrow
\\mathbb{R}](//upload.wikimedia.org/math/e/8/2/e82d3c4528c7906036b509fbfaebabc0.png)
and class of measurable functions
![\\mathcal{F}](//upload.wikimedia.org/math/2/6/a/26afd73f8c17f310707120691ccc4a35.png),

![ \\mathbb{E} \\Phi (||\\mathbb{P}\_n - P||\_{\\mathcal{F}}) \\leq
\\mathbb{E} \\Phi (2 ||\\mathbb{P}\^0\_n||\_{\\mathcal{F}})
](//upload.wikimedia.org/math/6/1/e/61e57cf93344bf632cf3da50e5da092c.png)

The proof of the Symmetrization lemma relies on introducing independent
copies of the original variables
![X\_i](//upload.wikimedia.org/math/4/f/c/4fc3e3c98d13ed389817f11dc66c10a6.png)
(sometimes referred to as a *ghost sample*) and replacing the inner
expectation of the LHS by these copies. After an application of Jensen's
inequality different signs could be introduced (hence the name
symmetrization) without changing the expectation. The proof can be found
below because of its instructive nature.

[Proof]

       Introduce the "ghost sample"  to be independent copies of . For fixed values of  one has:

![||\\mathbb{P}\_n - P||\_{\\mathcal{F}} = \\sup\_{f \\in \\mathcal{F}}
\\dfrac{1}{n} \\left|\\sum\_{i = 1}\^n [f(X\_i) - \\mathbb{E} f(Y\_i)]
\\right| \\leq \\mathbb{E}\_{Y} \\sup\_{f \\in \\mathcal{F}}
\\dfrac{1}{n} \\left|\\sum\_{i = 1}\^n [f(X\_i) - f(Y\_i)]
\\right|](//upload.wikimedia.org/math/7/b/2/7b2725a4e68d33f1ea0434af2e72621f.png)

      Therefore by Jensen's inequality:

![\\Phi(||\\mathbb{P}\_n - P||\_{\\mathcal{F}}) \\leq \\mathbb{E}\_{Y}
\\Phi \\left(\\left|\\left| \\dfrac{1}{n}\\sum\_{i = 1}\^n [f(X\_i) -
f(Y\_i)] \\right|\\right|\_{\\mathcal{F}}
\\right)](//upload.wikimedia.org/math/7/9/6/796c5ab7b3e8a5f687a5bcd58e4af16d.png)

      Taking expectation with respect to  gives:

![\\mathbb{E}\\Phi(||\\mathbb{P}\_n - P||\_{\\mathcal{F}}) \\leq
\\mathbb{E}\_{X} \\mathbb{E}\_{Y} \\Phi \\left(\\left|\\left|
\\dfrac{1}{n}\\sum\_{i = 1}\^n [f(X\_i) - f(Y\_i)]
\\right|\\right|\_{\\mathcal{F}}\\right)](//upload.wikimedia.org/math/e/0/d/e0d7cb0a35952a7f7236bf446ca1580a.png)

      Note that adding a minus sign in front of a term  doesn't change the RHS, because it's a symmetric function of 
      and . Therefore the RHS remains the same under "sign perturbation":

![\\mathbb{E} \\Phi \\left( \\left|\\left| \\dfrac{1}{n}\\sum\_{i =
1}\^n e\_i[f(X\_i) - f(Y\_i)] \\right|\\right|\_{\\mathcal{F}} \\right)
](//upload.wikimedia.org/math/1/3/b/13b58d1354eb511bd59e6a19ba246952.png)

      for any . Therefore:

![\\mathbb{E}\\Phi(||\\mathbb{P}\_n - P||\_{\\mathcal{F}}) \\leq
\\mathbb{E}\_{\\varepsilon} \\mathbb{E} \\Phi \\left( \\left|\\left|
\\dfrac{1}{n}\\sum\_{i = 1}\^n \\varepsilon\_i [f(X\_i) - f(Y\_i)]
\\right|\\right|\_{\\mathcal{F}}
\\right)](//upload.wikimedia.org/math/2/9/a/29ae9d84cdd8f8b25a235acc22ab087c.png)

      Finally using first triangle inequality and then convexity of  gives:

![\\mathbb{E}\\Phi(||\\mathbb{P}\_n - P||\_{\\mathcal{F}}) \\leq
\\dfrac{1}{2}\\mathbb{E}\_{\\varepsilon} \\mathbb{E} \\Phi \\left( 2
\\left|\\left| \\dfrac{1}{n}\\sum\_{i = 1}\^n \\varepsilon\_i
f(X\_i)\\right|\\right|\_{\\mathcal{F}} \\right) +
\\dfrac{1}{2}\\mathbb{E}\_{\\varepsilon} \\mathbb{E} \\Phi \\left( 2
\\left|\\left| \\dfrac{1}{n}\\sum\_{i = 1}\^n \\varepsilon\_i
f(Y\_i)\\right|\\right|\_{\\mathcal{F}}
\\right)](//upload.wikimedia.org/math/f/7/e/f7ead6a3c20c8e7c78ecc561c82179a5.png)

      Where the last two expressions on the RHS are the same, which concludes the proof.

A typical way of proving empirical CLTs, first uses symmetrization to
pass the empirical process to
![\\mathbb{P}\_n\^0](//upload.wikimedia.org/math/7/a/1/7a1d9090dc9127fdb3dfba280a7042a7.png)
and then argue conditionally on the data, using the fact that Rademacher
processes are simple processes with nice properties.

### VC Connection[[edit](/w/index.php?title=Vapnik%E2%80%93Chervonenkis_theory&action=edit&section=5 "Edit section: VC Connection")]

It turns out that there is a fascinating connection between certain
combinatorial properties of the set
![\\mathcal{F}](//upload.wikimedia.org/math/2/6/a/26afd73f8c17f310707120691ccc4a35.png)
and the entropy numbers. Uniform covering numbers can be controlled by
the notion of *Vapnik-Cervonenkis classes of sets* - or shortly *VC
sets*.

Take a collection of subsets of the sample space
![\\mathcal{X}](//upload.wikimedia.org/math/5/4/8/548ad3254513a0a221b4b07fd87e5d9a.png)
-![\\mathcal{C}](//upload.wikimedia.org/math/9/8/c/98c20e34c90ebe87fa78ddb2b83977fd.png).
A collection of sets
![\\mathcal{C}](//upload.wikimedia.org/math/9/8/c/98c20e34c90ebe87fa78ddb2b83977fd.png)
is said to *pick out* a certain subset of the finite set ![S =
\\{x\_1,\\ldots, x\_n\\} \\subset
\\mathcal{X}](//upload.wikimedia.org/math/0/b/2/0b24a8463df033063c074f2037fef330.png)
if ![S = S \\cap
C](//upload.wikimedia.org/math/9/3/e/93ec9064b7398ef036d5531f8c34fc91.png)
for some ![C \\in
\\mathcal{C}](//upload.wikimedia.org/math/8/1/c/81ca17e9bc7eb7a57a680645f7fc9bed.png).
![\\mathcal{C}](//upload.wikimedia.org/math/9/8/c/98c20e34c90ebe87fa78ddb2b83977fd.png)
is said to *shatter*
![S](//upload.wikimedia.org/math/5/d/b/5dbc98dcc983a70728bd082d1a47546e.png)
if it picks out each of its
![2\^n](//upload.wikimedia.org/math/9/a/a/9aa0ec0374c89d2f7f3d9cd2e05a4bc5.png)
subsets. The *VC-index* (similar to [VC
dimension](/wiki/VC_dimension "VC dimension") + 1 for an appropriately
chosen classifier set)
![V(\\mathcal{C})](//upload.wikimedia.org/math/7/7/7/7776528e56a8cad8906d4b49bc6cc2d6.png)
of
![\\mathcal{C}](//upload.wikimedia.org/math/9/8/c/98c20e34c90ebe87fa78ddb2b83977fd.png)
is the smallest
![n](//upload.wikimedia.org/math/7/b/8/7b8b965ad4bca0e41ab51de7b31363a1.png)
for which no set of size
![n](//upload.wikimedia.org/math/7/b/8/7b8b965ad4bca0e41ab51de7b31363a1.png)
is shattered by
![\\mathcal{C}](//upload.wikimedia.org/math/9/8/c/98c20e34c90ebe87fa78ddb2b83977fd.png).

[Sauer's lemma](/wiki/Sauer%E2%80%93Shelah_lemma "Sauer–Shelah lemma")
then states that the number ![\\Delta\_n(\\mathcal{C}, x\_1, \\ldots,
x\_n)](//upload.wikimedia.org/math/7/c/1/7c17ebd2b69dea79a1a1e6f0d5656c45.png)
of subsets picked out by a VC-class
![\\mathcal{C}](//upload.wikimedia.org/math/9/8/c/98c20e34c90ebe87fa78ddb2b83977fd.png)
satisfies:

![\\max\_{x\_1,\\ldots, x\_n} \\Delta\_n(\\mathcal{C}, x\_1, \\ldots,
x\_n) \\leq \\sum\_{j = 0}\^{V(\\mathcal{C}) - 1} {n \\choose j} \\leq
\\left( \\frac{n e}{V(\\mathcal{C}) - 1}\\right)\^{V(\\mathcal{C}) -
1}](//upload.wikimedia.org/math/5/a/4/5a464aacc5e0d331d242ba911957c25c.png)

Which is a polynomial number ![O(n\^{V(\\mathcal{C}) -
1})](//upload.wikimedia.org/math/9/d/0/9d0f431dbad626f39b8db64454377c56.png)
of subsets rather than an exponential number. Intuitively this means
that a finite VC-index implies that
![\\mathcal{C}](//upload.wikimedia.org/math/9/8/c/98c20e34c90ebe87fa78ddb2b83977fd.png)
has an apparent simplistic structure.

A similar bound can be shown (with a different constant, same rate) for
the so-called *VC subgraph classes*. For a function ![f : \\mathcal{X}
\\mapsto
\\mathbb{R}](//upload.wikimedia.org/math/1/0/f/10fd81ad41786d7cf9db9909f09ce90d.png)
the
[*subgraph*](/wiki/Hypograph_(mathematics) "Hypograph (mathematics)") is
a subset of ![\\mathcal{X} \\times
\\mathbb{R}](//upload.wikimedia.org/math/4/7/9/47912237b4277ca1e68d4b213076c8b7.png)
such that: ![\\{(x,t): t <
f(x)\\}](//upload.wikimedia.org/math/4/1/7/41704c5470e2f4592cf1bf6d7eb500fe.png).
A collection of
![\\mathcal{F}](//upload.wikimedia.org/math/2/6/a/26afd73f8c17f310707120691ccc4a35.png)
is called a VC subgraph class if all subgraphs form a VC-class.

Consider a set of indicator functions ![ \\mathcal{I}\_{\\mathcal{C}} =
\\{1\_C: C \\in \\mathcal{C}
\\}](//upload.wikimedia.org/math/5/c/9/5c98364a559cb4dc58de2be5b02f23da.png)
in
![L\_1(Q)](//upload.wikimedia.org/math/a/a/8/aa8b362fe77610dcd3f045967d78109d.png)
for discrete empirical type of measure
![Q](//upload.wikimedia.org/math/f/0/9/f09564c9ca56850d4cd6b3319e541aee.png)
(or equivalently for any probability measure
![Q](//upload.wikimedia.org/math/f/0/9/f09564c9ca56850d4cd6b3319e541aee.png)).
It can then be shown that quite remarkably, for ![r \\geq
1](//upload.wikimedia.org/math/2/4/a/24a4eeec71d093622c71797697beb93e.png):

![N(\\varepsilon, \\mathcal{I}\_{\\mathcal{C}}, L\_r(Q)) \\leq
KV(\\mathcal{C}) (4e)\^{V(\\mathcal{C})}
\\left(\\dfrac{1}{\\varepsilon}\\right)\^{r (V(\\mathcal{C}) -
1)}](//upload.wikimedia.org/math/8/4/b/84b8d3cd4d0b263471c0d663a7c8a4bb.png)

Further consider the *symmetric convex hull* of a set
![\\mathcal{F}](//upload.wikimedia.org/math/2/6/a/26afd73f8c17f310707120691ccc4a35.png):
![\\operatorname{sconv}\\mathcal{F}](//upload.wikimedia.org/math/f/1/9/f19672ddcc7fc357e5045985af6d0604.png)
being the collection of functions of the form ![\\sum\_{i =1}\^m
\\alpha\_i
f\_i](//upload.wikimedia.org/math/a/8/0/a8010711f0a69390c545d782483b4045.png)
with ![\\sum\_{i =1}\^m |\\alpha\_i| \\leq
1](//upload.wikimedia.org/math/7/b/c/7bc9aa296723c82719e06c7da06a93a1.png).
Then if

![N(\\varepsilon||F||\_{Q,2}, \\mathcal{F}, L\_2(Q)) \\leq C
\\left(\\dfrac{1}{\\varepsilon}\\right)\^V](//upload.wikimedia.org/math/5/5/8/5582c07f5bbb147392110e5e9b2fc2c3.png)

the following is valid for the convex hull of
![\\mathcal{F}](//upload.wikimedia.org/math/2/6/a/26afd73f8c17f310707120691ccc4a35.png):

![\\log N(\\varepsilon||F||\_{Q,2}, \\operatorname{sconv}\\mathcal{F},
L\_2(Q)) \\leq K \\left(\\dfrac{1}{\\varepsilon}\\right)\^{\\frac{2V}{V
+
2}}](//upload.wikimedia.org/math/4/c/b/4cb2e20ab24f8a2f3ffd6510898080bb.png)

The important consequence of this fact is that the power of
![1/\\varepsilon](//upload.wikimedia.org/math/d/2/c/d2cca1f6c163303227aeafa21b715987.png)
-- ![2V/(V +
2)](//upload.wikimedia.org/math/8/a/1/8a11a91449e1c06a61c3a41f6186aa62.png)
is strictly less than 2, which is just enough so that the entropy
integral is going to converge, and therefore the class
![\\operatorname{sconv}\\mathcal{F}](//upload.wikimedia.org/math/f/1/9/f19672ddcc7fc357e5045985af6d0604.png)
is going to be
![P](//upload.wikimedia.org/math/4/4/c/44c29edb103a2872f519ad0c9a0fdaaa.png)-Donsker.

Finally an example of a VC-subgraph class is considered. Any
finite-dimensional vector space
![\\mathcal{F}](//upload.wikimedia.org/math/2/6/a/26afd73f8c17f310707120691ccc4a35.png)
of measurable functions ![f:\\mathcal{X} \\mapsto
\\mathbb{R}](//upload.wikimedia.org/math/1/0/f/10fd81ad41786d7cf9db9909f09ce90d.png)
is VC-subgraph of index smaller than or equal to ![\\dim(\\mathcal{F}) +
2](//upload.wikimedia.org/math/e/3/9/e39dc0cae3b62ca72988a8c8e293dca9.png).

[Proof]

      Take  points . The vectors:

![(f(x\_1), \\ldots, f(x\_n)) - (t\_1, \\ldots,
t\_n)](//upload.wikimedia.org/math/2/0/7/2070fca766ef9d968ebed189a15de5d5.png)

      are in a  dimensional subspace of . Take , a non-zero vector that is orthogonal to this subspace. Therefore:

![\\sum\_{a\_i \> 0} a\_i (f(x\_i) - t\_i) = \\sum\_{a\_i < 0} (-a\_i)
(f(x\_i) - t\_i), \\quad \\forall f \\in
\\mathcal{F}](//upload.wikimedia.org/math/9/3/d/93d42386c77dedbb02a8a3dee7edb8f4.png)

      Consider the set . This set cannot be picked out since if there is some  such that  that would imply 
      that the LHS is strictly positive but the RHS is non-negative.

There are generalizations of the notion VC subgraph class, e.g. there is
the notion of pseudo-dimension. The interested reader can look
into^[[4]](//en.wikipedia.org/wiki/Vapnik%E2%80%93Chervonenkis_theory#endnote_pollard)^.

VC Inequality[[edit](/w/index.php?title=Vapnik%E2%80%93Chervonenkis_theory&action=edit&section=6 "Edit section: VC Inequality")]
--------------------------------------------------------------------------------------------------------------------------------

A similar setting is considered, which is more common to [machine
learning](/wiki/Machine_learning "Machine learning"). Let
![\\mathcal{X}](//upload.wikimedia.org/math/5/4/8/548ad3254513a0a221b4b07fd87e5d9a.png)
is a feature space and ![\\mathcal{Y} =
\\{0,1\\}](//upload.wikimedia.org/math/8/e/8/8e8bc7f6f9b9d3e431dfb00af6c02831.png).
A function ![f : \\mathcal{X} \\mapsto
\\mathcal{Y}](//upload.wikimedia.org/math/b/7/8/b78f7a4fcaf7cc09fc00a49749a264fb.png)
is called a classifier. Let
![\\mathcal{F}](//upload.wikimedia.org/math/2/6/a/26afd73f8c17f310707120691ccc4a35.png)
be a set of classifiers. Similarly to the previous section, define the
*[shattering coefficient](/wiki/Shattered_set "Shattered set")*
![S(\\mathcal{F},n) = \\max\_{x\_1,\\ldots, x\_n} |\\{(f(x\_1), \\ldots,
f(x\_n)), f \\in
\\mathcal{F}\\}|](//upload.wikimedia.org/math/4/5/d/45d1e7b581f4201e04affeaf92da7051.png).
The shattering coefficient is also known as growth function. Note here
that there is a 1-1 mapping between each of the functions in
![\\mathcal{F}](//upload.wikimedia.org/math/2/6/a/26afd73f8c17f310707120691ccc4a35.png)
and the set on which the function is 1. Therefore in terms of the
previous section the shattering coefficient is precisely
![\\max\_{x\_1,\\ldots, x\_n} \\Delta\_n(\\mathcal{C}, x\_1, \\ldots,
x\_n)](//upload.wikimedia.org/math/f/6/0/f609e3c5ce23c6d06417cbce0a8348d4.png)
for
![\\mathcal{C}](//upload.wikimedia.org/math/9/8/c/98c20e34c90ebe87fa78ddb2b83977fd.png)
being the collection of all sets described above. Now for the same
reasoning as before, namely using Sauer's Lemma it can be shown that
![S(\\mathcal{F},n)](//upload.wikimedia.org/math/2/9/c/29c29b0d7a85cac5317db62525e8ae33.png)
is going to be polynomial in
![n](//upload.wikimedia.org/math/7/b/8/7b8b965ad4bca0e41ab51de7b31363a1.png)
provided that the class
![\\mathcal{F}](//upload.wikimedia.org/math/2/6/a/26afd73f8c17f310707120691ccc4a35.png)
has a finite VC-dimension or equivalently the collection
![\\mathcal{C}](//upload.wikimedia.org/math/9/8/c/98c20e34c90ebe87fa78ddb2b83977fd.png)
has finite VC-index.

Let ![D\_n = \\{(X\_1, Y\_1), \\ldots,
(X\_n,Y\_m)\\}](//upload.wikimedia.org/math/0/e/e/0eecfea5ae5a5c1dec967b6666222a6a.png)
is an observed dataset. Assume that the data is generated by an unknown
probability distribution
![P\_{XY}](//upload.wikimedia.org/math/a/f/3/af3c21b7689fafdaaffc35f85979b6c4.png).
Define ![R(f) = P(f(X) \\neq
Y)](//upload.wikimedia.org/math/b/a/d/bade527e16bba4bfa4dde32522b7ff3d.png)
to be the expected 0/1 loss. Of course since
![P\_{XY}](//upload.wikimedia.org/math/a/f/3/af3c21b7689fafdaaffc35f85979b6c4.png)
is unknown in general, one has no access to ![R(f)
](//upload.wikimedia.org/math/d/4/e/d4e039a9a365e50aa9a4f6914646db96.png).
However the *empirical risk*, given by:

![\\hat{R}\_n(f) = \\dfrac{1}{n}\\sum\_{i = 1}\^n \\mathbb{I}(f(X\_n)
\\neq
Y\_n)](//upload.wikimedia.org/math/4/2/c/42ceb909aa5f4b2dd871287d6e14df65.png)

can certainly be evaluated. Then one has the following Theorem:

**Theorem (VC Inequality)** For binary classification and the 0/1 loss
function we have the following generalization bounds:

![P\\left(\\sup\_{f \\in \\mathcal{F}} |\\hat{R}\_n(f) - R(f)|
\>\\varepsilon \\right) \\leq 8 S(\\mathcal{F},n)
e\^{-n\\varepsilon\^2/32}
](//upload.wikimedia.org/math/c/2/8/c280e408e866b1fcca5e3ad5a67e5bc5.png)

and

![\\mathbb{E}\\left[\\sup\_{f \\in \\mathcal{F}} |\\hat{R}\_n(f) - R(f)|
\\right] \\leq 2 \\sqrt{\\dfrac{\\log S(\\mathcal{F},n) + \\log
2}{n}}](//upload.wikimedia.org/math/0/4/1/041ddfb56ce88c27e3dca0dc4eeb2a46.png)

In words the VC inequality is saying that as the sample increases,
provided that
![\\mathcal{F}](//upload.wikimedia.org/math/2/6/a/26afd73f8c17f310707120691ccc4a35.png)
has a finite VC dimension, the empirical 0/1 risk becomes a good proxy
for the expecred 0/1 risk. Note that both RHS of the two inequalities
will converge to 0, provided that
![S(\\mathcal{F},n)](//upload.wikimedia.org/math/2/9/c/29c29b0d7a85cac5317db62525e8ae33.png)
grows polynomially in
![n](//upload.wikimedia.org/math/7/b/8/7b8b965ad4bca0e41ab51de7b31363a1.png).

The connection between this framework and the Empirical Process
framework is evident. Here one is dealing with a modified empirical
process ![|\\hat{R}\_n -
R|\_{\\mathcal{F}}](//upload.wikimedia.org/math/4/a/8/4a83757985e2dbcb63ce31e739cc1f7e.png)
but not surprisingly the ideas are the same. The proof of the (first
part of) VC inequality, relies on symmetrization, and then argue
conditionally on the data using concentration inequalities (in
particular [Hoeffding's
inequality](/wiki/Hoeffding%27s_inequality "Hoeffding's inequality")).
The interested reader can check the book
^[[5]](//en.wikipedia.org/wiki/Vapnik%E2%80%93Chervonenkis_theory#endnote_aptpp)^
Theorems 12.4 and 12.5.

References[[edit](/w/index.php?title=Vapnik%E2%80%93Chervonenkis_theory&action=edit&section=7 "Edit section: References")]
--------------------------------------------------------------------------------------------------------------------------

-   **[\^](#ref_nslt)** [Vapnik, Vladimir
    N](/wiki/Vladimir_Vapnik "Vladimir Vapnik") (2000). *The Nature of
    Statistical Learning Theory*. Information Science and Statistics.
    [Springer-Verlag](/wiki/Springer-Verlag "Springer-Verlag").
    [ISBN](/wiki/International_Standard_Book_Number "International Standard Book Number")
    [978-0-387-98780-4](/wiki/Special:BookSources/978-0-387-98780-4 "Special:BookSources/978-0-387-98780-4").
-   [Vapnik, Vladimir N](/wiki/Vladimir_Vapnik "Vladimir Vapnik")
    (1989). Statistical Learning Theory.
    [Wiley-Interscience](/wiki/John_Wiley_%26_Sons "John Wiley & Sons").
    [ISBN](/wiki/International_Standard_Book_Number "International Standard Book Number")
    [0-471-03003-1](/wiki/Special:BookSources/0-471-03003-1 "Special:BookSources/0-471-03003-1").
-   **[\^](#ref_wcep)** van der Vaart, Aad W.; Wellner, Jon A. (2000).
    *Weak Convergence and Empirical Processes: With Applications to
    Statistics* (2nd ed.). Springer.
    [ISBN](/wiki/International_Standard_Book_Number "International Standard Book Number")
    [978-0-387-94640-5](/wiki/Special:BookSources/978-0-387-94640-5 "Special:BookSources/978-0-387-94640-5").
-   **[\^](#ref_aptpp)** Gyorfi, L.; Devroye, L.; Lugosi, G. (1996). *A
    probabilistic theory of pattern recognition.* (1st ed.). Springer.
    [ISBN](/wiki/International_Standard_Book_Number "International Standard Book Number")
    [978-0387946184](/wiki/Special:BookSources/978-0387946184 "Special:BookSources/978-0387946184").
-   See references in articles: [Richard M.
    Dudley](/wiki/Richard_M._Dudley "Richard M. Dudley"), [empirical
    processes](/wiki/Empirical_processes "Empirical processes"),
    [Shattered set](/wiki/Shattered_set "Shattered set").
-   **[\^](#ref_pollard)** Pollard, David (1990). Empirical Processes:
    Theory and Applications. NSF-CBMS Regional Conference Series in
    Probability and Statistics Volume 2.
    [ISBN](/wiki/International_Standard_Book_Number "International Standard Book Number")
    [0-940600-16-1](/wiki/Special:BookSources/0-940600-16-1 "Special:BookSources/0-940600-16-1").
-   Bousquet, O.; Boucheron, S.; Lugosi, G. (2004). "Introduction to
    Statistical Learning Theory". *Advanced Lectures on Machine Learning
    Lecture Notes in Artificial Intelligence 3176, 169-207. (Eds.)
    Bousquet, O., U. von Luxburg and G. Ratsch, Springer*.
-   Vapnik, V.; Chervonenkis, A. (2004). "On the Uniform Convergence of
    Relative Frequencies of Events to Their Probabilities". *Theory
    Probab. Appl., 16(2), 264–280*.

![image](//en.wikipedia.org/wiki/Special:CentralAutoLogin/start?type=1x1)

Retrieved from
"[http://en.wikipedia.org/w/index.php?title=Vapnik–Chervonenkis\_theory&oldid=611161865](http://en.wikipedia.org/w/index.php?title=Vapnik–Chervonenkis_theory&oldid=611161865)"

[Categories](/wiki/Help:Category "Help:Category"):

-   [Computational learning
    theory](/wiki/Category:Computational_learning_theory "Category:Computational learning theory")
-   [Empirical
    process](/wiki/Category:Empirical_process "Category:Empirical process")

Navigation menu
---------------

### Personal tools

-   [Create
    account](/w/index.php?title=Special:UserLogin&returnto=Vapnik%E2%80%93Chervonenkis+theory&type=signup)
-   [Log
    in](/w/index.php?title=Special:UserLogin&returnto=Vapnik%E2%80%93Chervonenkis+theory "You're encouraged to log in; however, it's not mandatory. [o]")

### Namespaces

-   [Article](/wiki/Vapnik%E2%80%93Chervonenkis_theory "View the content page [c]")
-   [Talk](/wiki/Talk:Vapnik%E2%80%93Chervonenkis_theory "Discussion about the content page [t]")

### Variants[](#)

### Views

-   [Read](/wiki/Vapnik%E2%80%93Chervonenkis_theory)
-   [Edit](/w/index.php?title=Vapnik%E2%80%93Chervonenkis_theory&action=edit "You can edit this page. Please use the preview button before saving [e]")
-   [View
    history](/w/index.php?title=Vapnik%E2%80%93Chervonenkis_theory&action=history "Past versions of this page [h]")

### More[](#)

### Search

[](/wiki/Main_Page "Visit the main page")

### Navigation

-   [Main page](/wiki/Main_Page "Visit the main page [z]")
-   [Contents](/wiki/Portal:Contents "Guides to browsing Wikipedia")
-   [Featured
    content](/wiki/Portal:Featured_content "Featured content – the best of Wikipedia")
-   [Current
    events](/wiki/Portal:Current_events "Find background information on current events")
-   [Random article](/wiki/Special:Random "Load a random article [x]")
-   [Donate to
    Wikipedia](https://donate.wikimedia.org/wiki/Special:FundraiserRedirector?utm_source=donate&utm_medium=sidebar&utm_campaign=C13_en.wikipedia.org&uselang=en "Support us")
-   [Wikimedia Shop](//shop.wikimedia.org "Visit the Wikimedia Shop")

### Interaction

-   [Help](/wiki/Help:Contents "Guidance on how to use and edit Wikipedia")
-   [About Wikipedia](/wiki/Wikipedia:About "Find out about Wikipedia")
-   [Community
    portal](/wiki/Wikipedia:Community_portal "About the project, what you can do, where to find things")
-   [Recent
    changes](/wiki/Special:RecentChanges "A list of recent changes in the wiki [r]")
-   [Contact page](//en.wikipedia.org/wiki/Wikipedia:Contact_us)

### Tools

-   [What links
    here](/wiki/Special:WhatLinksHere/Vapnik%E2%80%93Chervonenkis_theory "List of all English Wikipedia pages containing links to this page [j]")
-   [Related
    changes](/wiki/Special:RecentChangesLinked/Vapnik%E2%80%93Chervonenkis_theory "Recent changes in pages linked from this page [k]")
-   [Upload file](/wiki/Wikipedia:File_Upload_Wizard "Upload files [u]")
-   [Special
    pages](/wiki/Special:SpecialPages "A list of all special pages [q]")
-   [Permanent
    link](/w/index.php?title=Vapnik%E2%80%93Chervonenkis_theory&oldid=611161865 "Permanent link to this revision of the page")
-   [Page
    information](/w/index.php?title=Vapnik%E2%80%93Chervonenkis_theory&action=info)
-   [Wikidata
    item](//www.wikidata.org/wiki/Q819095 "Link to connected data repository item [g]")
-   [Cite this
    page](/w/index.php?title=Special:Cite&page=Vapnik%E2%80%93Chervonenkis_theory&id=611161865 "Information on how to cite this page")

### Print/export

-   [Create a
    book](/w/index.php?title=Special:Book&bookcmd=book_creator&referer=Vapnik%E2%80%93Chervonenkis+theory)
-   [Download as
    PDF](/w/index.php?title=Special:Book&bookcmd=render_article&arttitle=Vapnik%E2%80%93Chervonenkis+theory&oldid=611161865&writer=rl)
-   [Printable
    version](/w/index.php?title=Vapnik%E2%80%93Chervonenkis_theory&printable=yes "Printable version of this page [p]")

### Languages

-   [Français](//fr.wikipedia.org/wiki/Th%C3%A9orie_de_Vapnik-Chervonenkis "Théorie de Vapnik-Chervonenkis – French")
-   [Magyar](//hu.wikipedia.org/wiki/Statisztikai_tanul%C3%A1s "Statisztikai tanulás – Hungarian")
-   [中文](//zh.wikipedia.org/wiki/VC%E7%90%86%E8%AE%BA "VC理论 – Chinese")
-   [](#)

[Edit
links](//www.wikidata.org/wiki/Q819095#sitelinks-wikipedia "Edit interlanguage links")

-   This page was last modified on 2 June 2014 at 00:47.\
-   Text is available under the [Creative Commons Attribution-ShareAlike
    License](//en.wikipedia.org/wiki/Wikipedia:Text_of_Creative_Commons_Attribution-ShareAlike_3.0_Unported_License)[](//creativecommons.org/licenses/by-sa/3.0/);
    additional terms may apply. By using this site, you agree to the
    [Terms of Use](//wikimediafoundation.org/wiki/Terms_of_Use) and
    [Privacy Policy](//wikimediafoundation.org/wiki/Privacy_policy).
    Wikipedia® is a registered trademark of the [Wikimedia Foundation,
    Inc.](//www.wikimediafoundation.org/), a non-profit organization.

-   [Privacy
    policy](//wikimediafoundation.org/wiki/Privacy_policy "wikimedia:Privacy policy")
-   [About Wikipedia](/wiki/Wikipedia:About "Wikipedia:About")
-   [Disclaimers](/wiki/Wikipedia:General_disclaimer "Wikipedia:General disclaimer")
-   [Contact Wikipedia](//en.wikipedia.org/wiki/Wikipedia:Contact_us)
-   [Developers](https://www.mediawiki.org/wiki/Special:MyLanguage/How_to_contribute)
-   [Mobile
    view](//en.m.wikipedia.org/w/index.php?title=Vapnik%E2%80%93Chervonenkis_theory&mobileaction=toggle_view_mobile)

-   [![Wikimedia
    Foundation](//bits.wikimedia.org/images/wikimedia-button.png)](//wikimediafoundation.org/)
-   [![Powered by
    MediaWiki](//bits.wikimedia.org/static-1.24wmf21/skins/common/images/poweredby_mediawiki_88x31.png)](//www.mediawiki.org/)


This markdown document has been converted from the html document located at:
https://en.wikipedia.org/wiki/Vapnik%E2%80%93Chervonenkis_theory
