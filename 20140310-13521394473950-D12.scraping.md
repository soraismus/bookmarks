[![Pro
JavaScript](/content/images/2014/Feb/ProJavascript2_01.png)](http://webdesignporto.com)

\

[Follow @ProJavaScript](https://twitter.com/ProJavaScript)

The making of the HackerNews Crawler (+interesting tips to know)
================================================================

![image](/content/images/2014/Feb/HN.jpg) First of all I would like to
mine some data from [HackerNews](https://news.ycombinator.com/), but
will not be covered in this post. Here you will learn how to build a
crawler for difficult sites like HackerNews.

As always the tools are JavaScript, in this case node.js with:

-   **cheerio**: for using jQuery style when querying in the html;
-   **request**: for requesting the html pages;

Main things the crawler should do
=================================

-   Get an entry page;
-   Crawl that page;
-   Extract the data;
-   Get the next page url;
-   Repeat;

Easy, right? Well not that easy with HackerNews!

Let's get started
=================

The first thing we will use is my own Crawler class which looks like
this:

    var futures = require('futures');
    var Future  = futures.future;
    var cheerio = require('cheerio');
    var async   = require('async');
    var request = require('request');
    var _       = require('underscore');



    var Crawler = function(o){
        o = _.extend({
            concurrentRequests : 2,
            fromPage           : 1,
            toPage             : 5,

            // Minimum debug level. Higher shows more logs.
            debugLevel         : 1,
        }, o);

        this.__concurrentRequests = o.concurrentRequests;
        this.__fromPage           = o.fromPage;
        this.__toPage             = o.toPage;
        this.__debugLevel         = o.debugLevel;

        this.onEnd = Future();
        this.__pages = [];
        for(var i = this.__fromPage; i < this.__toPage; i++){
            this.__pages.push(i);
        }
    }


    _.extend(Crawler.prototype, {
        log: function(text, level){
            if(this.__debugLevel >= level){
                console.log(text);
            }
        },



        start: function(){
            var pageData = [];
            var itemsData = [];
            var _this = this;
            var onEachRequest = function(pageNumber, callback){
                this.log('Crawling page: ' + pageNumber + '(' + this.getUrlForPage(pageNumber) + ')' + '...', 2);
                this.requestUrl(this.getUrlForPage(pageNumber)).when(function(data){
                    // pageData[pageNumber] = data;
                    // itemsData = itemsData.concat(data);
                    _this.saveItems(data);
                    callback();
                })
            }.bind(this);
            var onDone = function(err){
                if(err){
                    this.log('Err: ' + err, 1);
                }else{
                    this.log(itemsData, 1);
                    this.log('Done without errors.', 1);
                }
                this.onEnd.deliver(itemsData);
            }.bind(this)
            async.eachLimit(this.__pages, this.__concurrentRequests, onEachRequest, onDone);
        },



        requestUrl: function(url, future){
            future = future || Future();
            var _this = this;
            request({
                    url: url,
                    // timeout: 20,
                    encoding: 'binary', // To display correct portuguese characters.
                },
                function(err, resp, body){
                    if(err){
                        this.log('Error loading: ' + url, 1);
                        return _this.requestUrl(url, future)
                    }
                    var $ = cheerio.load(body);
                    var reloadPage = false;
                    var data = this.extractData($, function(){
                        reloadPage = true;
                    });
                    if(reloadPage){
                        this.log('Reloading url: ' + url, 1);
                        return _this.requestUrl(url, future);
                    }
                    future.deliver(data)
                }.bind(this))
            return future;
        },



        // Don't call in your overridden method.
        extractData: function($, reloadPageFn){
            var itemsData = [];
            var reloadPage = false;
            var $items = this.extractItemsFromPage($, function(){
                reloadPage = true;
            });
            if(reloadPage){
                return reloadPageFn();
            }
            $items.each(function(key, item){
                var data = this.extractDataFromItem($(item), key, itemsData);
                if(data){
                    // Don't push, in this way we can change based on the index.
                    itemsData[key] = data;
                }
            }.bind(this))
            return itemsData;
        },



        extractItemsFromPage: function($, reloadPageFn){
            console.warn('Implement in your subclass. Return the items.');
        },



        extractDataFromItem: function($item){
            console.warn('Implement in your subclass. Return the data extracted.');
        },



        getUrlForPage: function(page){
            console.warn('Implement in your subclass. Return the data.');
        },



        saveItems: function(items){
            console.warn('Implement in your subclass. Save data.');
        }
    })


    module.exports = Crawler;

All this code does is what we defined in the "Main things the crawler
should do". The only difference is that is a really generic class (and
abstract) which can be used for many different sites.

Let's learn the Crawler API
---------------------------

### Starting

    var myCrawler = new Crawler()
    myCrawler.onEnd.when(function(items){
        // Items is just an array of simple JavaScript
        // objects which contains the data crawled.
    })
    myCrawler.start();

### Implementing the Crawler

You have 2 ways to organize your implementation.

If you want a **fine grain control** then you just have to override the
`extractData` and `getUrlForPage` method and if you want to save the
data then override the `saveItems` method too.

    // $: contains the dom, you can use just like jQuery;
    //    $('body > div.hello') will find the first div
    //    with "hello" class in the body tag.
    //
    // reloadPageFn: call this function if you want to reload
    //                  the current page (in case of some kind of error).
    //
    // Return: return an array of objects that contains the crawled data.
    extractData: function($, reloadPageFn){},


    // Receives the page number and returns an url.
    getUrlForPage: function(page){},


    // items: as explained before, just an array of objects.
    saveItems: function(items){
        // save here to the database if you want
    }

If you want a **less fine way to control** then you will have to
override more functions:

    // Like the extractData from the previous section but you have to
    // return the item group. You just return the html item, not
    // the data itself. This method is called on each page crawled.
    extractItemsFromPage: function($, reloadPageFn){},


    // Here you extract the data from the $item element. The $item element
    // Has been filtered by the extractItemsFromPage method before.
    // This method is called on each item in each page.
    extractDataFromItem: function($item){},

Study HackerNews
----------------

Is a good time to create a file named Crawler.js containing the previous
code and another file named HackerNewsCrawler.js in the same folder and
load the dependencies:

    npm install futures cheerio async request underscore --save

The file:

    // HackerNewsCrawler.js
    var _       = require('underscore');
    var Crawler = require('./Crawler');

    var HackerNewsCrawler = function(o){
        o = _.extend({
            concurrentRequests : 1,
            fromPage           : 0,
            toPage             : 5,
            debugLevel         : 2,
        }, o);
        Crawler.apply(this, arguments);
        this.__nextPageUrl = '';
        this.__baseUrl = 'https://news.ycombinator.com';
    }


    _.extend(HackerNewsCrawler.prototype, Crawler.prototype, {
    })

### Getting the next page

We can see that you can get to page 5 without going through 1 (the first
one), 2, 3 and 4. In order to reach the page \#2 to you have to go to
the [first page](https://news.ycombinator.com/) and click on the More
button. This More button is special as it changes very often and looks
like this:

    https://news.ycombinator.com/x?fnid=umXL5mJvs2FIV6PQvXqQ4w

You only need `umXL5mJvs2FIV6PQvXqQ4w` in order to go to page 2. After
you crawl page 2 you will get another More link that looks like this:

    https://news.ycombinator.com/x?fnid=5bi2BjHgCz4w2hwzunPDdB

Again seems random. Maybe is a strange algorithm based on some
timestamp.

The `extractItemsFromPage` method is called on each page, therefore is a
good place to get the next url. Let's implement it in the
HackerNewsCrawler.js:

    extractItemsFromPage: function($, reloadPageFn){
        this.__nextPageUrl = null;
        $('body a').each(function(key, value){
            var $a = $(value);
            var url = $a.attr('href')

            // Next page starts with '/x?fnid='
            if(url && url.substr(0, 8) === '/x?fnid='){
                return this.__nextPageUrl = url;
            }
            if($a.text() === 'More'){
                // Read later why is required.
                return this.__nextPageUrl = true;
            }
        }.bind(this))
        if(!this.__nextPageUrl){
            console.log($('body').toString());
            return reloadPageFn();
        }
        this.log('Found next page url: ' + this.__nextPageUrl, 2);
    },

In order to get the next url you have to check all links (you can filter
it better than me) that starts with the string: `'/x?fnid='` and then
save it. You can see that the code look exactly like jQuery!

In the first run I've noticed that in my browser the More link looks
like `/x?fnid=umXL5mJvs2FIV6PQvXqQ4w` but is not really true in the
crawler. In the crawler, only on the first page the More link is
`'/news2'`. Really strange, but might be because of some headers, I
don't know.

Let's implement the `getUrlForPage` method:

    getUrlForPage: function(page){
        if(page === 0){
            return this.__baseUrl;
        }else if(page === 1){
            return this.__baseUrl + '/news2';
        }else{
            return this.__baseUrl + this.__nextPageUrl;
        }
    },

The crawler will start at page 0, which is the main HackerNews page,
then it will go to page 1, which is
[https://news.ycombinator.com/news2](https://news.ycombinator.com/news2).
Only after page 1 we have to find the More link that looks like
[https://news.ycombinator.com/x?fnid=umXL5mJvs2FIV6PQvXqQ4w](https://news.ycombinator.com/x?fnid=umXL5mJvs2FIV6PQvXqQ4w).
This link is found in the previous method `extractItemsFromPage`.

### Extracting the data

Do you remember the `extractItemsFromPage` method? It should return the
items from the page.

    extractItemsFromPage: function($, reloadPageFn){
        this.__nextPageUrl = null;
        // console.log($('body').toString())
        // return;
        $('body a').each(function(key, value){
            var $a = $(value);
            var url = $a.attr('href')
            // console.log(url, $a.text())

            // Next page starts with '/x?fnid='
            if(url && url.substr(0, 8) === '/x?fnid='){
                return this.__nextPageUrl = url;
            }
            if($a.text() === 'More'){
                console.log(url)
                return this.__nextPageUrl = true;
            }
        }.bind(this))
        if(!this.__nextPageUrl){
            console.log($('body').toString());
            return reloadPageFn();
        }
        this.log('Found next page url: ' + this.__nextPageUrl, 2);
        return $('body > center > table > tr > td > table > tr');
    },

And is done with
`return $('body > center > table > tr > td > table > tr');`. Here we
have another problem. If you look in chrome with the inspector you will
se the following hierarchy:

    body > center > table > tbody > td > tr > td
    // instead of
    body > center > table > tr > td

which is the true html.

Another problem is that the HackerNews still uses tables and makes no
difference between a row containing the title or the sub-title:

![image](/content/images/2014/Feb/HackerNews.jpg)

We instead have to check if a row is a title or subtitle or ghost:

![image](/content/images/2014/Feb/HackerNews2.jpg)

If we look deeper we can see that the titles have a class named "title"
and sub-titles "subtext": \
 ![image](/content/images/2014/Feb/HackerNews_3.jpg)

So if we find a title class then we save a new record, by returing the
object. If we find a subtext class then we add a property on the
previous created object. We do nothing for the rest. Here is the
`extractDataFromItem` method which is called for each `tr` tag.

    extractDataFromItem: function($item, i, itemsData){
        // The table has a lot of tr and the difference between a title
        // and a subtitle is only the class into the tr.
        if($item.find('tr > td.title > a').length){
            var url = $item.find('td > a').attr('href');
            var title = $item.find('td > a').text();
            if(title === 'More') return;
            return {
                url   : url,
                title : title,
            }
        }
        else if($item.find('tr > td.subtext').length){
            if(!itemsData[i - 1]){
                return
            }
            var points = $item.find('tr > td.subtext > span').text();
            points = points.replace(' points', '');
            itemsData[i - 1].points = Number(points);
        }
    },

The points text is removed so we can easily sort by the point number.

Saving the data
---------------

In order to save the data we will be using Mongodb with mongoose (which
is not required for now).

Creating the model:

    var mongoose = require('mongoose')
    mongoose.connect('mongodb://localhost/hackernews');
    var PostSchema = new mongoose.Schema({
        title  : String,
        url    : String,
        points : Number
    },{
        autoIndex: true
       ,strict: true
    });
    var Post = mongoose.model('Post', PostSchema);

Avoid repeated urls and implementing the `saveItems` method:

    saveItems: function(items){
        items = _.compact(items);

        Post.find({url: {$in: _.map(items, function(item){return item.url;})}}, function(err, foundItems){
            // console.log(_.map(items, function(item){return item.url}))
            var foundUrls = _.map(foundItems, function(item){return item.url})

            // Filter the new ones
            items = _.map(items, function(item){
                if(foundUrls.indexOf(item.url) === -1){
                    return item;
                }
            })
            items = _.compact(items);
            if(items.length === 0) return;
            Post.collection.insert(items, function(err, result){})
        })
    }

Just filtering the items, removing the empty ones and the already
inserted. Then we just insert them in the database.

Starting to crawl
-----------------

Let's crawl the first (in this case the last) 5000 pages:

    var c = new HackerNewsCrawler({
        fromPage : 0,
        toPage   : 5000,
    });
    c.start();

Then from the command line:

    mongod
    node HackerNewsCrawler.js

You will see something like this:

    Crawling page: 0(https://news.ycombinator.com)...
    news2
    Found next page url: true
    Crawling page: 1(https://news.ycombinator.com/news2)...
    Found next page url: /x?fnid=jP8oEoiDELBSFmlutEp82C
    Crawling page: 2(https://news.ycombinator.com/x?fnid=jP8oEoiDELBSFmlutEp82C)...
    Found next page url: /x?fnid=811sQnqyuipbalhQ6mVT5W
    Crawling page: 3(https://news.ycombinator.com/x?fnid=811sQnqyuipbalhQ6mVT5W)...
    Found next page url: /x?fnid=k04pMYns2pn1fDzd9VggFP
    Crawling page: 4(https://news.ycombinator.com/x?fnid=k04pMYns2pn1fDzd9VggFP)...
    Found next page url: /x?fnid=igkMSHZMCJHy4MZgBDBSda

This is fine, it's working!

But you will never reach the 5000 pages because HackerNews only allows
you to go up to 45 pages:

    Crawling page: 42(https://news.ycombinator.com/x?fnid=k7b87qtcDqxgHeuF9N4i4C)...
    Found next page url: /x?fnid=byw6X9SnlahcUsfIJEcCgT
    Crawling page: 43(https://news.ycombinator.com/x?fnid=byw6X9SnlahcUsfIJEcCgT)...
    Found next page url: /x?fnid=IzNBqzecJMoyTQ2YEQEMHh
    Crawling page: 44(https://news.ycombinator.com/x?fnid=IzNBqzecJMoyTQ2YEQEMHh)...

If any error happens the crawler will reaload the page. Go in your
mongodb database and check out your new data!

![image](/content/images/2014/Feb/HN-1.jpg)

Your word:
==========

> The making of the HackerNews Crawler with node.js (interesting tips to
> know) [http://t.co/HF1npgZREB](http://t.co/HF1npgZREB)
>
> — Pro JavaScript (@ProJavaScript) [February 24,
> 2014](https://twitter.com/ProJavaScript/statuses/438100180215541760)

Further reading
===============

-   [Flappy bird
    tutorial](http://www.webdesignporto.com/flappy-bird-tutorial-how-to-create-your-own-game-for-mobile-and-browser/?utm_source=http://www.webdesignporto.com/the-making-of-the-hackernews-crawler-interesting-tips-to-know/&utm_medium=internal-further-reading&utm_campaign=internal):
    how to create your own game for mobile and browser;
-   [React.js in pure
    JavaScript](http://www.webdesignporto.com/react-js-in-pure-javascript-facebook-library/?utm_source=http://www.webdesignporto.com/the-making-of-the-hackernews-crawler-interesting-tips-to-know/&utm_medium=internal-further-reading&utm_campaign=internal)
    (Facebook library) - Deep explanation for newbies

Please enable JavaScript to view the [comments powered by
Disqus.](http://disqus.com/?ref_noscript)

[comments powered by Disqus](http://disqus.com)

Posted on Monday 24 Feb 2014

###### [Totty](https://plus.google.com/117110087634936225779?rel=author)

1 1

\

[Follow @ProJavaScript](https://twitter.com/ProJavaScript)

All content copyright [Pro JavaScript](/) © 2014 • All rights reserved.
\
 [Google+](https://plus.google.com/104273320601694264357)

[Google+](https://plus.google.com/104273320601694264357)

This markdown document has been converted from the html document located at:
http://www.webdesignporto.com/the-making-of-the-hackernews-crawler-interesting-tips-to-know/#header?utm_source=internal-index-title&utm_medium=internal-index-title&utm_campaign=internal-index-title
