[![Atomic Object
Spin](http://d37rcl8t6g8sj5.cloudfront.net/wp-content/themes/spin/images/AO-wordmark-white.png)](/)

Atomic Object's Blog On Software Design & Development

[Skip to content](#content "Skip navigation to the content")

### [We're hiring!](http://detroit.atomicobject.com/?utm_source=spin&utm_medium=header&utm_campaign=aod-hiring "Visit our Detroit info page")

We're actively seeking developers and designers for our [Ann
Arbor](http://annarbor.atomicobject.com/?utm_source=spin&utm_medium=header&utm_campaign=aoa2-hiring)
&
[Detroit](http://detroit.atomicobject.com/?utm_source=spin&utm_medium=header&utm_campaign=aod-hiring)
locations.

[« Simplify iOS Models With Mantle – An
Intro](http://spin.atomicobject.com/2014/06/23/ios-models-mantle/)

[GPG + Git: The pass Password Manager
»](http://spin.atomicobject.com/2014/06/25/diy-password-manager-gpg-git/)

An Introduction to Gradient Descent and Linear Regression
=========================================================

By [Matt
Nedrich](http://spin.atomicobject.com/author/nedrich/ "View all posts by Matt Nedrich")
| Published: June 24, 2014

[Gradient descent](http://en.wikipedia.org/wiki/Gradient_descent) is one
of those “greatest hits” algorithms that can offer a new perspective for
solving problems. Unfortunately, it’s rarely taught in undergraduate
computer science programs. In this post I’ll give an introduction to the
gradient descent algorithm, and walk through an example that
demonstrates how gradient descent can be used to solve machine learning
problems such as linear regression.

At a theoretical level, gradient descent is an algorithm that minimizes
functions. Given a function defined by a set of parameters, gradient
descent starts with an initial set of parameter values and iteratively
moves toward a set of parameter values that minimize the function. This
iterative minimization is achieved using calculus, taking steps in the
negative direction of the function
[gradient](http://en.wikipedia.org/wiki/Gradient).

It’s sometimes difficult to see how this mathematical explanation
translates into a practical setting, so it’s helpful to look at an
example. The canonical example when explaining gradient descent is
linear regression.

Linear Regression Example
-------------------------

Simply stated, the goal of linear regression is to fit a line to a set
of points. Consider the following data.

[![points\_for\_linear\_regression1](http://d37rcl8t6g8sj5.cloudfront.net/wp-content/uploads/points_for_linear_regression1.png)](http://d37rcl8t6g8sj5.cloudfront.net/wp-content/uploads/points_for_linear_regression1.png)

Let’s suppose we want to model the above set of points with a line. To
do this we’ll use the standard `y = mx + b` line equation where `m` is
the line’s slope and `b` is the line’s y-intercept. To find the best
line for our data, we need to find the best set of slope `m` and
y-intercept `b` values.

A standard approach to solving this type of problem is to define an
error function (also called a cost function) that measures how “good” a
given line is. This function will take in a `(m,b)` pair and return an
error value based on how well the line fits our data. To compute this
error for a given line, we’ll iterate through each `(x,y)` point in our
data set and sum the square distances between each point’s `y` value and
the candidate line’s `y` value (computed at `mx + b`). It’s conventional
to square this distance to ensure that it is positive and to make our
error function differentiable. In python, computing the error for a
given line will look like:

~~~~ {.python style="font-family:monospace;"}
# y = mx + b
# m is slope, b is y-intercept
def computeErrorForLineGivenPoints(b, m, points):
    totalError = 0
    for i in range(0, len(points)):
        totalError += (points[i].y - (m * points[i].x + b)) ** 2
    return totalError / float(len(points))
~~~~

Formally, this error function looks like:\

[![linear\_regression\_error1](http://d37rcl8t6g8sj5.cloudfront.net/wp-content/uploads/linear_regression_error1.png)](http://d37rcl8t6g8sj5.cloudfront.net/wp-content/uploads/linear_regression_error1.png)

Lines that fit our data better will result in lower error values. If we
minimize this function, we will get the best line for our data. Since
our error function consists of two parameters (`m` and `b`) we can
visualize it as a two-dimensional surface. This is what it looks like
for our data set:

[![gradient\_descent\_error\_surface](http://d37rcl8t6g8sj5.cloudfront.net/wp-content/uploads/gradient_descent_error_surface.png)](http://d37rcl8t6g8sj5.cloudfront.net/wp-content/uploads/gradient_descent_error_surface.png)

Each point in this two-dimensional space represents a line. The height
of the function at each point is the error value for that line. You can
see that some lines yield smaller error values than others (i.e., fit
our data better). When we run gradient descent search, we will start
from some location on this surface and move downhill to find the line
with the lowest error.

To run gradient descent on this error function, we first need to compute
its gradient. The gradient will act like a compass and always point us
downhill. To compute it, we will need to differentiate our error
function. Since our function is defined by two parameters (`m` and `b`),
we will need to compute a partial derivative for each. These derivatives
work out to be:

[![linear\_regression\_gradient1](http://d37rcl8t6g8sj5.cloudfront.net/wp-content/uploads/linear_regression_gradient1.png)](http://d37rcl8t6g8sj5.cloudfront.net/wp-content/uploads/linear_regression_gradient1.png)

We now have all the tools needed to run gradient descent. We can
initialize our search to start at any pair of `m` and `b` values (i.e.,
any line) and let the gradient descent algorithm march downhill on our
error function towards the best line. Each iteration will update `m` and
`b` to a line that yields slightly lower error than the previous
iteration. The direction to move in for each iteration is calculated
using the two partial derivatives from above and looks like this:

~~~~ {.python style="font-family:monospace;"}
def stepGradient(b_current, m_current, points, learningRate):
    b_gradient = 0
    m_gradient = 0
    N = float(len(points))
    for i in range(0, len(points)):
        b_gradient += -(2/N) * (points[i].y - ((m_current*points[i].x) + b_current))
        m_gradient += -(2/N) * points[i].x * (points[i].y - ((m_current * points[i].x) + b_current))
    new_b = b_current - (learningRate * b_gradient)
    new_m = m_current - (learningRate * m_gradient)
    return [new_b, new_m]
~~~~

The `learningRate` variable controls how large of a step we take
downhill during each iteration. If we take too large of a step, we may
step over the minimum. However, if we take small steps, it will require
many iterations to arrive at the minimum.

Below are some snapshots of gradient descent running for 2000 iterations
for our example problem. We start out at point `m = -1` `b = 0`. Each
iteration `m` and `b` are updated to values that yield slightly lower
error than the previous iteration. The left plot displays the current
location of the gradient descent search (blue dot) and the path taken to
get there (black line). The right plot displays the corresponding line
for the current search location. Eventually we ended up with a pretty
accurate fit.

[![gradient\_descent\_search](http://d37rcl8t6g8sj5.cloudfront.net/wp-content/uploads/gradient_descent_search1.png)](http://d37rcl8t6g8sj5.cloudfront.net/wp-content/uploads/gradient_descent_search1.png)

We can also observe how the error changes as we move toward the minimum.
A good way to ensure that gradient descent is working correctly is to
make sure that the error decreases for each iteration. Below is a plot
of error values for the first 100 iterations of the above gradient
search.

[![gradient\_descent\_error\_by\_iteration](http://d37rcl8t6g8sj5.cloudfront.net/wp-content/uploads/gradient_descent_error_by_iteration.png)](http://d37rcl8t6g8sj5.cloudfront.net/wp-content/uploads/gradient_descent_error_by_iteration.png)

We’ve now seen how gradient descent can be applied to solve a linear
regression problem. While the model in our example was a line, the
concept of minimizing a cost function to tune parameters also applies to
regression problems that use higher order polynomials and other problems
found around the machine learning world.

While we were able to scratch the surface for learning gradient descent,
there are several additional concepts that are good to be aware of that
we weren’t able to discuss. A few of these include:

-   **Convexity** – In our linear regression problem, there was only one
    minimum. This made our error surface
    [convex](http://en.wikipedia.org/wiki/Convex_function). Regardless
    of where we started, we would eventually arrive at the absolute
    minimum. In general, this need not be the case. It’s possible to
    have a problem with local minima that a gradient search can get
    stuck in. There are several approaches to mitigate this (e.g.,
    [stochastic gradient
    search](http://en.wikipedia.org/wiki/Stochastic_gradient_descent)).
-   **Performance** – We used vanilla gradient descent with a learning
    rate of 0.0005 in the above example, and ran it for 2000 iterations.
    There are approaches such a [line
    search](http://en.wikipedia.org/wiki/Line_search), that can reduce
    the number of iterations required. For the above example, line
    search reduces the number of iterations to arrive at a reasonable
    solution from several thousand to around 50.
-   **Convergence** – We didn’t talk about how to determine when the
    search finds a solution. This is typically done by looking for small
    changes in error iteration-to-iteration (e.g., where the gradient is
    near zero).

For more information about gradient descent, linear regression, and
other machine learning topics, I would strongly recommend Andrew Ng’s
[machine learning course](https://www.coursera.org/course/ml) on
Coursera.\

![image](http://0.gravatar.com/avatar/07df703abaa070c86cbaf1d142a9b51d?s=80&d=http%3A%2F%2F0.gravatar.com%2Favatar%2Fad516503a11cd5ca435acc9bb6523536%3Fs%3D80&r=G)
Matt Nedrich ([1 Posts](http://spin.atomicobject.com/author/nedrich/))

This entry was posted in [Development
Techniques](http://spin.atomicobject.com/category/development/development-techniques/ "View all posts in Development Techniques")
and tagged [algorithms](http://spin.atomicobject.com/tag/algorithms/),
[gradient descent](http://spin.atomicobject.com/tag/gradient-descent/),
[machine learning](http://spin.atomicobject.com/tag/machine-learning/).
Bookmark the
[permalink](http://spin.atomicobject.com/2014/06/24/gradient-descent-linear-regression/ "Permalink to An Introduction to Gradient Descent and Linear Regression").
Trackbacks are closed, but you can [post a
comment](#respond "Post a comment").

[« Simplify iOS Models With Mantle – An
Intro](http://spin.atomicobject.com/2014/06/23/ios-models-mantle/)

[GPG + Git: The pass Password Manager
»](http://spin.atomicobject.com/2014/06/25/diy-password-manager-gpg-git/)

### 4 Comments

1.  ![image](http://0.gravatar.com/avatar/24ebacd54f81ffb38a98f52f360e001d?s=80&d=http%3A%2F%2F0.gravatar.com%2Favatar%2Fad516503a11cd5ca435acc9bb6523536%3Fs%3D80&r=G)
    Chris

    Posted June 26, 2014 at 5:09 pm

    Maybe I’m missing something, but the y-intercept,slope points
    plotted in the “Gradient Search” graphs don’t seem to correspond to
    the blue lines being generated in the “Data and Current Line”
    graphs. The values for slope seem accurate but the y-intercepts seem
    off.

    [Reply](/2014/06/24/gradient-descent-linear-regression/?replytocom=179071#respond)

    -   ![image](http://0.gravatar.com/avatar/07df703abaa070c86cbaf1d142a9b51d?s=80&d=http%3A%2F%2F0.gravatar.com%2Favatar%2Fad516503a11cd5ca435acc9bb6523536%3Fs%3D80&r=G)
        Matt Nedrich

        Posted June 26, 2014 at 5:37 pm

        Hi Chris, thanks for the comment. The origin (0,0) doesn’t
        correspond to the bottom left of the plot (rather it’s one tick
        in on each axis) so it might be a little confusing to read. What
        specifically looks off?

        [Reply](/2014/06/24/gradient-descent-linear-regression/?replytocom=179086#respond)

2.  ![image](http://1.gravatar.com/avatar/dc6434641fb3e443a1514a494016bfc9?s=80&d=http%3A%2F%2F1.gravatar.com%2Favatar%2Fad516503a11cd5ca435acc9bb6523536%3Fs%3D80&r=G)
    [Ji-A](http://idealcandidate.com/)

    Posted June 27, 2014 at 1:21 pm

    This is very interesting. As I don’t have a comp sci background, can
    you explain when you would use gradient descent to solve a linear
    regression problem vs. using OLS? Thanks.

    [Reply](/2014/06/24/gradient-descent-linear-regression/?replytocom=179617#respond)

3.  ![image](http://0.gravatar.com/avatar/07df703abaa070c86cbaf1d142a9b51d?s=80&d=http%3A%2F%2F0.gravatar.com%2Favatar%2Fad516503a11cd5ca435acc9bb6523536%3Fs%3D80&r=G)
    Matt Nedrich

    Posted June 27, 2014 at 5:28 pm

    Hi Ji-A. I used a simple linear regression example in this post for
    simplicity. As you alluded to, the example in the post has a closed
    form solution that can be solved easily, so I wouldn’t use gradient
    descent to solve such a simplistic linear regression problem.
    However, gradient descent and the concept of parameter
    optimization/tuning is found all over the machine learning world, so
    I wanted to present it in a way that was easy to understand. In
    practice, my understanding is that gradient descent becomes more
    useful in the following scenarios:

    \1) As the number of parameters you need to solve for grows. In our
    example we had two parameters (m and b). In Andrew Ng’s Machine
    Learning class on Coursera, he suggests that when you have more than
    10,000 parameters gradient descent may be a better solution than the
    normal equation closed form solution. See the video here:
    [https://www.youtube.com/watch?v=B3vseKmgi8E&feature=youtu.be&t=11m27s](https://www.youtube.com/watch?v=B3vseKmgi8E&feature=youtu.be&t=11m27s)

    \2) When your system of equations is non-linear. Logistic regression
    (a common machine learning classification method) is an example of
    this.

    \3) When an approximate answer is “good enough”.

    [Reply](/2014/06/24/gradient-descent-linear-regression/?replytocom=179725#respond)

### Post a Comment [Cancel reply](/2014/06/24/gradient-descent-linear-regression/#respond)

Your email is *never* published nor shared. Required fields are marked
\*

Name \*

Email \*

Website

Comment

You may use these HTML tags and attributes:
`<a href="" title=""> <abbr title=""> <acronym title=""> <b> <blockquote cite=""> <cite> <code> <del datetime=""> <em> <i> <q cite=""> <strike> <strong> `

Don't subscribe All Replies to my comments Notify me of followup
comments via e-mail. You can also
[subscribe](http://spin.atomicobject.com/comment-subscriptions?srp=108735&sra=s)
without commenting.

-   ### Search

-   ### About Atomic Object

    We design and build world-class software for all sorts of clients.

    [Atomic
    Object](http://www.atomicobject.com/?utm_source=spin&utm_medium=widget&utm_campaign=about-ao)
    [Contact
    Us](http://www.atomicobject.com/pages/Contact+Us "Contact us on our website at atomicobject.com")

-   ### Categories

    -   [Atomic
        Thinking](http://spin.atomicobject.com/category/atomic-thinking/ "View all posts filed under Atomic Thinking")
        -   [Business
            Practices](http://spin.atomicobject.com/category/atomic-thinking/business-practices/ "View all posts filed under Business Practices")
        -   [Culture](http://spin.atomicobject.com/category/atomic-thinking/culture-atomic/ "View all posts filed under Culture")
        -   [Communities](http://spin.atomicobject.com/category/atomic-thinking/communities/ "View all posts filed under Communities")
        -   [Workplace](http://spin.atomicobject.com/category/atomic-thinking/workplace-atomic/ "View all posts filed under Workplace")
        -   [News &
            Events](http://spin.atomicobject.com/category/atomic-thinking/news-events/ "View all posts filed under News & Events")
        -   [Project
            Stories](http://spin.atomicobject.com/category/atomic-thinking/project-stories/ "View all posts filed under Project Stories")

    -   [Resources for
        Clients](http://spin.atomicobject.com/category/client-resources/ "How we work, and how it makes our clients' project stronger.")
        -   [Creating a Lean
            Startup](http://spin.atomicobject.com/category/client-resources/creating-lean-startup/ "View all posts filed under Creating a Lean Startup")
        -   [Choosing Your
            Team](http://spin.atomicobject.com/category/client-resources/choosing-team/ "View all posts filed under Choosing Your Team")
        -   [Planning Your
            Project](http://spin.atomicobject.com/category/client-resources/planning-estimating/ "View all posts filed under Planning Your Project")
        -   [Designing for
            Users](http://spin.atomicobject.com/category/client-resources/designing-users/ "View all posts filed under Designing for Users")
        -   [Managing Your
            Team](http://spin.atomicobject.com/category/client-resources/managing-team/ "View all posts filed under Managing Your Team")
        -   [Technical
            Practices](http://spin.atomicobject.com/category/client-resources/client-technical-practices/ "View all posts filed under Technical Practices")

    -   [Project & Team
        Management](http://spin.atomicobject.com/category/project-team-mgmt/ "View all posts filed under Project & Team Management")
    -   [UX &
        Design](http://spin.atomicobject.com/category/ux-design/ "View all posts filed under UX & Design")
        -   [UX/Design
            Practices](http://spin.atomicobject.com/category/ux-design/ux-design-practices/ "View all posts filed under UX/Design Practices")
        -   [UX/Design
            Techniques](http://spin.atomicobject.com/category/ux-design/uxdesign-techniques/ "View all posts filed under UX/Design Techniques")
        -   [UX/Design
            Tools](http://spin.atomicobject.com/category/ux-design/ux-design-tools/ "View all posts filed under UX/Design Tools")
        -   [Exploratory
            Testing](http://spin.atomicobject.com/category/ux-design/h-exploratory-testing/ "View all posts filed under Exploratory Testing")

    -   [Development](http://spin.atomicobject.com/category/development/ "View all posts filed under Development")
        -   [Development
            Practices](http://spin.atomicobject.com/category/development/development-practices/ "View all posts filed under Development Practices")
        -   [Development
            Techniques](http://spin.atomicobject.com/category/development/development-techniques/ "View all posts filed under Development Techniques")
        -   [Developer
            Tools](http://spin.atomicobject.com/category/development/developer-tools/ "View all posts filed under Developer Tools")

    -   [Platforms &
        Languages](http://spin.atomicobject.com/category/platforms-languages/ "View all posts filed under Platforms & Languages")
        -   [Functional
            Programming](http://spin.atomicobject.com/category/platforms-languages/funct-programming/ "View all posts filed under Functional Programming")
        -   [Presenter
            First](http://spin.atomicobject.com/category/platforms-languages/presenter-first-dev/ "View all posts filed under Presenter First")
        -   [Web
            Apps](http://spin.atomicobject.com/category/platforms-languages/web-apps/ "View all posts filed under Web Apps")
        -   [Mobile
            Apps](http://spin.atomicobject.com/category/platforms-languages/mobile-apps/ "View all posts filed under Mobile Apps")
        -   [Embedded
            Systems](http://spin.atomicobject.com/category/platforms-languages/embedded-systems/ "View all posts filed under Embedded Systems")
        -   [DevOps & System
            Admin.](http://spin.atomicobject.com/category/platforms-languages/devops/ "View all posts filed under DevOps & System Admin.")
        -   [Android](http://spin.atomicobject.com/category/platforms-languages/android-dev/ "View all posts filed under Android")
        -   [iOS / OS
            X](http://spin.atomicobject.com/category/platforms-languages/ios-osx/ "View all posts filed under iOS / OS X")
        -   [Objective-C](http://spin.atomicobject.com/category/platforms-languages/objective-c-dev/ "View all posts filed under Objective-C")
        -   [C &
            C++](http://spin.atomicobject.com/category/platforms-languages/c-c/ "View all posts filed under C & C++")
        -   [Java](http://spin.atomicobject.com/category/platforms-languages/java-dev/ "View all posts filed under Java")
        -   [jRuby](http://spin.atomicobject.com/category/platforms-languages/jruby-dev/ "View all posts filed under jRuby")
        -   [Python](http://spin.atomicobject.com/category/platforms-languages/python-platforms-languages/ "View all posts filed under Python")
        -   [.NET /
            WPF](http://spin.atomicobject.com/category/platforms-languages/dotnet-wpf/ "View all posts filed under .NET / WPF")
        -   [Ruby](http://spin.atomicobject.com/category/platforms-languages/ruby-dev/ "View all posts filed under Ruby")
        -   [Ruby
            Motion](http://spin.atomicobject.com/category/platforms-languages/ruby-motion-dev/ "View all posts filed under Ruby Motion")
        -   [Ruby on
            Rails](http://spin.atomicobject.com/category/platforms-languages/ruby-on-rails-dev/ "View all posts filed under Ruby on Rails")

    -   [The Software
        Life](http://spin.atomicobject.com/category/software-life/ "View all posts filed under The Software Life")
        -   [Growing as
            Makers](http://spin.atomicobject.com/category/software-life/growing-makers/ "View all posts filed under Growing as Makers")
        -   [Extracurricular
            Activities](http://spin.atomicobject.com/category/software-life/extracurriculars/ "View all posts filed under Extracurricular Activities")
        -   [Evolving the
            Industry](http://spin.atomicobject.com/category/software-life/software-industry/ "View all posts filed under Evolving the Industry")
        -   [Personal
            Optimization](http://spin.atomicobject.com/category/software-life/personal-optimization/ "View all posts filed under Personal Optimization")

    -   [Miscellaneous](http://spin.atomicobject.com/category/miscellaneous/ "View all posts filed under Miscellaneous")

-   ### Stay Connected

    [![rss icon for atomic
    object](http://d37rcl8t6g8sj5.cloudfront.net/wp-content/uploads/rss_icon.png)](http://spin.atomicobject.com/feed)
    [![twitter icon for atomic
    object](http://d37rcl8t6g8sj5.cloudfront.net/wp-content/uploads/twitter_icon.png)](http://twitter.com/atomicobject)
    [![facebook icon for atomic
    object](http://d37rcl8t6g8sj5.cloudfront.net/wp-content/uploads/fb_icon.png)](http://facebook.com/atomicobject)
    [![facebook icon for atomic
    object](http://d37rcl8t6g8sj5.cloudfront.net/wp-content/uploads/flickr_icon.png)](http://flickr.com/atomicobject)

-   [![Great Not
    Big](http://d37rcl8t6g8sj5.cloudfront.net/wp-content/uploads/great-not-big.jpg)](http://greatnotbig.com/?utm_source=spin&utm_medium=banner&utm_campaign=spin)
    Carl is the president and co-founder of [Atomic
    Object](http://www.atomicobject.com)

Copyright © [Atomic Object LLC.](http://www.atomicobject.com) — Grand
Rapids +1 616 776 6020 — Detroit +1 313 444 6010 — Ann Arbor +1 734 887
6020 — [Contact Us](http://www.atomicobject.com/pages/Contact+Us)

This markdown document has been converted from the html document located at:
http://spin.atomicobject.com/2014/06/24/gradient-descent-linear-regression/
