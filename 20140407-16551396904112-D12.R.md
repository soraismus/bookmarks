[![image](http://hackerretreat.com/wp-content/themes/hackretreatv02/images/logo_02.png)](http://hackerretreat.com/ "Hacker Retreat Berlin")
===========================================================================================================================================

[Menu](/)

-   [About](http://hackerretreat.com/about/)
-   [Contact](http://hackerretreat.com/contact/)
-   [Imprint](http://hackerretreat.com/imprint/)
-   [Blog](http://hackerretreat.com/blog/)
-   [FAQs](http://hackerretreat.com/faqs/)
-   [Be a Sponsor](http://hackerretreat.com/sponsors/)
-   [Residents/Mentors](http://hackerretreat.com/residents/)

-   [Be a Sponsor](http://hackerretreat.com/sponsors/)

-   [About](http://hackerretreat.com/about/)
-   [Contact](http://hackerretreat.com/contact/)
-   [Blog](http://hackerretreat.com/blog/)
-   [FAQs](http://hackerretreat.com/faqs/)
-   [Residents/Mentors](http://hackerretreat.com/residents/)

R: The Good Parts
=================

*Note: As i’m a facilitator for both Hacker Retreat and Data Science
Retreat, R expertise and interest will be in the room at this summer!
Want to be surrounded by smart+friendly professional coders? [Apply to
Hacker Retreat’s Batch-02](/?r=R "Apply to HRet Batch-02."). To ramp-up
fast as a data scientist, [apply to Data Science
Retreat](http://datascienceretreat.com/?r=R "Apply to Data Science Retreat.").
— Jose Quesada, Ph.D.*

Douglas Crockford’s JavaScript: the Good Parts book is now a classic.
It’s also a tiny, tiny book :). His premise is that if you avoid the bad
parts, JS is not such a bad language. By simply focusing on the better
parts of JavaScript, Crockford saved a lot of trees and elevated the
usage of JavaScript for many coders.

What about good and bad parts of another language, R? Bad parts for the
R language are compiled in The R Inferno – a list of things not to do
with R. There’s even a [Facebok page for R
haters](https://www.facebook.com/pages/I-hate-R-programming/119835724743926).
But let us not come here to bury R, but praise it…a little.

For newcomers, R is a dynamic language for statistical computing that
combines two language paradigms: lazy functional programming features
and object-oriented programming. But surprisingly, R’s functional
aspects – such as lazy sequences – are rarely used. This is an example
of a wider pattern: a lot of what’s wrong in R is due to usage, not to
the language itself (which to be sure, has warts). If you are interested
in the details on how people (mis) use R, I recommend reading the paper
by Floreal Morandat, Brandon Hill, Leo Osvald, and Jan Vitek (2012) [1].
There’s a lot of bad code in R out there because advanced functional
programming skills are rare among R programmers. And as R has become
surprisingly popular with non-programmers, what you see out in the wild
is not representative of what one could and should do with R.

Indeed, R (both the language and the platform) is a very interesting
place to be right now being both popular yet misunderstood as the same
time – so guidance in its usage is called for. [John D. Cook challenged
someone to write ‘R: the good
parts’](http://www.johndcook.com/blog/2009/05/01/r-the-good-parts/), and
what follows is my response. BTW, the source of the advice that follows:
I’m Dr. Jose Quesada and I’ve been using R professionally since 2000.
I’ve consulted/worked as a data scientist for the last two years, and
before that I was doing research with web-scale datasets. Together with
Dr. Kai Wu, I run an intensive 12-week course called [Data Science
Retreat](http://datascienceretreat.com)in Berlin, emphasizing mentoring,
pair-programming, and projects based on real-world, high-value problems.

So here’s an ‘R: the good parts’ post to start with
![:)](http://hackerretreat.com/wp-includes/images/smilies/icon_smile.gif)

### The Hadleyverse

[Hadley Wickham](http://had.co.nz/) is a prolific programmer; he has
recreated big chunks of R’s standard library, making his versions so
much better that people call it ‘the Hadleyverse’. His paper The
split-apply-combine strategy for data analysis argues that much of what
we do day-to-day is reshaping data. His packages help in that direction,
and the Hadleyverse is a collection of R packages (Plyr, lubridate,
ggplot, etc). If you are starting with R, it’s advisable to forget about
the standard lib and use these instead (except for Plyr: I recommend
using data.table’s syntax for subsetting and aggregating). In any case,
don’t bother learning the \*apply() family!

#### Plyr, for expressiveness

Functional programming idioms in R include a limited range of
higher-order functions, most notably a family of map/reduce operations
on multi-dimensional arrays, unhelpfully all called apply with an
optional initial consonant.

This makes for a mess of apply functions that nobody remembers
correctly. The plyr package fixes this.

Thinking at this level (applying functions to data structures; the core
of mapReduce) is very helpful. Plyr can now parallelize its operations
and make use of multiple processors simultaneously. However, I won’t go
into detail here because I have a better alternative below (data.table).
For the best map to the jungle that the apply() family is, [check this
StackOverflow
answer](http://stackoverflow.com/questions/3505701/r-grouping-functions-sapply-vs-lapply-vs-apply-vs-tapply-vs-by-vs-aggrega).

#### Lubridate for date data

Dealing with dates is painful in any language. If you need to do
split-apply-combine by date, the Lubridate library makes things easier.
Common uses: know if a date lands in an interval, create intervals,
aggregation by time period (something Excel users love to do, with pivot
tables).

#### Ggplot2 Wins

Ggplot2 is an implementation of Leland Wilkinson’s Grammar of Graphics—a
general scheme for data visualization which breaks up graphs into
semantic components such as scales and layers. This has been copied in
Python-land. It enables the user to add, remove or alter components in a
plot at a high level of abstraction. If you have seen a pretty R plot,
it was probably ggplot2 (or Trellis graphs!), and definitely NOT the
standard lib’s plot() function
![:)](http://hackerretreat.com/wp-includes/images/smilies/icon_smile.gif)

### Use [Data.table](http://cran.r-project.org/web/packages/data.table/index.html)

The core R data structure is called data.frame, and is fantastically
useful. Proof of its success is that it has been copied in Python
(Pandas) in Julia, and in Clojure (Incanter).

The R designers decided to not include a call-by-reference mechanism in
the language. Since then, R developed the reputation of being a memory
hog, because passing data to functions may imply copying such data.

Imagine you have a big data structure, say a large data.frame. If you
call a function several times on the results of the previous call, you
need a new copy of such data frame. This is pretty common on statistical
analysis. In practice R is pretty smart about reducing the number of
copies, e.g. Morandat et al. find only 37% of arguments end up being
copied. In any case, when datasets are large, any copying hurts
performance. More so when you don’t know for sure what libraries you use
are doing in the background. Data.table makes call-by-reference possible
again. Because a data.table is a data.frame, it is compatible with R
functions and packages that only accept data.frame.

It makes large datasets of millions of rows and hundreds of columns
trivial to use on standard hardware, even a laptop. Data.table also
makes reading files much faster than with read.table, and it even infers
the data types for each column. Memory usage is tiny compared to
data.frame. All in all, once you find data.table, you never go back. The
secret to its performance is that columns are efficiently indexed with
btrees, like an in-memory database.

Here’s an example by [Paul
Hiemstra](https://groups.google.com/forum/#!topic/manipulatr/Xo3-2FBI35k)
showing how as datasets get larger (grey titles) doing averages across
many factor levels (x axis) is much faster using data.table compared to
ave() -base R- and tddply() -plyr-.

![image](https://lh6.googleusercontent.com/s6wRhUailIEc9vlZlmqgdbjGZ0Etzqzi2Im8abvigXv97oFKKW2fHqLYiP-UlGCPQrH1mLgwKYIxfiZL8pUTGZBqjXRvCcFpTqOMmyKDmwFyN-BTiPbJEWFHo4rNbg)

But killer performance is not the only advantage. data.table has a much
better syntax for subsetting, aggregating, etc. It’s less verbose than
the standard syntax on data.frames, being both more compact and more
powerful. Example:

data.tabe query example

\# orders dat (data.table) by two columns, z and b dat[order(z, b)] \#
same thing with idiomatic data.frame syntax. note how verbose the
indexing is - I have to repeat dat a few times: dat &lt;-
dat[order(dat$name, pmax(dat$z, dat$b)), ]

1

2

3

4

\# orders dat (data.table) by two columns, z and b

dat[order(z, b)]

\# same thing with idiomatic data.frame syntax. note how verbose the
indexing is - I have to repeat dat a few times:

dat &lt;- dat[order(dat$name, pmax(dat$z, dat$b)), ]

Since it’s not in the standard library, data.table is not necessarily
used by other packages. But it’s just a matter of time: it should
replace data.frame, as it’s technically superior. I have no doubt that
as more packages use it, R memory performance will greatly improve.

### For predictive models, use the Caret package

The [caret package](http://caret.r-forge.r-project.org/) (short for
Classification And REgression Training) does a lot of things:

- Data splits

- Common preprocessing: creating dummy variables, identifying zero- and
near-zero-variance predictors, finding correlated predictors, centering
and scaling, etc

- Training (using Cross-validation)

- Common visualizations (eg: featurePlot)

It’s an abstraction layer on top of all machine learning packages in
CRAN (which is a lot! \~ 150 models). Kuhn, the author, thought
carefully about how to unify those packages. They all need to split data
into training, parametrization, and testing. They all do some form of
cross-validation; they all do training, and they all do prediction at
the end.

What happened here is that people started adding packages that sorta do
the same things, but in their own way. Caret offers an homogeneous
wrapper on top of them, providing functions that ‘just work’.

This is the package we use for teaching Data Science Retreat (in the
“Finding insights” section). It makes iterating and comparing different
predictive models very convenient.

### Use named arguments

Every design decision in R seems to favor interactive usage, at the cost
of hampering scalability (in the programmer sense). If something is
convenient because it involves less typing at the prompt, then it’s
there. Even if it might make writing large programs a bit more
difficult. Forget the ‘explicit is better than implicit’ maxim that
Python programmers live by.

Order of parameters doesn’t matter; you can also omit them (there’s
usually a default).

Named arguments are often used in R; Named arguments are very useful for
data analysis, because functions such as the standard library’s plot()
accept over 20 arguments.

This is something that Julia doesn’t have, and if I ever decided to
write a lot of Julia code, I’m sure I’ll miss R’s named arguments.

### Be more functional than imperative, if you can

The current R design is a compromise between the functional and the
imperative; it allows local side effects, but enforces purity across
function boundaries. R is not given as much credit as it deserves for
keeping this balance: Scala hogs it all; before it was OCAML. R can be
used in a pure functional way; the fact that it’s not simply reflects
the average user’s rush to get things done, skipping time needed for
better software design.

### Pass-by-reference missing? Usually not

R gets a lot of flak because of its memory consumption. But as a matter
of fact, if a function doesn’t modify a passed data frame, R doesn’t
actually copy it. It’s formally pass-by-value, but the implementation
uses a copy-on-write approach, so no copy is made when the function only
reads the parameter’s values. That at least covers the common case of
passing a bunch of data to a function that builds a statistical model.
As mentioned in the data.table section, on average only 37% of arguments
end up being copied. And on top of that, by using data.table you do have
pass-by-reference by default. If you want an actual copy of a data.table
object, you must use copy().

### Killer package manager & ecosystem

If you ever tried to install all Python dependencies to get it to do
scientific computing… you know what I’m talking about.

In comparison, R’s package manager ‘just works’.
Install.package(“name”)) is all you need, and dependencies are never a
problem. Even when the package has C code in it, builds are pretty solid
on most operating systems. This is not the case with say Python and pip,
where it’s usual to link to github repos in requirements.txt (a disaster
waiting to happen).

Packages are tested nightly. If their tests fail (packages must have
tests) they are quarantined, and eventually removed if not fixed. The R
core group fanatically tests on all platforms, including Solaris; this
imposes a serious burden for maintainers; Doug Bates just abandoned R
because it was too tiresome to get his (crucial) packages to pass all
the tests on fringe platforms.

The R ecosystem provides an interesting case study on how non-software
engineers build and maintain software systems (not bad at all). The
community is active; it used to be that the mailing list was the best
place to get help (!), but nowadays StackOverflow is brimming with R
activity.

The number of user-contributed packages has doubled in the last two
years, primarily because many new packages are added. The number of
dependencies per package is similar to that of Debian packages.

The R community is freaking impressive in their creativity. In fact,
what they create is often copied in other scientific computing
communities (data.frame -\> pandas; ggplot2 -\> [yhat’s
ggplot](https://github.com/yhat/ggplot/), etc)

### Documentation

Every function has one or more examples in the docs. This is a gold
standard other programming languages should emulate! Packages are
extensively documented, with user-contributed packages having no less
documentation than core packages. In all sets of packages, the
proportion of .rd files is the largest (the median of the ratio of rd
files to source ones is 1.5 for Base, and 1.25 for recommended
packages).

The standard language comes with several datasets, and packages come
with datasets. There’s even a generic example() function that shows up
what a package can do (including graphics).

On top of that, there are hundreds of R books for any topic. Books on
advanced statistical techniques are likely going to have R examples, and
the R package comes out about at the same time as the key paper.
Journals like Journal of Statistical Software and the R Journal are
nearly entirely devoted to R packages.

R packages even have vignettes (longer forms of documentation), with
actual examples. Since R ships data together with code, the examples are
executable. It’s the best ‘desert island’ language: if you had no
internet connection, using only the R docs could keep you going no
problem.

### Graphics not an afterthought

Aggregators like [R-bloggers](http://www.r-bloggers.com/) highlight
sites around the Web that use R, frequently presenting demonstrations of
what R is capable of doing, including source code. Most of us have seen
graphics in the wild that really sold us on R. Not only are graphs
pretty and flexible; most packages come with some nice defaults. Each
package is a small DSL to solve a well-defined problem, with
best-practices built-in, even for graphs.

### Excellent tooling

R studio is making some seriously good stuff. It’s the first HTML5 IDE
that people actually like, and use over fully native alternatives. If
you don’t have much memory or cores on your local computer, [R-studio
can run on the
server](http://blog.yhathq.com/posts/r-in-the-cloud-part-1.html) so it’s
easy to set up an EC2 instance to run it. It even has a powerful
debugger. Whereas in other toolchains you’d have just a terminal, sftp,
and a local editor, with R-studio you have a fully functioning IDE that
runs on the server.

The excellent Rmarkdown package makes literate programming a breeze, and
it makes trivial to create simple presentations with actual code.

There’s even a Web framework for R:
[Shiny](http://www.rstudio.com/shiny/). It uses websockets to let you
alter graphs in real time, and it’s about the fastest way to put
together a specialized administrative dashboard to monitor and gain
insight into your applications. This alone adds a lot of business value.

### Possibilities for optimization

R as a language is defined by its implementation. And a poor
implementation, at that. For many years there was only one, carefully
guarded by the ‘R Elders’, who frown upon humanity from the highness of
the R mailing list.

But this is changing. The R VM doesn’t need to suck. The same empirical
survey of a corpus of R code (by Morandat et al.) indicates that the
majority of practical program fragments is actually reasonably safe. And
in the last months, there have been some impressive attempts at
improving the R interpreter itself. [Pretty quick R
(pqR)](http://radfordneal.github.io/pqR/) is a drop-in replacement that
adds a few tweaks that improve performance significantly. It also uses
helper threads to makes parallel computation easier.
[Renjin](http://www.renjin.org/)implements R on the JVM; although most
packages don’t work (yet) this can be a very interesting development for
enterprise deployments. [Riposte is a total rewrite in
C++](https://github.com/jtalbot/riposte). And the standard compiler
package does turns our function into bytecode which can be executed by
the computer faster, producing 30-60% improvements with no code changes.

### Summary: how to use the good parts

1.  Never use the standard lib sapply, tapply etc functions. Back in the
    day I’d have said ‘use the hadleyverse’, but nowadays there are
    better, more performant alternatives.

2.  Don’t use S4, and if you can, don’t use S3 (object systems).

3.  Use the caret package instead of writing crossvalidation code
    yourself for each model.

4.  Master the non-standard data structures (vectors, arrays, lists,
    data frames, and matrices). But don’t bother too much with
    data.frame. The days of data.frame are numbered.

5.  Use data.table in every case you’d use data.frame. It has it’s own
    query syntax. Makes plyr etc obsolete.

6.  Use traceback() and debug() on the console. Test on a clean
    workspace often. Use a proper visual debugger (both Rstudio and
    StatET have one).

7.  Use the compiler package and pqR if you want ‘free’ improvements
    without touching your code.

Want to Learn This Stuff In-Person?
-----------------------------------

Want to be surrounded by smart+friendly professional coders?

[Apply to Hacker Retreat’s Batch-02](/?r=R "Apply to HRet Batch-02.")

To ramp-up fast as a data scientist,

[apply to Data Science
Retreat](http://datascienceretreat.com/?r=R "Apply to Data Science Retreat.")

References:
-----------

[1] Floréal Morandat, Brandon Hill, Leo Osvald, Jan Vitek

Evaluating the design of the R language: objects and functions for data
analysis

In Proceedings of the 26th European conference on Object-Oriented
Programming (2012), pp. 104-131, doi:10.1007/978-3-642-31057-7\_6

[2] Baltasar Trancón-y-Widemann, Carl Friedrich Bolz, Clemens Grelck:
The Functional Programming Language R and the Paradigm of Dynamic
Scientific Programming – (Position Paper). Trends in Functional
Programming (2012): 182-197

Batch-02 will start in May 2014. If you are interested in doing the
Retreat, let's talk.

[Set up a short interview](https://hackerretreat.typeform.com/to/Hlykmh)

Not ready to apply, but want to stay in touch and level-up your code-fu
anyway?

-   [About](http://hackerretreat.com/about/)
-   [Contact](http://hackerretreat.com/contact/)
-   [Imprint](http://hackerretreat.com/imprint/)
-   [Blog](http://hackerretreat.com/blog/)
-   [FAQs](http://hackerretreat.com/faqs/)
-   [Mentors](http://hackerretreat.com/residents/)

© 2014 Hacker Retreat. [Terms of Use](/#/) | [Policy](/#/) | [Be a
Sponsor](/sponsors/) | [Hire Talent](/want-hire-software-talent/)

-   [@hackerRetreat](https://twitter.com/hackerRetreat)


This markdown document has been converted from the html document located at:
http://hackerretreat.com/r-good-parts/
