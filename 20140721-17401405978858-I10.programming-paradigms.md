[Search](https://www.google.com/webhp?tab=Xw)
[Images](http://www.google.com/imghp?hl=en&tab=Xi)
[Maps](https://maps.google.com/maps?hl=en&tab=Xl)
[Play](https://play.google.com/?hl=en&tab=X8)
[YouTube](https://www.youtube.com/?tab=X1)
[News](https://news.google.com/nwshp?hl=en&tab=Xn)
[Gmail](https://mail.google.com/mail/?tab=Xm)
[Drive](https://drive.google.com/?tab=Xo) [More
»](http://www.google.com/intl/en/options/)

[Send Feedback](http://www.google.com/intl/en-US/+/learnmore/forum/) |
[Web History](http://www.google.com/history/optout?hl=en) |
[Help](http://www.google.com/support/profiles/?p=help_center&hl=en-US) |
[Sign
in](https://accounts.google.com/ServiceLogin?service=oz&passive=1209600&continue=https://plus.google.com/%2BBeauCronin/posts/KpeRdJKR6Z1?gpsrc%3Dgplp0)

[](/stream "Home")

Home

[](/stream)

Home

[](/me)

Profile

[](/people)

People

[](/photos)

Photos

[](/communities)

Communities

[](/events)

Events

[](/hangouts)

Hangouts

[](/dashboard)

Pages

[](/local)

Local

[](/settings/plus)

Settings

Feedback

Help · Region

[Privacy &
Terms](http://www.google.com/intl/en-US/+/policy/content.html) · [Maps
Terms](http://www.google.com/intl/en-US/help/terms_maps.html)

[![image](//lh4.googleusercontent.com/-7GHamqOuGgU/AAAAAAAAAAI/AAAAAAAAAHw/9A36fVH_f_k/s100-c-k-no/photo.jpg)](./107971134877020469960)

[Beau Cronin](./107971134877020469960)

[![image](https://lh4.googleusercontent.com/-7GHamqOuGgU/AAAAAAAAAAI/AAAAAAAAAHw/aXvpvimQ02E/s46-c-k-no/photo.jpg)](./107971134877020469960)

### [Beau Cronin](./107971134877020469960)

Shared publicly - [2013-03-23](107971134877020469960/posts/KpeRdJKR6Z1)

**Why Probabilistic Programming Matters**\
\
Last week, DARPA announced a new program to fund research in
probabilistic programming languages. While the accompanying news stories
gave a certain angle on why this was important, this is a new research
area and it's still pretty mysterious to most people who care about
machine intelligence.\
\
So: what is probabilistic programming, and why does it matter? Read on
for my thoughts.\
\
A probabilistic programming language is a language which includes random
events as first-class primitives. When the expressive power of a real
programming language is brought to bear on random events, the developer
can easily encode sophisticated **structured stochastic processes** -
i.e., probabilistic models of the events that might have occurred in the
world to produce a given collection of data or observations.\
\
But just writing down a probability model as a computer program wouldn't
be particularly exciting - that's just a matter of syntax. The real
power of a probabilistic programming language lies in its compiler or
runtime environment (like other languages, probabilistic ones can be
either compiled or interpreted). In addition to its usual duties, the
compiler or runtime needs to figure out how to perform **inference** on
the program. Inference answers the question: of all of the ways in which
a program containing random choices could execute, which of those
execution paths provides the best explanation for the data?\
\
Another way of thinking about this: unlike a traditional program, which
only runs in the forward directions, a probabilistic program is run in
both the forward and backward direction. It runs forward to compute the
consequences of the assumptions it contains about the world (i.e., the
model space it represents), but it also runs backward from the data to
constrain the possible explanations. In practice, many probabilistic
programming systems will cleverly interleave these forward and backward
operations to efficiently home in on the best explanations.\
\
So, that's probabilistic programming in a nutshell. It's clearly cool,
but why does it matter?\
\
**Probabilistic programming will unlock narrative explanations of
data**, one of the holy grails of business analytics and the unsung hero
of scientific persuasion. People think in terms of stories - thus the
unreasonable power of the anecdote to drive decision-making,
well-founded or not. But existing analytics largely fails to provide
this kind of story; instead, numbers seemingly appear out of thin air,
with little of the causal context that humans prefer when weighing their
options.\
\
But probabilistic programs can be written in an explicitly "generative"
fashion - i.e., a program directly encodes a space of hypotheses, where
each one comprises a candidate explanation of how the world produced the
observed data. The specific solutions that are chosen from this space
then constitute specific causal and narrative explanations for the
data.\
\
The dream here is to combine the best aspects of anecdotal and
statistical reasoning - the persuasive power of story-telling, and the
predictiveness and generalization abilities of the larger body of data.\
\
**Probabilistic programming decouples modeling and inference.** Just as
modern databases separate querying from indexing and storage, and
high-level languages and compilers separate algorithmic issues from
hardware execution, probabilistic programming languages provide a
crucial abstraction boundary that is missing in existing learning
systems.\
\
While modeling and inference/learning have long been seen as
conceptually separate activities, they have been tightly linked in
practice. What this means is that the models that are applied to a given
problem are very tightly constrained by the need to derive efficient
inference schemes - and these schemes are incredibly time-consuming to
derive and implement, requiring very specialized skills and expertise.
And even small changes to the model can necessitate wholesale rethinking
of the inference approach. This is no way to run a railroad.\
\
If probabilistic programming reaches its potential, these two activities
will finally be decoupled by a solid abstraction barrier: the programmer
will write the program he wants, and then leave it to the compiler or
runtime to sort out the inference (though see below for challenges).
This should allow the probabilistic programmer to encode and leverage
much more of his domain knowledge, utimately leading to much more
powerful reasoning and better data-driven decisions.\
\
**Probabilistic programming enables more general, abstract reasoning**,
because these programs can directly encode many levels of reasoning and
then perform inference across all of these levels simultaneously.
Existing learning systems are largely confined to learning the model
parameters that provide the best explanation of the data, but they are
unable to reason about whether the model class itself is appropriate.\
\
Probabilistic programs, on the other hand, can reason across all of
these levels at the same time: learning model parameters, and also
choosing the best models from a given class, and also deciding between
entirely different model classes. While there is no silver bullet to
modeling bias and the application of inappropriate models, this
capability will nevertheless go a long way toward providing the best
explanations for data, not only at the quantitative but also the
qualitative level.\
\
Finally, I should mention some of the key challenges that researchers
will have to solve if probabilistic programming is to have these kinds
of impacts:\
\
**Creating the compilers and runtimes that automatically generate good
inference schemes for arbitrary probabilistic programs is very, very
hard.** If the discussion above sounds too good to be true, that's
because it just might be. Researchers have a lot of work ahead of them
if they want to make all of this work in the general setting. The most
successful systems to date (BUGS, [infer.net](http://infer.net),
[factor.ie](http://factor.ie)) have worked by limiting the expressive
power of the language, and thus reducing the complexity of the inference
problem.\
\
**All abstractions are leaky, and the abstraction between modeling
(i.e., writing a probabilistic program) and inference is no different.**
What this means is that some probabilistic programs will result in much
faster inference than others, for reasons that may not be easy for the
programmer to understand. In traditional high-level programming
languages, these problems are typically solved by profiling and
optimization, and probabilistic systems will need to come with the
analogous set of tools so that programmers can inspect and solve the
performance problems they encounter.\
\
Finally, **probabilistic programming requires a different combination of
skills** from traditional development, on the one hand, and machine
learning or statistical inference on the other. I think it would be a
shame if these systems, once they were developed, remained at the
fringes - used only by the kinds of programmers who are currently drawn
to, say, languages like Lisp and Haskell.\
\
While these challenges are indeed great, I have very high hopes for
probabilistic programming over the next few years. Some of the very
smartest people I've ever known or worked with are involved in these
projects, and I think they're going to make a lot of progress.\
\
Note: I have no involvement with any current probabilistic programming
research, nor with the DARPA program; at this point, I'm just an
interested observer.\
\
Update: edited for typos, and to add title.﻿

[![image](//lh3.googleusercontent.com/proxy/crahshLHyG_poVGB4po7HcVVqXnpxTKNkC-o379Sm9BGadMKl2IlPxaBvSG8Fz-lYIQrLcx3A7Oevz7JAd_XolPbxFlweHs9DqCfQFZp9UY3GEAyMC9Kk6Aa0AYsCLR7Uw=w120-h120)![image](//lh3.googleusercontent.com/proxy/crahshLHyG_poVGB4po7HcVVqXnpxTKNkC-o379Sm9BGadMKl2IlPxaBvSG8Fz-lYIQrLcx3A7Oevz7JAd_XolPbxFlweHs9DqCfQFZp9UY3GEAyMC9Kk6Aa0AYsCLR7Uw=w120-h120)](http://www.wired.com/dangerroom/2013/03/darpa-machine-learning-2/all/1)

[So It Begins: Darpa Sets Out to Make Computers That Can Teach
Themselves | Danger Room |
Wired.com](http://www.wired.com/dangerroom/2013/03/darpa-machine-learning-2/all/1)

[wired.com](http://www.wired.com/dangerroom/2013/03/darpa-machine-learning-2/all/1)

142

117

![Beau Cronin's profile
photo](https://lh4.googleusercontent.com/-7GHamqOuGgU/AAAAAAAAAAI/AAAAAAAAAHw/aXvpvimQ02E/s28-c-k-no/photo.jpg "Beau Cronin")![John
Fields's profile
photo](https://lh6.googleusercontent.com/-sdgRpHTp4ao/AAAAAAAAAAI/AAAAAAAADIM/RXI1F8Og-p0/s28-c-k-no/photo.jpg "John Fields")![Vijay
Varanasi's profile
photo](https://lh6.googleusercontent.com/-rGs3o6Muyo8/AAAAAAAAAAI/AAAAAAAAAAA/8r_7aNXxrKM/s28-c-k-no/photo.jpg "Vijay Varanasi")![Christian
Nyumbayire's profile
photo](https://lh3.googleusercontent.com/-bJet5y2DZo0/AAAAAAAAAAI/AAAAAAAAAE8/2hysxetzxk8/s28-c-k-no/photo.jpg "Christian Nyumbayire")

25 comments

[![image](https://lh6.googleusercontent.com/-FOtilGrxp1w/AAAAAAAAAAI/AAAAAAAACuo/5d6fFnpLm4w/s28-c-k-no/photo.jpg)](./110933609939852080663)

[Carter Schonwald](./110933609939852080663)

+

2\
3\
2\

Reply

I think the simplest solution is to defringe using Haskell as a serious
tool in the analytical community :) \
(note: i'm not currently doing anything touching on probabilistic
programming, but I know folks who are building serious probabilistic
programming products on top of a haskell substrate)﻿

[![image](https://lh5.googleusercontent.com/-1vaQFCIa7iE/AAAAAAAAAAI/AAAAAAAAAGc/e-g71OWdJuw/s28-c-k-no/photo.jpg)](./111162250133962560131)

[Cameron Davidson-Pilon](./111162250133962560131)

+

5\
6\
5\

Reply

Hi I'm the main author of Probablisitc Programming and Bayesian Methods
for Hackers, I enjoyed your thoughts here, would you mind if I quoted
you in the book? ﻿

[![image](https://lh4.googleusercontent.com/-7GHamqOuGgU/AAAAAAAAAAI/AAAAAAAAAHw/aXvpvimQ02E/s28-c-k-no/photo.jpg)](./107971134877020469960)

[Beau Cronin](./107971134877020469960)

+

0\
1\
0\

Reply

+[Cameron Davidson-Pilon](/111162250133962560131) Sure thing - I like
where you're going with that book so far. Do you know about the tutorial
the Church guys have put together?﻿

[![image](https://lh4.googleusercontent.com/-7GHamqOuGgU/AAAAAAAAAAI/AAAAAAAAAHw/aXvpvimQ02E/s28-c-k-no/photo.jpg)](./107971134877020469960)

[Beau Cronin](./107971134877020469960)

+

0\
1\
0\

Reply

+[Cameron Davidson-Pilon](/111162250133962560131) Also, I'd be happy to
give other feedback if and when you're ready.﻿

[![image](https://lh4.googleusercontent.com/-7GHamqOuGgU/AAAAAAAAAAI/AAAAAAAAAHw/aXvpvimQ02E/s28-c-k-no/photo.jpg)](./107971134877020469960)

[Beau Cronin](./107971134877020469960)

+

0\
1\
0\

Reply

+[Carter Schonwald](/110933609939852080663) I wasn't trying to troll
with the Lisp/Haskell stuff - I just meant that I want this stuff to
have the widest possible usage down the road, and I hope that it will
find it's way into even the most mainstream languages. This is what the
[infer.net](http://infer.net) folks at MSR have been aiming for, though
I don't think that stuff has made it out into the wild in any
significant way yet.﻿

[![image](https://lh5.googleusercontent.com/-1vaQFCIa7iE/AAAAAAAAAAI/AAAAAAAAAGc/e-g71OWdJuw/s28-c-k-no/photo.jpg)](./111162250133962560131)

[Cameron Davidson-Pilon](./111162250133962560131)

+

0\
1\
0\

Reply

+[Beau Cronin](/107971134877020469960) Hi Beau, I'm not familiar with
the Church tutorial, can you provide a link? Also, considering your
expertise in the field, I'm sure we will employ your opinions of the
book later. ﻿

[![image](https://lh5.googleusercontent.com/-kET_K7S7cac/AAAAAAAAAAI/AAAAAAAAFxA/Vt9xbFvdg6U/s28-c-k-no/photo.jpg)](./106633533692256339236)

[Larysa Visengeriyeva](./106633533692256339236)

+

0\
1\
0\

Reply

+[Cameron Davidson-Pilon](/111162250133962560131) +[Beau
Cronin](/107971134877020469960) ... the church tutorial
[http://projects.csail.mit.edu/church/wiki/Church](http://projects.csail.mit.edu/church/wiki/Church)\
Moreover, every talk by N.Goodman is very valuable.﻿

[![image](https://lh5.googleusercontent.com/-1vaQFCIa7iE/AAAAAAAAAAI/AAAAAAAAAGc/e-g71OWdJuw/s28-c-k-no/photo.jpg)](./111162250133962560131)

[Cameron Davidson-Pilon](./111162250133962560131)

+

0\
1\
0\

Reply

+[Larysa Visengeriyeva](/106633533692256339236) Awesome, thanks!﻿

[![image](https://lh6.googleusercontent.com/-FOtilGrxp1w/AAAAAAAAAAI/AAAAAAAACuo/5d6fFnpLm4w/s28-c-k-no/photo.jpg)](./110933609939852080663)

[Carter Schonwald](./110933609939852080663)

+

0\
1\
0\

Reply

+[Beau Cronin](/107971134877020469960) wasn't suggesting you were
trolling :) \
\
the issue with [infer.net](http://infer.net)
[http://research.microsoft.com/en-us/um/cambridge/projects/infernet/](http://research.microsoft.com/en-us/um/cambridge/projects/infernet/)\
is that, like many awesome MSR projects, that no one can easily use it
for their own ends. the MSR License just means you can use it in a black
box way, can't really contrib, and you cant' reuse ideas elsewhere. (Z3
is probably the most successful of such MSR projects in terms of wider
use, and its scope of usage is pretty small as things go). (and unless
they actually have sales resources attached to it, probably not
practical to get a commercial distro of [infer.net](http://infer.net))﻿

[![image](https://lh6.googleusercontent.com/-FOtilGrxp1w/AAAAAAAAAAI/AAAAAAAACuo/5d6fFnpLm4w/s28-c-k-no/photo.jpg)](./110933609939852080663)

[Carter Schonwald](./110933609939852080663)

+

0\
1\
0\

Reply

(also: because its an MSR research project first, no one is invested in
advocacy of using it for applied things outside of perhaps within
microsoft)﻿

[![image](https://lh5.googleusercontent.com/-yOHLWnNM8Hk/AAAAAAAAAAI/AAAAAAAABN0/ii5M1DKYlqU/s28-c-k-no/photo.jpg)](./103362529593094621422)

[Gabriel Synnaeve](./103362529593094621422)

+

0\
1\
0\

Reply

I used PRISM (Sato et al.) for my Master thesis and I started my PhD
using ProBT (from Probayes, which is a C++ API for probabilistic
programming in C++, derived from a previous Common Lisp probabilistic
programming DSL by Bessière et al. 1999), I've tried BLOG (Russell et
al.) and Alchemy (Domingos et al.). I've switched back to implementing
everything myself these days. For prototyping, probabilistic programming
is great, but once you have finished that step, and if you need to run
something over and over, I find it still way too slow. (Also, I do less
byzantine things than in the past...)﻿

[![image](https://lh6.googleusercontent.com/-FOtilGrxp1w/AAAAAAAAAAI/AAAAAAAACuo/5d6fFnpLm4w/s28-c-k-no/photo.jpg)](./110933609939852080663)

[Carter Schonwald](./110933609939852080663)

+

0\
1\
0\

Reply

+[Gabriel Synnaeve](/103362529593094621422) yeah, there doesn't seem to
be any sane path towards using HPC style tricks to get good performance
for probabilistic modelling languages. (I'm not working on probabilistic
programming language tools myself, but I know folks who are, such as
[bayeshive.com](http://bayeshive.com) ). I think you're correct on the
point that they're perhaps best suited for validating a modelling idea
on a cute sized data set, at least for the near future.\
\
 ﻿

[![image](https://lh4.googleusercontent.com/-7GHamqOuGgU/AAAAAAAAAAI/AAAAAAAAAHw/aXvpvimQ02E/s28-c-k-no/photo.jpg)](./107971134877020469960)

[Beau Cronin](./107971134877020469960)

+

0\
1\
0\

Reply

+[Gabriel Synnaeve](/103362529593094621422) Yes - this is exactly where
the state of the art is these days, and this matches my own experience
(though I haven't used as many of the options as you have). I really
hope that inference performance gets serious attention with this funding
program.\
\
Also, imagine how many developers there are for whom implementing
him/herself just isn't an option - and how big an impact there could be
if these capabilities were more widely available.﻿

[![image](https://lh6.googleusercontent.com/-FOtilGrxp1w/AAAAAAAAAAI/AAAAAAAACuo/5d6fFnpLm4w/s28-c-k-no/photo.jpg)](./110933609939852080663)

[Carter Schonwald](./110933609939852080663)

+

0\
1\
0\

Reply

+[Beau Cronin](/107971134877020469960) 1 internet of upvotes for saying
it that way. Thats a very very very compelling point.﻿

[![image](https://lh5.googleusercontent.com/-habgQgQ7XCI/AAAAAAAAAAI/AAAAAAAAA_M/LR4AuQ7Qd68/s28-c-k-no/photo.jpg)](./115410297695317287275)

[Jim Whitescarver](./115410297695317287275)

+

0\
1\
0\

Reply

Great article! Thanks.\
\
I am working with a group working on an optimization technique using
maximum entropy. However, we lack an expert in probabilistic
programming. I would like some assistance in finding one who is
available now and may be interested in participating in our grant
proposal.﻿

[![image](https://lh6.googleusercontent.com/-Ttt09OUaofk/AAAAAAAAAAI/AAAAAAAABUY/E9Cq56b5Wpg/s28-c-k-no/photo.jpg)](./110532680346088059998)

[Anne van Rossum](./110532680346088059998)

+

0\
1\
0\

Reply

Automate inference? Is that possible?\
\
I'd like to use it for the hierarchical Bayesian models I am building.
The current inference methods, from Gibbs sampling, Metropolis-Hastings,
variational methods, to slice sampling, do not make use of the hierarchy
that I know exists in this model.\
\
That means that inference is really slow. If you have a "compiler" that
comes up with a brilliant way to do inference, I am all ears...﻿

[![image](//lh3.googleusercontent.com/uFp_tsTJboUY7kue5XAsGA=s28)](./115996767362863490155)

[Mark Leeds](./115996767362863490155)

+

0\
1\
0\

Reply

Hi Anne: See the software called STAN which is being developed by
researchers in Columbia's Statistics Department.
[http://mc-stan.org/](http://mc-stan.org/).\
I only used it minimally so I can't comment but I've heard that it's
really good. There is also JAGS in R which is quite nice.﻿

[![image](https://lh4.googleusercontent.com/-7GHamqOuGgU/AAAAAAAAAAI/AAAAAAAAAHw/aXvpvimQ02E/s28-c-k-no/photo.jpg)](./107971134877020469960)

[Beau Cronin](./107971134877020469960)

+

0\
1\
0\

Reply

Yes stan﻿

[![image](https://lh6.googleusercontent.com/-rGs3o6Muyo8/AAAAAAAAAAI/AAAAAAAAAAA/8r_7aNXxrKM/s28-c-k-no/photo.jpg)](./112786630850912254657)

[Vijay Varanasi](./112786630850912254657)

+

0\
1\
0\

Reply

Potentially dumb question : how is this different from Genetic
Programming?\
\
(Specifically, this part of the article reminded me of GP :
"probabilistic programs can be written in an explicitly "generative"
fashion - i.e., a program directly encodes a space of hypotheses, where
each one comprises a candidate explanation of how the world produced the
observed data. The specific solutions that are chosen from this space
then constitute specific causal and narrative explanations for the
data.")﻿

[![image](https://lh5.googleusercontent.com/-habgQgQ7XCI/AAAAAAAAAAI/AAAAAAAAA_M/LR4AuQ7Qd68/s28-c-k-no/photo.jpg)](./115410297695317287275)

[Jim Whitescarver](./115410297695317287275)

+

0\
1\
0\

Reply

The problem is discovery of hypotheses, and the linear dimensionality
between hypotheses. Quantum, optical, neural and evolutionary system
analysis share feature discovery relating the possible to the probable
by maximum relative entropy.﻿

[![image](https://lh3.googleusercontent.com/-7cHDyhCVQ4w/AAAAAAAAAAI/AAAAAAAAAIs/ukbfkizMvHo/s28-c-k-no/photo.jpg)](./104144896745957625633)

[Tieson Molly](./104144896745957625633)

+

0\
1\
0\

Reply

they would be a great use case for systems with many more cores than we
see today. Imagine a super pipeline where all the branches are worked
out to some degree, so a branch miss would not really be a penalty﻿

[![image](https://lh6.googleusercontent.com/-Ttt09OUaofk/AAAAAAAAAAI/AAAAAAAABUY/E9Cq56b5Wpg/s28-c-k-no/photo.jpg)](./110532680346088059998)

[Anne van Rossum](./110532680346088059998)

+

0\
1\
0\

Reply

+[Mark Leeds](/115996767362863490155) It seems this proves my point. I
only see MCMC. If you compare it with for example implementing a Fourier
transform, there is a lot of research on how to do this which starts
with the Cooley–Tukey algorithm. Likewise to do actual inference, there
are many, many choices. And a lot of things are still unknown. It seems
unlikely that automation will work, except for models that are not
advancing state of the art.﻿

[![image](https://lh6.googleusercontent.com/-Ttt09OUaofk/AAAAAAAAAAI/AAAAAAAABUY/E9Cq56b5Wpg/s28-c-k-no/photo.jpg)](./110532680346088059998)

[Anne van Rossum](./110532680346088059998)

+

0\
1\
0\

Reply

+[Vijay Varanasi](/112786630850912254657) Generative models,
[https://en.wikipedia.org/wiki/Generative\_model](https://en.wikipedia.org/wiki/Generative_model),
are very different from genetic programming. You probably have heard
about a Gaussian mixture model, or a Hidden Markov model. It does not
have the concept of a "fitness function", neither that of a "genetic
operator". ﻿

[![image](//lh3.googleusercontent.com/uFp_tsTJboUY7kue5XAsGA=s28)](./115996767362863490155)

[Mark Leeds](./115996767362863490155)

+

0\
1\
0\

Reply

Hi Anne: yes, stan is purely MCMC using a hamiltonian dynamics algorithm
developed by Radford Neal. and then I think improved and called
something else ( u-turn sampling ) by the people at Columbia. I
misunderstood because that's what I thought you were looking for. All
the best and my apologies.﻿

[![image](https://lh6.googleusercontent.com/-Ttt09OUaofk/AAAAAAAAAAI/AAAAAAAABUY/E9Cq56b5Wpg/s28-c-k-no/photo.jpg)](./110532680346088059998)

[Anne van Rossum](./110532680346088059998)

+

0\
1\
0\

Reply

+[Mark Leeds](/115996767362863490155) No problem! Thanks for the
references anyway, much appreciated.﻿

Add a comment...

This markdown document has been converted from the html document located at:
https://plus.google.com/+BeauCronin/posts/KpeRdJKR6Z1
