Boltzmann machine
=================

From Scholarpedia

  ----------------------------------------------------- ------------------------------------------------------------------------------ --------------------------------------------------------------------------------------------------------------------------------
  Geoffrey E. Hinton (2007), Scholarpedia, 2(5):1668.   [doi:10.4249/scholarpedia.1668](http://dx.doi.org/10.4249/scholarpedia.1668)   revision \#91075 [[link to/cite this article](/w/index.php?title=Boltzmann_machine&action=cite&rev=91075 "Boltzmann machine")]
  ----------------------------------------------------- ------------------------------------------------------------------------------ --------------------------------------------------------------------------------------------------------------------------------

Jump to: [navigation](#mw-head), [search](#p-search)

**Curator** and Contributors\

1.00 - **[Geoffrey E.
Hinton](/article/User:Geoffrey_E._Hinton "User:Geoffrey E. Hinton")**

0.50 -

[Eugene M.
Izhikevich](/article/User:Eugene_M._Izhikevich "User:Eugene M. Izhikevich")

[Benjamin
Bronner](/article/User:Benjamin_Bronner "User:Benjamin Bronner")

[Barak
Pearlmutter](/article/User:Barak_Pearlmutter "User:Barak Pearlmutter")

[Zoubin
Ghahramani](/article/User:Zoubin_Ghahramani "User:Zoubin Ghahramani")

-   [Dr. Geoffrey E. Hinton, University of Toronto,
    CANADA](/article/User:Geoffrey_E._Hinton "User:Geoffrey E. Hinton")

A **Boltzmann machine** is a network of symmetrically connected,
neuron-like units that make stochastic decisions about whether to be on
or off. Boltzmann machines have a simple learning
[algorithm](/article/Algorithm "Algorithm") (Hinton & Sejnowski, 1983)
that allows them to discover interesting features that represent complex
regularities in the training data. The learning algorithm is very slow
in networks with many layers of feature detectors, but it is fast in
"restricted Boltzmann machines" that have a single layer of feature
detectors. Many hidden layers can be learned efficiently by *composing*
restricted Boltzmann machines, using the feature activations of one as
the training data for the next.

Boltzmann machines are used to solve two quite different computational
problems. For a *search* problem, the weights on the connections are
fixed and are used to represent a cost function. The stochastic
[dynamics](/article/Dynamical_Systems "Dynamical Systems") of a
Boltzmann machine then allow it to sample binary state vectors that have
low values of the cost function.

For a *learning* problem, the Boltzmann machine is shown a set of binary
data vectors and it must learn to generate these vectors with high
probability. To do this, it must find weights on the connections so
that, relative to other possible binary vectors, the data vectors have
low values of the cost function. To solve a learning problem, Boltzmann
machines make many small updates to their weights, and each update
requires them to solve many different search problems.

Contents
--------

-   [1 The stochastic dynamics of a Boltzmann
    machine](#The_stochastic_dynamics_of_a_Boltzmann_machine)
-   [2 Learning in Boltzmann machines](#Learning_in_Boltzmann_machines)
    -   [2.1 Learning without hidden
        units](#Learning_without_hidden_units)
    -   [2.2 Learning with hidden units](#Learning_with_hidden_units)

-   [3 Different types of Boltzmann
    machine](#Different_types_of_Boltzmann_machine)
    -   [3.1 Higher-order Boltzmann
        machines](#Higher-order_Boltzmann_machines)
    -   [3.2 Conditional Boltzmann
        machines](#Conditional_Boltzmann_machines)
    -   [3.3 Mean field Boltzmann
        machines](#Mean_field_Boltzmann_machines)
    -   [3.4 Non-binary units](#Non-binary_units)

-   [4 The speed of learning](#The_speed_of_learning)
-   [5 Restricted Boltzmann machines](#Restricted_Boltzmann_machines)
    -   [5.1 Learning deep networks by composing restricted Boltzmann
        machines](#Learning_deep_networks_by_composing_restricted_Boltzmann_machines)

-   [6 Relationships to other models](#Relationships_to_other_models)
    -   [6.1 Markov random fields and Ising
        models](#Markov_random_fields_and_Ising_models)
    -   [6.2 Graphical models](#Graphical_models)
    -   [6.3 Gibbs sampling](#Gibbs_sampling)
    -   [6.4 Conditional random fields](#Conditional_random_fields)

-   [7 References](#References)
-   [8 See also](#See_also)

The stochastic dynamics of a Boltzmann machine
----------------------------------------------

When unit \\(i\\) is given the opportunity to update its binary state,
it first computes its total input, \\(z\_i\\ ,\\) which is the sum of
its own bias, \\(b\_i\\ ,\\) and the weights on connections coming from
other active units: \\[\\tag{1} z\_i = b\_i + \\sum\_j s\_j w\_{ij} \\]

where \\(w\_{ij}\\) is the weight on the connection between \\(i\\) and
\\(j\\ ,\\) and \\(s\_j\\) is \\(1\\) if unit \\(j\\) is on and \\(0\\)
otherwise. Unit \\(i\\) then turns on with a probability given by the
logistic function: \\[\\tag{2} prob(s\_i=1) \\ \\ = \\ \\
\\frac{1}{1+e\^{-z\_i}} \\]

\
 If the units are updated sequentially in any order that does not depend
on their total inputs, the network will eventually reach a Boltzmann
distribution (also called its
[equilibrium](/article/Equilibrium "Equilibrium") or stationary
distribution) in which the probability of a state vector, \\(v\\ ,\\) is
determined solely by the "energy" of that state vector relative to the
energies of all possible binary state vectors: \\[\\tag{3}
P(\\mathbf{v}) = e\^{-E(\\mathbf{v})}/\\sum\_\\mathbf{u}
e\^{-E(\\mathbf{u})} \\]

As in [Hopfield nets](/article/Hopfield_Network "Hopfield Network"), the
energy of state vector \\(\\mathbf{v}\\) is defined as \\[\\tag{4}
E(\\mathbf{v}) = -\\sum\_i s\^\\mathbf{v}\_i b\_i -\\sum\_{i<j}
s\^\\mathbf{v}\_i s\^\\mathbf{v}\_j w\_{ij} \\]

where \\(s\^\\mathbf{v}\_i\\) is the binary state assigned to unit
\\(i\\) by state vector \\(\\mathbf{v}\\ .\\)

If the weights on the connections are chosen so that the energies of
state vectors represent the cost of those state vectors, then the
stochastic dynamics of a Boltzmann machine can be viewed as a way of
escaping from poor local optima while searching for low-cost solutions.
The total input to unit \\(i\\ ,\\) \\(z\_i\\ ,\\) represents the
difference in energy depending on whether that unit is off or on, and
the fact that unit \\(i\\) occasionally turns on even if \\(z\_i\\) is
negative means that the energy can occasionally increase during the
search, thus allowing the search to jump over energy barriers.

The search can be improved by using simulated annealing. This scales
down all of the weights and energies by a factor, \\(T\\ ,\\) which is
analogous to the temperature of a physical system. By reducing T from a
large initial value to a small final value, it is possible to benefit
from the fast equilibration at high temperatures and still have a final
equilibrium distribution that makes low-cost solutions much more
probable than high-cost ones. At a temperature of \\(0\\) the update
rule becomes deterministic and a Boltzmann machine turns into a
[Hopfield Network](/article/Hopfield_Network "Hopfield Network")

Learning in Boltzmann machines
------------------------------

### Learning without hidden units

Given a training set of state vectors (the data), learning consists of
finding weights and biases (the parameters) that make those state
vectors good. More specifically, the aim is to find weights and biases
that define a Boltzmann distribution in which the training vectors have
high probability. By differentiating Eq. ([3](#Eq-3)) and using the fact
that \\(\\partial E(\\mathbf{v})/\\partial w\_{ij} = -
s\_i\^\\mathbf{v}s\_j\^\\mathbf{v}\\) it can be shown that \\[\\tag{5}
\\bigg\\langle \\frac{\\partial \\log P(\\mathbf{v})}{\\partial w\_{ij}}
\\bigg\\rangle\_\\mathrm{data} = \\langle s\_i s\_j
\\rangle\_\\mathrm{data} - \\langle s\_i s\_j \\rangle\_\\mathrm{model}
\\]

where \\(\\langle \\cdot \\rangle\_\\mathrm{data}\\) is an expected
value in the data distribution and \\(\\langle \\cdot
\\rangle\_\\mathrm{model}\\) is an expected value when the Boltzmann
machine is sampling state vectors from its equilibrium distribution at a
temperature of 1. To perform gradient ascent in the log probability that
the Boltzmann machine would generate the observed data when sampling
from its equilibrium distribution, \\(w\_{ij}\\) is incremented by a
small learning rate times the RHS of Eq. ([5](#Eq-5)). The learning rule
for the bias, \\(b\_i\\ ,\\) is the same as Eq. ([5](#Eq-5)), but with
\\(s\_j\\) ommitted.

If the observed data specifies a binary state for every unit in the
Boltzmann machine, the learning problem is convex: There are no
non-global optima in the parameter space. However, sampling from
\\(\\langle \\cdot \\rangle\_\\mathrm{model}\\) may involve overcoming
energy barriers in the binary [state
space](/article/State_space "State space").

### Learning with hidden units

Learning becomes much more interesting if the Boltzmann machine consists
of some "visible" units, whose states can be observed, and some "hidden"
units whose states are not specified by the observed data. The hidden
units act as latent variables (features) that allow the Boltzmann
machine to model distributions over visible state vectors that cannot be
modelled by direct pairwise interactions between the visible units. A
surprising property of Boltzmann machines is that, even with hidden
units, the learning rule remains unchanged. This makes it possible to
learn binary features that capture higher-order structure in the data.
With hidden units, the expectation \\(\\langle s\_i s\_j
\\rangle\_\\mathrm{data}\\) is the average, over all data vectors, of
the expected value of \\(s\_i s\_j\\) when a data vector is clamped on
the visible units and the hidden units are repeatedly updated until they
reach equilibrium with the clamped data vector.

It is surprising that the learning rule is so simple because
\\(\\partial \\log P(\\mathbf{v})/\\partial w\_{ij}\\) depends on all
the other weights in the network. Fortunately, the locally available
difference in the two correlations in Eq. ([5](#Eq-5)) tells
\\(w\_{ij}\\) everthing it needs to know about the other weights. This
makes it unnecessary to explicitly propagate error derivatives, as in
the backpropagation algorithm.

Different types of Boltzmann machine
------------------------------------

### Higher-order Boltzmann machines

The stochastic dynamics and the learning rule can accommodate more
complicated energy functions (Sejnowski, 1986). For example, the
quadratic energy function in Eq. ([4](#Eq-4)) can be replaced by an
energy function whose typical term is \\(s\_i s\_j s\_k w\_{ijk}\\ .\\)
The total input to unit \\(i\\) that is used in the update rule must
then be replaced by \\(z\_i = b\_i + \\sum\_{j<k} s\_j s\_k w\_{ijk}\\
.\\) The only change in the learning rule is that \\(s\_i s\_j\\) is
replaced by \\(s\_i s\_j s\_k\\ .\\)

### Conditional Boltzmann machines

Boltzmann machines model the distribution of the data vectors, but there
is a simple extension for modeling conditional distributions (Ackley et.
al. ,1985). The only difference between the visible and the hidden units
is that, when sampling \\(\\langle s\_i s\_j \\rangle\_\\mathrm{data}\\
,\\) the visible units are clamped and the hidden units are not. If a
subset of the visible units are also clamped when sampling \\( \\langle
s\_i s\_j \\rangle\_\\mathrm{model} \\) this subset acts as "input"
units and the remaining visible units act as "output" units. The same
learning rule applies, but now it maximizes the log probabilities of the
observed output vectors conditional on the input vectors.

### Mean field Boltzmann machines

Instead of using units that have stochastic binary states, it is
possible to use "mean field" units that have deterministic, real-valued
states between 0 and 1, as in an analog Hopfield net. Eq. ([2](#Eq-2))
is used to compute an "ideal" value for a unit's state given the current
states of the other units and the actual value is moved towards the
ideal value by some fraction of the difference. If this fraction is
small, all the units can be updated in parallel. The same learning rules
can be used by simply replacing the stochastic, binary values by the
deterministic real-values (Petersen and Andersen, 1987), but the
learning algorithm is hard to justify and mean field nets have problems
modeling multi-modal distributions.

### Non-binary units

The binary stochastic units used in Boltzmann machines can be
generalized to "softmax" units that have more than 2 discrete values,
Gaussian units whose output is simply their total input plus Gaussian
noise, binomial units, Poisson units, and any other type of unit that
falls in the exponential family (Welling et. al., 2005). This family is
characterized by the fact that the adjustable parameters have linear
effects on the log probabilities. The general form of the gradient
required for learning is simply the change in the sufficient statistics
caused by clamping data on the visible units.

The speed of learning
---------------------

Learning is typically very slow in Boltzmann machines with many hidden
layers because large networks can take a long time to approach their
equilibrium distribution, especially when the weights are large and the
equilibrium distribution is highly multimodal, as it usually is when the
visible units are unclamped. Even if samples from the equilibrium
distribution can be obtained, the learning signal is very noisy because
it is the difference of two sampled expectations. These difficulties can
be overcome by restricting the connectivity, simplifying the learning
algorithm, and learning one hidden layer at a time.

Restricted Boltzmann machines
-----------------------------

A restricted Boltzmann machine (Smolensky, 1986) consists of a layer of
visible units and a layer of hidden units with no visible-visible or
hidden-hidden connections. With these restrictions, the hidden units are
conditionally independent given a visible vector, so unbiased samples
from \\( \\langle s\_i s\_j \\rangle\_\\mathrm{data}\\) can be obtained
in one parallel step. To sample from \\( \\langle s\_i s\_j
\\rangle\_\\mathrm{model}\\) still requires multiple iterations that
alternate between updating all the hidden units in parallel and updating
all of the visible units in parallel. However, learning still works well
if \\(\\langle s\_i s\_j \\rangle\_\\mathrm{model}\\) is replaced by
\\(\\langle s\_i s\_j \\rangle\_\\mathrm{reconstruction}\\) which is
obtained as follows:

1.  Starting with a data vector on the visible units, update all of the
    hidden units in parallel.
2.  Update all of the visible units in parallel to get a
    "reconstruction".
3.  Update all of the hidden units again.

This efficient learning procedure does approximate gradient descent in a
quantity called "contrastive divergence" and works well in practice
(Hinton, 2002).

### Learning deep networks by composing restricted Boltzmann machines

After learning one hidden layer, the activity vectors of the hidden
units, when they are being driven by the real data, can be treated as
"data" for training another restricted Boltzmann machine. This can be
repeated to learn as many hidden layers as desired. After learning
multiple hidden layers in this way, the whole network can be viewed as a
single, multilayer generative model and each additional hidden layer
improves a lower bound on the probability that the multilayer model
would generate the training data (Hinton et. al., 2006).

Learning one hidden layer at a time is a very effective way to learn
deep neural networks with many hidden layers and millions of weights.
Even though the learning is unsupervised, the highest level features are
typically much more useful for classification than the raw data vectors.
These deep networks can be fine-tuned to be better at classification or
[dimensionality
reduction](/article/Dimensionality_reduction "Dimensionality reduction")
using the backpropagation algorithm (Hinton & Salakhutdinov, 2006).
Alternatively, they can be fine-tuned to be better generative models
using a version of the "wake-sleep" algorithm (Hinton et. al., 2006).

Relationships to other models
-----------------------------

### Markov random fields and Ising models

Boltzmann machines are a type of Markov random field, but most Markov
random fields have simple, local interaction weights which are designed
by hand rather than being learned. Boltzmann machines are Ising models,
but Ising models typically use random or hand-designed interaction
weights.

### Graphical models

The learning algorithm for Boltzmann machines was the first learning
algorithm for undirected graphical models with hidden variables (Jordan
1998). When restricted Boltzmann machines are composed to learn a deep
network, the top two layers of the resulting graphical model form an
unrestricted Boltzmann machine, but the lower layers form a directed
acyclic graph with directed connections from higher layers to lower
layers (Hinton et. al. 2006).

### Gibbs sampling

The search procedure for Boltzmann machines is an early example of Gibbs
sampling, a [Markov chain](/article/Markov_chain "Markov chain") Monte
Carlo method which was invented independently (Geman & Geman, 1984) and
was also inspired by simulated annealing.

### [Conditional random fields](/article/Conditional_random_fields "Conditional random fields")

Conditional random fields (Lafferty et. al., 2001) can be viewed as
simplified versions of higher-order, conditional Boltzmann machines in
which the hidden units have been eliminated. This makes the learning
problem convex, but removes the ability to learn new features.

References
----------

-   Ackley, D., Hinton, G., and Sejnowski, T. (1985). A Learning
    Algorithm for Boltzmann Machines. *Cognitive Science*, 9(1):147-169.

-   Geman, S. and Geman, D. (1984). Stochastic relaxation, Gibbs
    distributions, and the
    [Bayesian](/article/Bayesian_statistics "Bayesian statistics")
    restoration of images. *IEEE Transactions on Pattern Analysis and
    Machine Intelligence*, 6(6):721-741.

-   Hinton, G. E. (2002). Training products of experts by minimizing
    contrastive divergence. *[Neural](/article/Neuron "Neuron")
    Computation*, 14(8):1711-1800.

-   Hinton, G. E, Osindero, S., and Teh, Y. W. (2006). A fast learning
    algorithm for [deep belief
    nets](/article/Deep_Belief_Networks "Deep Belief Networks"). *Neural
    Computation*, 18:1527-1554.

-   Hinton, G. E. and Salakhutdinov, R. R. (2006). Reducing the
    dimensionality of data with neural networks. *Science*, 313:504-507.

-   Hinton, G. E. and Sejnowski, T. J. (1983). Optimal Perceptual
    Inference. *Proceedings of the IEEE conference on Computer
    [Vision](/article/Vision "Vision") and Pattern Recognition*,
    Washington DC, pp. 448-453.

-   Jordan, M. I. (1998) Learning in Graphical Models, MIT press,
    Cambridge Mass.

-   Lafferty, J. and McCallum, A. and Pereira, F. (2001) Conditional
    random fields: Probabilistic models for segmenting and labeling
    sequence data. *Proc. 18th International Conf. on Machine Learning*,
    pages 282-289 Morgan Kaufmann, San Francisco, CA

-   Peterson, C. and Anderson, J.R. (1987), A mean field theory learning
    algorithm for neural networks. *[Complex
    Systems](/article/Complex_Systems "Complex Systems")*,
    1(5):995--1019.

-   Sejnowski, T. J. (1986). Higher-order Boltzmann machines. *AIP
    Conference Proceedings*, 151(1):398-403.

-   Smolensky, P. (1986). Information processing in dynamical systems:
    Foundations of harmony theory. In Rumelhart, D. E. and McClelland,
    J. L., editors, *Parallel Distributed Processing: Volume 1:
    Foundations*, pages 194-281. MIT Press, Cambridge, MA.

-   Welling, M., Rosen-Zvi, M., and Hinton, G. E. (2005). Exponential
    family harmoniums with an application to information retrieval.
    *Advances in Neural Information Processing Systems 17*, pages
    1481-1488. MIT Press, Cambridge, MA.

**Internal references**

-   James Meiss (2007) [Dynamical
    systems](/article/Dynamical_systems "Dynamical systems").
    [Scholarpedia](/article/Scholarpedia "Scholarpedia"), 2(2):1629.
-   Eugene M. Izhikevich (2007)
    [Equilibrium](/article/Equilibrium "Equilibrium"). Scholarpedia,
    2(10):2014.
-   John J. Hopfield (2007) [Hopfield
    network](/article/Hopfield_network "Hopfield network").
    Scholarpedia, 2(5):1977.

See also
--------

[Associative
Memory](/w/index.php?title=Associative_Memory&action=edit&redlink=1 "Associative Memory (page does not exist)"),
[Boltzmann
Distribution](/w/index.php?title=Boltzmann_Distribution&action=edit&redlink=1 "Boltzmann Distribution (page does not exist)"),
[Hopfield Network](/article/Hopfield_Network "Hopfield Network"),
[Neural
Networks](/w/index.php?title=Neural_Networks&action=edit&redlink=1 "Neural Networks (page does not exist)"),
[Simulated
Annealing](/w/index.php?title=Simulated_Annealing&action=edit&redlink=1 "Simulated Annealing (page does not exist)"),
[Unsupervised
Learning](/w/index.php?title=Unsupervised_Learning&action=edit&redlink=1 "Unsupervised Learning (page does not exist)")

  -----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
  Sponsored by: [Eugene M. Izhikevich, Editor-in-Chief of Scholarpedia, the peer-reviewed open-access encyclopedia](/article/User:Eugene_M._Izhikevich "User:Eugene M. Izhikevich")
  [Reviewed by](http://www.scholarpedia.org/w/index.php?title=Boltzmann_machine&oldid=12031): [Anonymous](/article/User:Anonymous "User:Anonymous")
  [Reviewed by](http://www.scholarpedia.org/w/index.php?title=Boltzmann_machine&oldid=12477): [Dr. Zoubin Ghahramani, Department of Engineering, University of Cambridge, UK, and Carnegie Mellon Univerisity, USA](/article/User:Zoubin_Ghahramani "User:Zoubin Ghahramani")
  [Reviewed by](http://www.scholarpedia.org/w/index.php?title=Boltzmann_machine&oldid=10790): [Dr. Barak Pearlmutter, Hamilton Institute, National University of Ireland Maynooth, Co. Kildare, Ireland](/article/User:Barak_Pearlmutter "User:Barak Pearlmutter")
  Accepted on: [2007-05-24 18:16:00 GMT](http://www.scholarpedia.org/w/index.php?title=Boltzmann_machine&oldid=13364)
  -----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

Retrieved from
"[http://www.scholarpedia.org/w/index.php?title=Boltzmann\_machine&oldid=91075](http://www.scholarpedia.org/w/index.php?title=Boltzmann_machine&oldid=91075)"

[Categories](/article/Special:Categories "Special:Categories"):

-   [Recurrent Neural
    Networks](/article/Category:Recurrent_Neural_Networks "Category:Recurrent Neural Networks")
-   [Computational
    Intelligence](/article/Category:Computational_Intelligence "Category:Computational Intelligence")
-   [Computational
    Neuroscience](/article/Category:Computational_Neuroscience "Category:Computational Neuroscience")
-   [Neural
    Networks](/article/Category:Neural_Networks "Category:Neural Networks")

##### Personal tools

-   [Log in / create
    account](/w/index.php?title=Special:UserLogin&returnto=Boltzmann+machine "You are encouraged to log in; however, it is not mandatory [o]")

##### Namespaces

-   [Page](/article/Boltzmann_machine "View the content page [c]")
-   [Discussion](/article/Talk:Boltzmann_machine "Discussion about the content page [t]")

#### 

##### Variants[](#)

##### Views

-   [Read](/article/Boltzmann_machine)
-   [View
    source](/w/index.php?title=Boltzmann_machine&action=edit "This page is protected.
    You can view its source [e]")
-   [View
    history](/w/index.php?title=Boltzmann_machine&action=history "Past revisions of this page [h]")

##### Actions[](#)

##### Search

![Search](/w/skins/vector/images/search-ltr.png?303)

[](/article/Main_Page "Visit the main page")

##### Navigation

-   [Main page](/article/Main_Page "Visit the main page [z]")
-   [Propose a new article](/article/Special:ProposeArticle)
-   [About](/article/Scholarpedia:About)
-   [Random article](/article/Special:Random "Load a random page [x]")
-   [Help](/article/Scholarpedia:Help)
-   [F.A.Q.'s](/article/Help:Frequently_Asked_Questions)
-   [Blog](http://blog.scholarpedia.org)

##### Focal areas

-   [Astrophysics](/article/Encyclopedia:Astrophysics)
-   [Celestial mechanics](/article/Encyclopedia:Celestial_Mechanics)
-   [Computational
    neuroscience](/article/Encyclopedia:Computational_neuroscience)
-   [Computational
    intelligence](/article/Encyclopedia:Computational_intelligence)
-   [Dynamical systems](/article/Encyclopedia:Dynamical_systems)
-   [Physics](/article/Encyclopedia:Physics)
-   [Touch](/article/Encyclopedia:Touch)
-   [More topics](/article/Scholarpedia:Topics)

##### Activity

-   [Recently published articles](/article/Special:RecentlyPublished)
-   [Recently sponsored articles](/article/Special:RecentlySponsored)
-   [Recent
    changes](/article/Special:RecentChanges "A list of recent changes in the wiki [r]")
-   [All articles](/article/Special:AllPages)
-   [List all Curators](/article/Special:ListCurators)
-   [List all users](/article/Special:ListUsers)
-   [Scholarpedia Journal](/article/Special:Journal)

##### Tools

-   [What links
    here](/article/Special:WhatLinksHere/Boltzmann_machine "A list of all wiki pages that link here [j]")
-   [Related
    changes](/article/Special:RecentChangesLinked/Boltzmann_machine "Recent changes in pages linked from this page [k]")
-   [Special
    pages](/article/Special:SpecialPages "A list of all special pages [q]")
-   [Printable
    version](/w/index.php?title=Boltzmann_machine&printable=yes)
-   [Permanent
    link](/w/index.php?title=Boltzmann_machine&oldid=91075 "Permanent link to this revision of the page")

-   [![image](/w/skins/vector/images/twitter.png?303)](https://twitter.com/scholarpedia)
-   [![image](https://ssl.gstatic.com/images/icons/gplus-16.png)](https://plus.google.com/112873162496270574424)
-   [![image](/w/skins/vector/images/facebook.png?303)](http://www.facebook.com/Scholarpedia)
-   [![image](/w/skins/vector/images/linkedin.png?303)](http://www.linkedin.com/groups/Scholarpedia-4647975/about)

-   [![Powered by
    MediaWiki](/w/skins/common/images/poweredby_mediawiki_88x31.png)](http://www.mediawiki.org/)
    [![Powered by
    MathJax](/w/skins/common/images/MathJaxBadge.gif)](http://www.mathjax.org/)
    [![Creative Commons
    License](/skins/common/88x31.png)](http://creativecommons.org/licenses/by-nc-sa/3.0/deed.en_US)

-   "Boltzmann machine" by [Geoffrey E.
    Hinton](http://www.scholarpedia.org/article/Boltzmann_machine) is
    licensed under a [Creative Commons
    Attribution-NonCommercial-ShareAlike 3.0 Unported
    License](http://creativecommons.org/licenses/by-nc-sa/3.0/deed.en_US).
    Permissions beyond the scope of this license are described in the
    [Terms of
    Use](http://www.scholarpedia.org/article/Scholarpedia:Terms_of_use)

-   [Privacy
    policy](/article/Scholarpedia:Privacy_policy "Scholarpedia:Privacy policy")
-   [About
    Scholarpedia](/article/Scholarpedia:About "Scholarpedia:About")
-   [Disclaimers](/article/Scholarpedia:General_disclaimer "Scholarpedia:General disclaimer")


This markdown document has been converted from the html document located at:
http://www.scholarpedia.org/article/Boltzmann_machine
